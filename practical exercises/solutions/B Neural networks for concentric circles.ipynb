{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on concentric circles data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200 # Number of points in dataset\n",
    "np.random.seed(0)\n",
    "r1 = np.random.gamma(10, 0.1, n//2)\n",
    "r2 = np.random.gamma(30, 0.1, n//2)\n",
    "phi1 = np.random.uniform(-np.pi,np.pi,n//2)\n",
    "phi2 = np.random.uniform(-np.pi,np.pi,n//2)\n",
    "r = np.hstack((r1, r2))\n",
    "phi = np.hstack((phi1, phi2))\n",
    "group = np.hstack((np.repeat(0,n//2), np.repeat(1,n//2)))\n",
    "x0 = r*np.cos(phi)\n",
    "x1 = r*np.sin(phi)\n",
    "X = np.stack((x0,x1), axis=1)\n",
    "y = group\n",
    "from sklearn.utils import shuffle\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we fit a model with ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dennis/.local/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "nn_relu = models.Sequential()\n",
    "nn_relu.add(layers.Dense(5, input_dim=2, activation=\"relu\"))\n",
    "nn_relu.add(layers.Dense(5, activation=\"relu\"))\n",
    "nn_relu.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dennis/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/800\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.9033 - acc: 0.5000 - val_loss: 0.9303 - val_acc: 0.5000\n",
      "Epoch 2/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.8870 - acc: 0.5000 - val_loss: 0.9103 - val_acc: 0.5000\n",
      "Epoch 3/800\n",
      "160/160 [==============================] - 0s 99us/step - loss: 0.8730 - acc: 0.4937 - val_loss: 0.8911 - val_acc: 0.5000\n",
      "Epoch 4/800\n",
      "160/160 [==============================] - 0s 117us/step - loss: 0.8591 - acc: 0.4813 - val_loss: 0.8731 - val_acc: 0.5000\n",
      "Epoch 5/800\n",
      "160/160 [==============================] - 0s 170us/step - loss: 0.8446 - acc: 0.4813 - val_loss: 0.8574 - val_acc: 0.5000\n",
      "Epoch 6/800\n",
      "160/160 [==============================] - 0s 111us/step - loss: 0.8331 - acc: 0.4750 - val_loss: 0.8427 - val_acc: 0.5000\n",
      "Epoch 7/800\n",
      "160/160 [==============================] - 0s 126us/step - loss: 0.8225 - acc: 0.4688 - val_loss: 0.8286 - val_acc: 0.5000\n",
      "Epoch 8/800\n",
      "160/160 [==============================] - 0s 136us/step - loss: 0.8118 - acc: 0.4750 - val_loss: 0.8159 - val_acc: 0.5000\n",
      "Epoch 9/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.8019 - acc: 0.4688 - val_loss: 0.8042 - val_acc: 0.5000\n",
      "Epoch 10/800\n",
      "160/160 [==============================] - 0s 54us/step - loss: 0.7936 - acc: 0.4688 - val_loss: 0.7932 - val_acc: 0.5000\n",
      "Epoch 11/800\n",
      "160/160 [==============================] - 0s 125us/step - loss: 0.7857 - acc: 0.4562 - val_loss: 0.7830 - val_acc: 0.4750\n",
      "Epoch 12/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.7774 - acc: 0.4500 - val_loss: 0.7737 - val_acc: 0.4750\n",
      "Epoch 13/800\n",
      "160/160 [==============================] - 0s 120us/step - loss: 0.7702 - acc: 0.4250 - val_loss: 0.7652 - val_acc: 0.4750\n",
      "Epoch 14/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.7630 - acc: 0.3875 - val_loss: 0.7575 - val_acc: 0.4250\n",
      "Epoch 15/800\n",
      "160/160 [==============================] - 0s 116us/step - loss: 0.7564 - acc: 0.3625 - val_loss: 0.7504 - val_acc: 0.3750\n",
      "Epoch 16/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.7508 - acc: 0.3000 - val_loss: 0.7435 - val_acc: 0.3500\n",
      "Epoch 17/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.7452 - acc: 0.3000 - val_loss: 0.7369 - val_acc: 0.2750\n",
      "Epoch 18/800\n",
      "160/160 [==============================] - 0s 126us/step - loss: 0.7398 - acc: 0.2750 - val_loss: 0.7307 - val_acc: 0.3000\n",
      "Epoch 19/800\n",
      "160/160 [==============================] - 0s 123us/step - loss: 0.7347 - acc: 0.2875 - val_loss: 0.7249 - val_acc: 0.3750\n",
      "Epoch 20/800\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.7300 - acc: 0.2750 - val_loss: 0.7193 - val_acc: 0.3250\n",
      "Epoch 21/800\n",
      "160/160 [==============================] - 0s 119us/step - loss: 0.7251 - acc: 0.2812 - val_loss: 0.7142 - val_acc: 0.3250\n",
      "Epoch 22/800\n",
      "160/160 [==============================] - 0s 107us/step - loss: 0.7204 - acc: 0.3000 - val_loss: 0.7095 - val_acc: 0.3250\n",
      "Epoch 23/800\n",
      "160/160 [==============================] - 0s 103us/step - loss: 0.7168 - acc: 0.2938 - val_loss: 0.7047 - val_acc: 0.3250\n",
      "Epoch 24/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.7122 - acc: 0.2875 - val_loss: 0.7005 - val_acc: 0.3250\n",
      "Epoch 25/800\n",
      "160/160 [==============================] - 0s 144us/step - loss: 0.7085 - acc: 0.2812 - val_loss: 0.6965 - val_acc: 0.3000\n",
      "Epoch 26/800\n",
      "160/160 [==============================] - 0s 148us/step - loss: 0.7049 - acc: 0.2875 - val_loss: 0.6925 - val_acc: 0.3500\n",
      "Epoch 27/800\n",
      "160/160 [==============================] - 0s 159us/step - loss: 0.7011 - acc: 0.2938 - val_loss: 0.6888 - val_acc: 0.3000\n",
      "Epoch 28/800\n",
      "160/160 [==============================] - 0s 119us/step - loss: 0.6978 - acc: 0.3125 - val_loss: 0.6851 - val_acc: 0.3000\n",
      "Epoch 29/800\n",
      "160/160 [==============================] - 0s 153us/step - loss: 0.6944 - acc: 0.3250 - val_loss: 0.6818 - val_acc: 0.3000\n",
      "Epoch 30/800\n",
      "160/160 [==============================] - 0s 148us/step - loss: 0.6910 - acc: 0.3312 - val_loss: 0.6786 - val_acc: 0.3000\n",
      "Epoch 31/800\n",
      "160/160 [==============================] - 0s 194us/step - loss: 0.6881 - acc: 0.3687 - val_loss: 0.6754 - val_acc: 0.3500\n",
      "Epoch 32/800\n",
      "160/160 [==============================] - 0s 146us/step - loss: 0.6850 - acc: 0.4000 - val_loss: 0.6723 - val_acc: 0.3500\n",
      "Epoch 33/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.6819 - acc: 0.4125 - val_loss: 0.6695 - val_acc: 0.4000\n",
      "Epoch 34/800\n",
      "160/160 [==============================] - 0s 125us/step - loss: 0.6791 - acc: 0.4312 - val_loss: 0.6667 - val_acc: 0.4000\n",
      "Epoch 35/800\n",
      "160/160 [==============================] - 0s 129us/step - loss: 0.6761 - acc: 0.4625 - val_loss: 0.6640 - val_acc: 0.4250\n",
      "Epoch 36/800\n",
      "160/160 [==============================] - 0s 159us/step - loss: 0.6734 - acc: 0.4937 - val_loss: 0.6612 - val_acc: 0.4250\n",
      "Epoch 37/800\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.6706 - acc: 0.5000 - val_loss: 0.6586 - val_acc: 0.4250\n",
      "Epoch 38/800\n",
      "160/160 [==============================] - 0s 123us/step - loss: 0.6679 - acc: 0.5062 - val_loss: 0.6560 - val_acc: 0.4250\n",
      "Epoch 39/800\n",
      "160/160 [==============================] - 0s 118us/step - loss: 0.6651 - acc: 0.5062 - val_loss: 0.6535 - val_acc: 0.4250\n",
      "Epoch 40/800\n",
      "160/160 [==============================] - 0s 143us/step - loss: 0.6626 - acc: 0.5125 - val_loss: 0.6510 - val_acc: 0.4250\n",
      "Epoch 41/800\n",
      "160/160 [==============================] - 0s 128us/step - loss: 0.6600 - acc: 0.5125 - val_loss: 0.6485 - val_acc: 0.4250\n",
      "Epoch 42/800\n",
      "160/160 [==============================] - 0s 108us/step - loss: 0.6573 - acc: 0.5125 - val_loss: 0.6461 - val_acc: 0.4250\n",
      "Epoch 43/800\n",
      "160/160 [==============================] - 0s 154us/step - loss: 0.6548 - acc: 0.5125 - val_loss: 0.6437 - val_acc: 0.4250\n",
      "Epoch 44/800\n",
      "160/160 [==============================] - 0s 126us/step - loss: 0.6524 - acc: 0.5187 - val_loss: 0.6411 - val_acc: 0.4250\n",
      "Epoch 45/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.6497 - acc: 0.5250 - val_loss: 0.6387 - val_acc: 0.4500\n",
      "Epoch 46/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.6472 - acc: 0.5250 - val_loss: 0.6364 - val_acc: 0.4750\n",
      "Epoch 47/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.6447 - acc: 0.5375 - val_loss: 0.6340 - val_acc: 0.5000\n",
      "Epoch 48/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.6421 - acc: 0.5375 - val_loss: 0.6316 - val_acc: 0.5000\n",
      "Epoch 49/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.6395 - acc: 0.5375 - val_loss: 0.6293 - val_acc: 0.5250\n",
      "Epoch 50/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.6370 - acc: 0.5563 - val_loss: 0.6270 - val_acc: 0.5500\n",
      "Epoch 51/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.6343 - acc: 0.5625 - val_loss: 0.6247 - val_acc: 0.5500\n",
      "Epoch 52/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.6317 - acc: 0.5875 - val_loss: 0.6223 - val_acc: 0.5750\n",
      "Epoch 53/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.6291 - acc: 0.5938 - val_loss: 0.6200 - val_acc: 0.5750\n",
      "Epoch 54/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.6264 - acc: 0.6063 - val_loss: 0.6177 - val_acc: 0.5750\n",
      "Epoch 55/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.6239 - acc: 0.6250 - val_loss: 0.6154 - val_acc: 0.6000\n",
      "Epoch 56/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.6212 - acc: 0.6250 - val_loss: 0.6130 - val_acc: 0.6000\n",
      "Epoch 57/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.6185 - acc: 0.6500 - val_loss: 0.6107 - val_acc: 0.6000\n",
      "Epoch 58/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.6159 - acc: 0.6562 - val_loss: 0.6084 - val_acc: 0.6000\n",
      "Epoch 59/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 79us/step - loss: 0.6133 - acc: 0.6813 - val_loss: 0.6061 - val_acc: 0.6000\n",
      "Epoch 60/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.6108 - acc: 0.6813 - val_loss: 0.6039 - val_acc: 0.6000\n",
      "Epoch 61/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.6081 - acc: 0.6750 - val_loss: 0.6017 - val_acc: 0.6000\n",
      "Epoch 62/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.6056 - acc: 0.7000 - val_loss: 0.5994 - val_acc: 0.6250\n",
      "Epoch 63/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.6029 - acc: 0.7000 - val_loss: 0.5971 - val_acc: 0.6500\n",
      "Epoch 64/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.6003 - acc: 0.7062 - val_loss: 0.5947 - val_acc: 0.6750\n",
      "Epoch 65/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.5979 - acc: 0.7000 - val_loss: 0.5924 - val_acc: 0.6750\n",
      "Epoch 66/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.5953 - acc: 0.7125 - val_loss: 0.5899 - val_acc: 0.6750\n",
      "Epoch 67/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.5926 - acc: 0.7188 - val_loss: 0.5875 - val_acc: 0.7000\n",
      "Epoch 68/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.5899 - acc: 0.7250 - val_loss: 0.5850 - val_acc: 0.7000\n",
      "Epoch 69/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.5872 - acc: 0.7250 - val_loss: 0.5825 - val_acc: 0.7000\n",
      "Epoch 70/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.5845 - acc: 0.7313 - val_loss: 0.5799 - val_acc: 0.7000\n",
      "Epoch 71/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.5818 - acc: 0.7313 - val_loss: 0.5774 - val_acc: 0.7000\n",
      "Epoch 72/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.5790 - acc: 0.7313 - val_loss: 0.5748 - val_acc: 0.7000\n",
      "Epoch 73/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.5762 - acc: 0.7438 - val_loss: 0.5724 - val_acc: 0.7000\n",
      "Epoch 74/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.5733 - acc: 0.7500 - val_loss: 0.5698 - val_acc: 0.7000\n",
      "Epoch 75/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.5704 - acc: 0.7625 - val_loss: 0.5672 - val_acc: 0.7000\n",
      "Epoch 76/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.5675 - acc: 0.7625 - val_loss: 0.5645 - val_acc: 0.7000\n",
      "Epoch 77/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.5644 - acc: 0.7750 - val_loss: 0.5618 - val_acc: 0.7000\n",
      "Epoch 78/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.5613 - acc: 0.7750 - val_loss: 0.5590 - val_acc: 0.7000\n",
      "Epoch 79/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.5582 - acc: 0.7750 - val_loss: 0.5562 - val_acc: 0.7000\n",
      "Epoch 80/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.5549 - acc: 0.7938 - val_loss: 0.5534 - val_acc: 0.7000\n",
      "Epoch 81/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.5517 - acc: 0.8063 - val_loss: 0.5505 - val_acc: 0.7000\n",
      "Epoch 82/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.5484 - acc: 0.8187 - val_loss: 0.5476 - val_acc: 0.7000\n",
      "Epoch 83/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.5451 - acc: 0.8250 - val_loss: 0.5445 - val_acc: 0.7000\n",
      "Epoch 84/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.5417 - acc: 0.8312 - val_loss: 0.5417 - val_acc: 0.7250\n",
      "Epoch 85/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.5382 - acc: 0.8438 - val_loss: 0.5388 - val_acc: 0.7250\n",
      "Epoch 86/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.5347 - acc: 0.8438 - val_loss: 0.5357 - val_acc: 0.7250\n",
      "Epoch 87/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.5313 - acc: 0.8563 - val_loss: 0.5327 - val_acc: 0.7250\n",
      "Epoch 88/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.5277 - acc: 0.8563 - val_loss: 0.5297 - val_acc: 0.7250\n",
      "Epoch 89/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.5242 - acc: 0.8625 - val_loss: 0.5266 - val_acc: 0.7250\n",
      "Epoch 90/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.5205 - acc: 0.8750 - val_loss: 0.5235 - val_acc: 0.7500\n",
      "Epoch 91/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.5166 - acc: 0.8750 - val_loss: 0.5205 - val_acc: 0.7750\n",
      "Epoch 92/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.5131 - acc: 0.8812 - val_loss: 0.5174 - val_acc: 0.8000\n",
      "Epoch 93/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.5091 - acc: 0.8812 - val_loss: 0.5142 - val_acc: 0.8000\n",
      "Epoch 94/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.5053 - acc: 0.8812 - val_loss: 0.5111 - val_acc: 0.8500\n",
      "Epoch 95/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.5013 - acc: 0.8812 - val_loss: 0.5078 - val_acc: 0.8500\n",
      "Epoch 96/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.4973 - acc: 0.8812 - val_loss: 0.5045 - val_acc: 0.8750\n",
      "Epoch 97/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.4932 - acc: 0.8812 - val_loss: 0.5013 - val_acc: 0.8750\n",
      "Epoch 98/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.4891 - acc: 0.8937 - val_loss: 0.4980 - val_acc: 0.8750\n",
      "Epoch 99/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.4849 - acc: 0.8937 - val_loss: 0.4946 - val_acc: 0.8750\n",
      "Epoch 100/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.4807 - acc: 0.8937 - val_loss: 0.4912 - val_acc: 0.8750\n",
      "Epoch 101/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.4765 - acc: 0.9000 - val_loss: 0.4878 - val_acc: 0.8750\n",
      "Epoch 102/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.4721 - acc: 0.9000 - val_loss: 0.4843 - val_acc: 0.8750\n",
      "Epoch 103/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.4678 - acc: 0.9000 - val_loss: 0.4808 - val_acc: 0.8750\n",
      "Epoch 104/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.4635 - acc: 0.9000 - val_loss: 0.4774 - val_acc: 0.8750\n",
      "Epoch 105/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.4590 - acc: 0.9000 - val_loss: 0.4740 - val_acc: 0.8750\n",
      "Epoch 106/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.4544 - acc: 0.9188 - val_loss: 0.4703 - val_acc: 0.8750\n",
      "Epoch 107/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.4499 - acc: 0.9188 - val_loss: 0.4669 - val_acc: 0.8750\n",
      "Epoch 108/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.4452 - acc: 0.9188 - val_loss: 0.4633 - val_acc: 0.8750\n",
      "Epoch 109/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.4405 - acc: 0.9188 - val_loss: 0.4596 - val_acc: 0.8750\n",
      "Epoch 110/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.4358 - acc: 0.9250 - val_loss: 0.4559 - val_acc: 0.8750\n",
      "Epoch 111/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.4309 - acc: 0.9313 - val_loss: 0.4521 - val_acc: 0.8750\n",
      "Epoch 112/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.4261 - acc: 0.9313 - val_loss: 0.4484 - val_acc: 0.8750\n",
      "Epoch 113/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.4212 - acc: 0.9313 - val_loss: 0.4444 - val_acc: 0.8750\n",
      "Epoch 114/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.4163 - acc: 0.9375 - val_loss: 0.4405 - val_acc: 0.8750\n",
      "Epoch 115/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.4113 - acc: 0.9375 - val_loss: 0.4367 - val_acc: 0.8750\n",
      "Epoch 116/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.4062 - acc: 0.9375 - val_loss: 0.4326 - val_acc: 0.8750\n",
      "Epoch 117/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.4012 - acc: 0.9375 - val_loss: 0.4284 - val_acc: 0.8750\n",
      "Epoch 118/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.3961 - acc: 0.9375 - val_loss: 0.4243 - val_acc: 0.8750\n",
      "Epoch 119/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 77us/step - loss: 0.3908 - acc: 0.9375 - val_loss: 0.4204 - val_acc: 0.8750\n",
      "Epoch 120/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.3856 - acc: 0.9375 - val_loss: 0.4163 - val_acc: 0.8750\n",
      "Epoch 121/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.3801 - acc: 0.9437 - val_loss: 0.4123 - val_acc: 0.8750\n",
      "Epoch 122/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.3747 - acc: 0.9437 - val_loss: 0.4080 - val_acc: 0.8750\n",
      "Epoch 123/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.3694 - acc: 0.9437 - val_loss: 0.4038 - val_acc: 0.8750\n",
      "Epoch 124/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.3639 - acc: 0.9437 - val_loss: 0.3994 - val_acc: 0.8750\n",
      "Epoch 125/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.3582 - acc: 0.9437 - val_loss: 0.3952 - val_acc: 0.8750\n",
      "Epoch 126/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.3524 - acc: 0.9437 - val_loss: 0.3908 - val_acc: 0.8750\n",
      "Epoch 127/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.3469 - acc: 0.9437 - val_loss: 0.3866 - val_acc: 0.8750\n",
      "Epoch 128/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.3408 - acc: 0.9437 - val_loss: 0.3820 - val_acc: 0.8750\n",
      "Epoch 129/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.3352 - acc: 0.9437 - val_loss: 0.3774 - val_acc: 0.9000\n",
      "Epoch 130/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.3293 - acc: 0.9437 - val_loss: 0.3729 - val_acc: 0.9000\n",
      "Epoch 131/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.3235 - acc: 0.9500 - val_loss: 0.3685 - val_acc: 0.9000\n",
      "Epoch 132/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.3178 - acc: 0.9562 - val_loss: 0.3642 - val_acc: 0.9000\n",
      "Epoch 133/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.3120 - acc: 0.9562 - val_loss: 0.3598 - val_acc: 0.9000\n",
      "Epoch 134/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.3060 - acc: 0.9625 - val_loss: 0.3554 - val_acc: 0.9000\n",
      "Epoch 135/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.3006 - acc: 0.9625 - val_loss: 0.3510 - val_acc: 0.9000\n",
      "Epoch 136/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.2948 - acc: 0.9688 - val_loss: 0.3464 - val_acc: 0.9000\n",
      "Epoch 137/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.2890 - acc: 0.9688 - val_loss: 0.3418 - val_acc: 0.9000\n",
      "Epoch 138/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.2834 - acc: 0.9750 - val_loss: 0.3370 - val_acc: 0.9000\n",
      "Epoch 139/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.2779 - acc: 0.9750 - val_loss: 0.3329 - val_acc: 0.9000\n",
      "Epoch 140/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.2721 - acc: 0.9813 - val_loss: 0.3281 - val_acc: 0.9000\n",
      "Epoch 141/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.2664 - acc: 0.9813 - val_loss: 0.3234 - val_acc: 0.9000\n",
      "Epoch 142/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.2609 - acc: 0.9813 - val_loss: 0.3189 - val_acc: 0.9250\n",
      "Epoch 143/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.2555 - acc: 0.9875 - val_loss: 0.3143 - val_acc: 0.9250\n",
      "Epoch 144/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.2501 - acc: 0.9875 - val_loss: 0.3096 - val_acc: 0.9250\n",
      "Epoch 145/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.2448 - acc: 0.9875 - val_loss: 0.3046 - val_acc: 0.9250\n",
      "Epoch 146/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.2396 - acc: 0.9813 - val_loss: 0.3002 - val_acc: 0.9250\n",
      "Epoch 147/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.2343 - acc: 0.9813 - val_loss: 0.2954 - val_acc: 0.9250\n",
      "Epoch 148/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.2293 - acc: 0.9813 - val_loss: 0.2907 - val_acc: 0.9250\n",
      "Epoch 149/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.2243 - acc: 0.9813 - val_loss: 0.2862 - val_acc: 0.9250\n",
      "Epoch 150/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.2192 - acc: 0.9875 - val_loss: 0.2813 - val_acc: 0.9500\n",
      "Epoch 151/800\n",
      "160/160 [==============================] - 0s 110us/step - loss: 0.2141 - acc: 0.9875 - val_loss: 0.2759 - val_acc: 0.9500\n",
      "Epoch 152/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.2091 - acc: 0.9875 - val_loss: 0.2705 - val_acc: 0.9500\n",
      "Epoch 153/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.2038 - acc: 0.9875 - val_loss: 0.2651 - val_acc: 0.9500\n",
      "Epoch 154/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.1984 - acc: 0.9875 - val_loss: 0.2595 - val_acc: 0.9500\n",
      "Epoch 155/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.1930 - acc: 0.9875 - val_loss: 0.2538 - val_acc: 0.9500\n",
      "Epoch 156/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.1878 - acc: 0.9875 - val_loss: 0.2483 - val_acc: 0.9500\n",
      "Epoch 157/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.1823 - acc: 0.9938 - val_loss: 0.2431 - val_acc: 0.9500\n",
      "Epoch 158/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.1773 - acc: 0.9938 - val_loss: 0.2385 - val_acc: 0.9500\n",
      "Epoch 159/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.1724 - acc: 0.9938 - val_loss: 0.2341 - val_acc: 0.9500\n",
      "Epoch 160/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.1681 - acc: 0.9938 - val_loss: 0.2300 - val_acc: 0.9500\n",
      "Epoch 161/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.1638 - acc: 0.9938 - val_loss: 0.2258 - val_acc: 0.9500\n",
      "Epoch 162/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.1597 - acc: 0.9938 - val_loss: 0.2220 - val_acc: 0.9500\n",
      "Epoch 163/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.1558 - acc: 0.9938 - val_loss: 0.2186 - val_acc: 0.9500\n",
      "Epoch 164/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.1524 - acc: 0.9938 - val_loss: 0.2155 - val_acc: 0.9500\n",
      "Epoch 165/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.1488 - acc: 0.9938 - val_loss: 0.2125 - val_acc: 0.9500\n",
      "Epoch 166/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.1454 - acc: 0.9938 - val_loss: 0.2097 - val_acc: 0.9500\n",
      "Epoch 167/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.1425 - acc: 0.9938 - val_loss: 0.2066 - val_acc: 0.9500\n",
      "Epoch 168/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.1394 - acc: 0.9938 - val_loss: 0.2036 - val_acc: 0.9500\n",
      "Epoch 169/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.1365 - acc: 0.9938 - val_loss: 0.2007 - val_acc: 0.9500\n",
      "Epoch 170/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.1337 - acc: 0.9938 - val_loss: 0.1982 - val_acc: 0.9500\n",
      "Epoch 171/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.1311 - acc: 0.9938 - val_loss: 0.1958 - val_acc: 0.9500\n",
      "Epoch 172/800\n",
      "160/160 [==============================] - 0s 119us/step - loss: 0.1285 - acc: 0.9938 - val_loss: 0.1935 - val_acc: 0.9500\n",
      "Epoch 173/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.1261 - acc: 0.9938 - val_loss: 0.1912 - val_acc: 0.9500\n",
      "Epoch 174/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.1237 - acc: 0.9938 - val_loss: 0.1890 - val_acc: 0.9500\n",
      "Epoch 175/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.1215 - acc: 0.9938 - val_loss: 0.1872 - val_acc: 0.9500\n",
      "Epoch 176/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.1193 - acc: 0.9938 - val_loss: 0.1850 - val_acc: 0.9500\n",
      "Epoch 177/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.1171 - acc: 0.9938 - val_loss: 0.1829 - val_acc: 0.9500\n",
      "Epoch 178/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.1152 - acc: 0.9938 - val_loss: 0.1802 - val_acc: 0.9500\n",
      "Epoch 179/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 104us/step - loss: 0.1131 - acc: 0.9938 - val_loss: 0.1782 - val_acc: 0.9500\n",
      "Epoch 180/800\n",
      "160/160 [==============================] - 0s 108us/step - loss: 0.1112 - acc: 0.9938 - val_loss: 0.1760 - val_acc: 0.9500\n",
      "Epoch 181/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.1094 - acc: 0.9938 - val_loss: 0.1742 - val_acc: 0.9500\n",
      "Epoch 182/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.1077 - acc: 0.9938 - val_loss: 0.1723 - val_acc: 0.9750\n",
      "Epoch 183/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.1060 - acc: 0.9938 - val_loss: 0.1698 - val_acc: 0.9750\n",
      "Epoch 184/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.1044 - acc: 0.9938 - val_loss: 0.1682 - val_acc: 0.9750\n",
      "Epoch 185/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1028 - acc: 0.9938 - val_loss: 0.1663 - val_acc: 0.9750\n",
      "Epoch 186/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.1011 - acc: 0.9938 - val_loss: 0.1644 - val_acc: 0.9750\n",
      "Epoch 187/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0996 - acc: 0.9938 - val_loss: 0.1624 - val_acc: 0.9750\n",
      "Epoch 188/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0981 - acc: 0.9938 - val_loss: 0.1609 - val_acc: 0.9750\n",
      "Epoch 189/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0966 - acc: 0.9938 - val_loss: 0.1590 - val_acc: 0.9750\n",
      "Epoch 190/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0953 - acc: 0.9938 - val_loss: 0.1573 - val_acc: 0.9750\n",
      "Epoch 191/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0938 - acc: 0.9938 - val_loss: 0.1557 - val_acc: 0.9750\n",
      "Epoch 192/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0925 - acc: 0.9938 - val_loss: 0.1540 - val_acc: 0.9750\n",
      "Epoch 193/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0913 - acc: 0.9938 - val_loss: 0.1529 - val_acc: 0.9750\n",
      "Epoch 194/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0901 - acc: 0.9938 - val_loss: 0.1511 - val_acc: 0.9750\n",
      "Epoch 195/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0888 - acc: 0.9938 - val_loss: 0.1497 - val_acc: 0.9750\n",
      "Epoch 196/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0876 - acc: 0.9938 - val_loss: 0.1487 - val_acc: 0.9750\n",
      "Epoch 197/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0864 - acc: 0.9938 - val_loss: 0.1471 - val_acc: 0.9750\n",
      "Epoch 198/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0853 - acc: 0.9938 - val_loss: 0.1460 - val_acc: 0.9750\n",
      "Epoch 199/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0842 - acc: 0.9938 - val_loss: 0.1448 - val_acc: 0.9750\n",
      "Epoch 200/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0830 - acc: 0.9938 - val_loss: 0.1434 - val_acc: 0.9750\n",
      "Epoch 201/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0820 - acc: 0.9938 - val_loss: 0.1424 - val_acc: 0.9750\n",
      "Epoch 202/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0809 - acc: 0.9938 - val_loss: 0.1411 - val_acc: 0.9750\n",
      "Epoch 203/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0799 - acc: 0.9938 - val_loss: 0.1401 - val_acc: 0.9750\n",
      "Epoch 204/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0789 - acc: 0.9938 - val_loss: 0.1386 - val_acc: 0.9750\n",
      "Epoch 205/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0780 - acc: 0.9938 - val_loss: 0.1375 - val_acc: 0.9750\n",
      "Epoch 206/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0770 - acc: 0.9938 - val_loss: 0.1365 - val_acc: 0.9750\n",
      "Epoch 207/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0761 - acc: 0.9938 - val_loss: 0.1352 - val_acc: 0.9750\n",
      "Epoch 208/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0751 - acc: 0.9938 - val_loss: 0.1342 - val_acc: 0.9750\n",
      "Epoch 209/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0743 - acc: 0.9938 - val_loss: 0.1330 - val_acc: 0.9750\n",
      "Epoch 210/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0734 - acc: 0.9938 - val_loss: 0.1325 - val_acc: 0.9750\n",
      "Epoch 211/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0727 - acc: 0.9938 - val_loss: 0.1312 - val_acc: 0.9750\n",
      "Epoch 212/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0717 - acc: 0.9938 - val_loss: 0.1301 - val_acc: 0.9750\n",
      "Epoch 213/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0709 - acc: 0.9938 - val_loss: 0.1291 - val_acc: 0.9750\n",
      "Epoch 214/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0701 - acc: 0.9938 - val_loss: 0.1283 - val_acc: 0.9750\n",
      "Epoch 215/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0693 - acc: 0.9938 - val_loss: 0.1275 - val_acc: 0.9750\n",
      "Epoch 216/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0686 - acc: 0.9938 - val_loss: 0.1268 - val_acc: 0.9750\n",
      "Epoch 217/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0678 - acc: 0.9938 - val_loss: 0.1259 - val_acc: 0.9750\n",
      "Epoch 218/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0670 - acc: 0.9938 - val_loss: 0.1251 - val_acc: 0.9750\n",
      "Epoch 219/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0664 - acc: 0.9938 - val_loss: 0.1243 - val_acc: 0.9750\n",
      "Epoch 220/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0657 - acc: 0.9938 - val_loss: 0.1234 - val_acc: 0.9750\n",
      "Epoch 221/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0649 - acc: 0.9938 - val_loss: 0.1228 - val_acc: 0.9750\n",
      "Epoch 222/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0643 - acc: 0.9938 - val_loss: 0.1222 - val_acc: 0.9750\n",
      "Epoch 223/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0636 - acc: 0.9938 - val_loss: 0.1213 - val_acc: 0.9750\n",
      "Epoch 224/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0629 - acc: 0.9938 - val_loss: 0.1208 - val_acc: 0.9750\n",
      "Epoch 225/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0623 - acc: 0.9938 - val_loss: 0.1199 - val_acc: 0.9750\n",
      "Epoch 226/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0617 - acc: 0.9938 - val_loss: 0.1192 - val_acc: 0.9750\n",
      "Epoch 227/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0611 - acc: 0.9938 - val_loss: 0.1181 - val_acc: 0.9750\n",
      "Epoch 228/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0605 - acc: 0.9938 - val_loss: 0.1177 - val_acc: 0.9750\n",
      "Epoch 229/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0598 - acc: 0.9938 - val_loss: 0.1168 - val_acc: 0.9750\n",
      "Epoch 230/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0593 - acc: 0.9938 - val_loss: 0.1160 - val_acc: 0.9750\n",
      "Epoch 231/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0587 - acc: 0.9938 - val_loss: 0.1152 - val_acc: 0.9750\n",
      "Epoch 232/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0581 - acc: 0.9938 - val_loss: 0.1149 - val_acc: 0.9750\n",
      "Epoch 233/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0576 - acc: 0.9938 - val_loss: 0.1141 - val_acc: 0.9750\n",
      "Epoch 234/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0570 - acc: 0.9938 - val_loss: 0.1136 - val_acc: 0.9750\n",
      "Epoch 235/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0565 - acc: 0.9938 - val_loss: 0.1129 - val_acc: 0.9750\n",
      "Epoch 236/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0560 - acc: 0.9938 - val_loss: 0.1123 - val_acc: 0.9750\n",
      "Epoch 237/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0555 - acc: 0.9938 - val_loss: 0.1111 - val_acc: 0.9750\n",
      "Epoch 238/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0549 - acc: 0.9938 - val_loss: 0.1105 - val_acc: 0.9750\n",
      "Epoch 239/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 82us/step - loss: 0.0544 - acc: 0.9938 - val_loss: 0.1099 - val_acc: 0.9750\n",
      "Epoch 240/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0539 - acc: 0.9938 - val_loss: 0.1093 - val_acc: 0.9750\n",
      "Epoch 241/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0534 - acc: 0.9938 - val_loss: 0.1086 - val_acc: 0.9750\n",
      "Epoch 242/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0529 - acc: 0.9938 - val_loss: 0.1083 - val_acc: 0.9750\n",
      "Epoch 243/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0525 - acc: 0.9938 - val_loss: 0.1077 - val_acc: 0.9750\n",
      "Epoch 244/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0520 - acc: 0.9938 - val_loss: 0.1073 - val_acc: 0.9750\n",
      "Epoch 245/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0515 - acc: 0.9938 - val_loss: 0.1066 - val_acc: 0.9750\n",
      "Epoch 246/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0511 - acc: 0.9938 - val_loss: 0.1057 - val_acc: 0.9750\n",
      "Epoch 247/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0507 - acc: 0.9938 - val_loss: 0.1055 - val_acc: 0.9750\n",
      "Epoch 248/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0502 - acc: 0.9938 - val_loss: 0.1047 - val_acc: 0.9750\n",
      "Epoch 249/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0497 - acc: 0.9938 - val_loss: 0.1042 - val_acc: 0.9750\n",
      "Epoch 250/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0493 - acc: 0.9938 - val_loss: 0.1036 - val_acc: 0.9750\n",
      "Epoch 251/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0489 - acc: 0.9938 - val_loss: 0.1033 - val_acc: 0.9750\n",
      "Epoch 252/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0485 - acc: 0.9938 - val_loss: 0.1026 - val_acc: 0.9750\n",
      "Epoch 253/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0481 - acc: 0.9938 - val_loss: 0.1023 - val_acc: 0.9750\n",
      "Epoch 254/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0477 - acc: 0.9938 - val_loss: 0.1016 - val_acc: 0.9750\n",
      "Epoch 255/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0473 - acc: 0.9938 - val_loss: 0.1008 - val_acc: 0.9750\n",
      "Epoch 256/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0469 - acc: 0.9938 - val_loss: 0.1004 - val_acc: 0.9750\n",
      "Epoch 257/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0465 - acc: 0.9938 - val_loss: 0.1000 - val_acc: 0.9750\n",
      "Epoch 258/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0462 - acc: 0.9938 - val_loss: 0.0994 - val_acc: 0.9750\n",
      "Epoch 259/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0457 - acc: 0.9938 - val_loss: 0.0990 - val_acc: 0.9750\n",
      "Epoch 260/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0454 - acc: 0.9938 - val_loss: 0.0984 - val_acc: 0.9750\n",
      "Epoch 261/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0450 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 0.9750\n",
      "Epoch 262/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0447 - acc: 0.9938 - val_loss: 0.0977 - val_acc: 0.9750\n",
      "Epoch 263/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0443 - acc: 1.0000 - val_loss: 0.0971 - val_acc: 0.9750\n",
      "Epoch 264/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0441 - acc: 1.0000 - val_loss: 0.0968 - val_acc: 0.9750\n",
      "Epoch 265/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0438 - acc: 1.0000 - val_loss: 0.0960 - val_acc: 0.9750\n",
      "Epoch 266/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0433 - acc: 1.0000 - val_loss: 0.0959 - val_acc: 0.9750\n",
      "Epoch 267/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0430 - acc: 1.0000 - val_loss: 0.0952 - val_acc: 0.9750\n",
      "Epoch 268/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0426 - acc: 1.0000 - val_loss: 0.0949 - val_acc: 0.9750\n",
      "Epoch 269/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0424 - acc: 1.0000 - val_loss: 0.0945 - val_acc: 0.9750\n",
      "Epoch 270/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0420 - acc: 1.0000 - val_loss: 0.0941 - val_acc: 0.9750\n",
      "Epoch 271/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0417 - acc: 1.0000 - val_loss: 0.0938 - val_acc: 0.9750\n",
      "Epoch 272/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0414 - acc: 1.0000 - val_loss: 0.0929 - val_acc: 0.9750\n",
      "Epoch 273/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0411 - acc: 1.0000 - val_loss: 0.0928 - val_acc: 0.9750\n",
      "Epoch 274/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0408 - acc: 1.0000 - val_loss: 0.0925 - val_acc: 0.9750\n",
      "Epoch 275/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0404 - acc: 1.0000 - val_loss: 0.0919 - val_acc: 0.9750\n",
      "Epoch 276/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0402 - acc: 1.0000 - val_loss: 0.0920 - val_acc: 0.9750\n",
      "Epoch 277/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0399 - acc: 1.0000 - val_loss: 0.0916 - val_acc: 0.9750\n",
      "Epoch 278/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0396 - acc: 1.0000 - val_loss: 0.0909 - val_acc: 0.9750\n",
      "Epoch 279/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0393 - acc: 1.0000 - val_loss: 0.0904 - val_acc: 0.9750\n",
      "Epoch 280/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0390 - acc: 1.0000 - val_loss: 0.0900 - val_acc: 0.9750\n",
      "Epoch 281/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0388 - acc: 1.0000 - val_loss: 0.0895 - val_acc: 0.9750\n",
      "Epoch 282/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0384 - acc: 1.0000 - val_loss: 0.0889 - val_acc: 0.9750\n",
      "Epoch 283/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0383 - acc: 1.0000 - val_loss: 0.0887 - val_acc: 0.9750\n",
      "Epoch 284/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0379 - acc: 1.0000 - val_loss: 0.0885 - val_acc: 0.9750\n",
      "Epoch 285/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0376 - acc: 1.0000 - val_loss: 0.0882 - val_acc: 0.9750\n",
      "Epoch 286/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0375 - acc: 1.0000 - val_loss: 0.0878 - val_acc: 0.9750\n",
      "Epoch 287/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0371 - acc: 1.0000 - val_loss: 0.0872 - val_acc: 0.9750\n",
      "Epoch 288/800\n",
      "160/160 [==============================] - 0s 121us/step - loss: 0.0369 - acc: 1.0000 - val_loss: 0.0873 - val_acc: 0.9750\n",
      "Epoch 289/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0366 - acc: 1.0000 - val_loss: 0.0872 - val_acc: 0.9750\n",
      "Epoch 290/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0364 - acc: 1.0000 - val_loss: 0.0864 - val_acc: 0.9750\n",
      "Epoch 291/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0361 - acc: 1.0000 - val_loss: 0.0860 - val_acc: 0.9750\n",
      "Epoch 292/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0359 - acc: 1.0000 - val_loss: 0.0859 - val_acc: 0.9750\n",
      "Epoch 293/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0357 - acc: 1.0000 - val_loss: 0.0858 - val_acc: 0.9750\n",
      "Epoch 294/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0354 - acc: 1.0000 - val_loss: 0.0853 - val_acc: 0.9750\n",
      "Epoch 295/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0352 - acc: 1.0000 - val_loss: 0.0852 - val_acc: 0.9750\n",
      "Epoch 296/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0350 - acc: 1.0000 - val_loss: 0.0848 - val_acc: 0.9750\n",
      "Epoch 297/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0347 - acc: 1.0000 - val_loss: 0.0844 - val_acc: 0.9750\n",
      "Epoch 298/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0345 - acc: 1.0000 - val_loss: 0.0843 - val_acc: 0.9750\n",
      "Epoch 299/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 52us/step - loss: 0.0342 - acc: 1.0000 - val_loss: 0.0840 - val_acc: 0.9750\n",
      "Epoch 300/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0341 - acc: 1.0000 - val_loss: 0.0836 - val_acc: 0.9750\n",
      "Epoch 301/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0338 - acc: 1.0000 - val_loss: 0.0833 - val_acc: 0.9750\n",
      "Epoch 302/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0335 - acc: 1.0000 - val_loss: 0.0828 - val_acc: 0.9750\n",
      "Epoch 303/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0334 - acc: 1.0000 - val_loss: 0.0825 - val_acc: 0.9750\n",
      "Epoch 304/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0332 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 0.9750\n",
      "Epoch 305/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0329 - acc: 1.0000 - val_loss: 0.0818 - val_acc: 0.9750\n",
      "Epoch 306/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0327 - acc: 1.0000 - val_loss: 0.0815 - val_acc: 0.9750\n",
      "Epoch 307/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0325 - acc: 1.0000 - val_loss: 0.0817 - val_acc: 0.9750\n",
      "Epoch 308/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0323 - acc: 1.0000 - val_loss: 0.0815 - val_acc: 0.9750\n",
      "Epoch 309/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0321 - acc: 1.0000 - val_loss: 0.0815 - val_acc: 0.9750\n",
      "Epoch 310/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0319 - acc: 1.0000 - val_loss: 0.0809 - val_acc: 0.9750\n",
      "Epoch 311/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0317 - acc: 1.0000 - val_loss: 0.0809 - val_acc: 0.9750\n",
      "Epoch 312/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0315 - acc: 1.0000 - val_loss: 0.0808 - val_acc: 0.9750\n",
      "Epoch 313/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0313 - acc: 1.0000 - val_loss: 0.0804 - val_acc: 0.9750\n",
      "Epoch 314/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0311 - acc: 1.0000 - val_loss: 0.0798 - val_acc: 0.9750\n",
      "Epoch 315/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0309 - acc: 1.0000 - val_loss: 0.0794 - val_acc: 0.9750\n",
      "Epoch 316/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0307 - acc: 1.0000 - val_loss: 0.0791 - val_acc: 0.9750\n",
      "Epoch 317/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0305 - acc: 1.0000 - val_loss: 0.0790 - val_acc: 0.9750\n",
      "Epoch 318/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0303 - acc: 1.0000 - val_loss: 0.0788 - val_acc: 0.9750\n",
      "Epoch 319/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0302 - acc: 1.0000 - val_loss: 0.0788 - val_acc: 0.9750\n",
      "Epoch 320/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0300 - acc: 1.0000 - val_loss: 0.0784 - val_acc: 0.9750\n",
      "Epoch 321/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0298 - acc: 1.0000 - val_loss: 0.0782 - val_acc: 0.9750\n",
      "Epoch 322/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0296 - acc: 1.0000 - val_loss: 0.0781 - val_acc: 0.9750\n",
      "Epoch 323/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0294 - acc: 1.0000 - val_loss: 0.0777 - val_acc: 0.9750\n",
      "Epoch 324/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0292 - acc: 1.0000 - val_loss: 0.0775 - val_acc: 0.9750\n",
      "Epoch 325/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0291 - acc: 1.0000 - val_loss: 0.0771 - val_acc: 0.9750\n",
      "Epoch 326/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0289 - acc: 1.0000 - val_loss: 0.0772 - val_acc: 0.9750\n",
      "Epoch 327/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0287 - acc: 1.0000 - val_loss: 0.0768 - val_acc: 0.9750\n",
      "Epoch 328/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0285 - acc: 1.0000 - val_loss: 0.0766 - val_acc: 0.9750\n",
      "Epoch 329/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0284 - acc: 1.0000 - val_loss: 0.0763 - val_acc: 0.9750\n",
      "Epoch 330/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0282 - acc: 1.0000 - val_loss: 0.0762 - val_acc: 0.9750\n",
      "Epoch 331/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0280 - acc: 1.0000 - val_loss: 0.0758 - val_acc: 0.9750\n",
      "Epoch 332/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0279 - acc: 1.0000 - val_loss: 0.0755 - val_acc: 0.9750\n",
      "Epoch 333/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0277 - acc: 1.0000 - val_loss: 0.0752 - val_acc: 0.9750\n",
      "Epoch 334/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0275 - acc: 1.0000 - val_loss: 0.0749 - val_acc: 0.9750\n",
      "Epoch 335/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0275 - acc: 1.0000 - val_loss: 0.0748 - val_acc: 0.9750\n",
      "Epoch 336/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0272 - acc: 1.0000 - val_loss: 0.0746 - val_acc: 0.9750\n",
      "Epoch 337/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0271 - acc: 1.0000 - val_loss: 0.0742 - val_acc: 0.9750\n",
      "Epoch 338/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0269 - acc: 1.0000 - val_loss: 0.0741 - val_acc: 0.9750\n",
      "Epoch 339/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0268 - acc: 1.0000 - val_loss: 0.0737 - val_acc: 0.9750\n",
      "Epoch 340/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0266 - acc: 1.0000 - val_loss: 0.0738 - val_acc: 0.9750\n",
      "Epoch 341/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0264 - acc: 1.0000 - val_loss: 0.0738 - val_acc: 0.9750\n",
      "Epoch 342/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0263 - acc: 1.0000 - val_loss: 0.0736 - val_acc: 0.9750\n",
      "Epoch 343/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0262 - acc: 1.0000 - val_loss: 0.0737 - val_acc: 0.9750\n",
      "Epoch 344/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0260 - acc: 1.0000 - val_loss: 0.0733 - val_acc: 0.9750\n",
      "Epoch 345/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0260 - acc: 1.0000 - val_loss: 0.0728 - val_acc: 0.9750\n",
      "Epoch 346/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0257 - acc: 1.0000 - val_loss: 0.0729 - val_acc: 0.9750\n",
      "Epoch 347/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0256 - acc: 1.0000 - val_loss: 0.0728 - val_acc: 0.9750\n",
      "Epoch 348/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0254 - acc: 1.0000 - val_loss: 0.0726 - val_acc: 0.9750\n",
      "Epoch 349/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0253 - acc: 1.0000 - val_loss: 0.0724 - val_acc: 0.9750\n",
      "Epoch 350/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0251 - acc: 1.0000 - val_loss: 0.0723 - val_acc: 0.9750\n",
      "Epoch 351/800\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.0250 - acc: 1.0000 - val_loss: 0.0717 - val_acc: 0.9750\n",
      "Epoch 352/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0249 - acc: 1.0000 - val_loss: 0.0718 - val_acc: 0.9750\n",
      "Epoch 353/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0247 - acc: 1.0000 - val_loss: 0.0718 - val_acc: 0.9750\n",
      "Epoch 354/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0246 - acc: 1.0000 - val_loss: 0.0715 - val_acc: 0.9750\n",
      "Epoch 355/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0244 - acc: 1.0000 - val_loss: 0.0712 - val_acc: 0.9750\n",
      "Epoch 356/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0244 - acc: 1.0000 - val_loss: 0.0712 - val_acc: 0.9750\n",
      "Epoch 357/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0242 - acc: 1.0000 - val_loss: 0.0709 - val_acc: 0.9750\n",
      "Epoch 358/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0241 - acc: 1.0000 - val_loss: 0.0709 - val_acc: 0.9750\n",
      "Epoch 359/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 76us/step - loss: 0.0239 - acc: 1.0000 - val_loss: 0.0704 - val_acc: 0.9750\n",
      "Epoch 360/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0238 - acc: 1.0000 - val_loss: 0.0704 - val_acc: 0.9750\n",
      "Epoch 361/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0237 - acc: 1.0000 - val_loss: 0.0701 - val_acc: 0.9750\n",
      "Epoch 362/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0235 - acc: 1.0000 - val_loss: 0.0699 - val_acc: 0.9750\n",
      "Epoch 363/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0234 - acc: 1.0000 - val_loss: 0.0696 - val_acc: 0.9750\n",
      "Epoch 364/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0233 - acc: 1.0000 - val_loss: 0.0695 - val_acc: 0.9750\n",
      "Epoch 365/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0232 - acc: 1.0000 - val_loss: 0.0695 - val_acc: 0.9750\n",
      "Epoch 366/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0230 - acc: 1.0000 - val_loss: 0.0695 - val_acc: 0.9750\n",
      "Epoch 367/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0229 - acc: 1.0000 - val_loss: 0.0691 - val_acc: 0.9750\n",
      "Epoch 368/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0228 - acc: 1.0000 - val_loss: 0.0691 - val_acc: 0.9750\n",
      "Epoch 369/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0227 - acc: 1.0000 - val_loss: 0.0690 - val_acc: 0.9750\n",
      "Epoch 370/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0226 - acc: 1.0000 - val_loss: 0.0685 - val_acc: 0.9750\n",
      "Epoch 371/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0225 - acc: 1.0000 - val_loss: 0.0684 - val_acc: 0.9750\n",
      "Epoch 372/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0223 - acc: 1.0000 - val_loss: 0.0685 - val_acc: 0.9750\n",
      "Epoch 373/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0222 - acc: 1.0000 - val_loss: 0.0683 - val_acc: 0.9750\n",
      "Epoch 374/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0221 - acc: 1.0000 - val_loss: 0.0681 - val_acc: 0.9750\n",
      "Epoch 375/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0220 - acc: 1.0000 - val_loss: 0.0679 - val_acc: 0.9750\n",
      "Epoch 376/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0219 - acc: 1.0000 - val_loss: 0.0680 - val_acc: 0.9750\n",
      "Epoch 377/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0218 - acc: 1.0000 - val_loss: 0.0678 - val_acc: 0.9750\n",
      "Epoch 378/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0216 - acc: 1.0000 - val_loss: 0.0675 - val_acc: 0.9750\n",
      "Epoch 379/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0215 - acc: 1.0000 - val_loss: 0.0674 - val_acc: 0.9750\n",
      "Epoch 380/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0215 - acc: 1.0000 - val_loss: 0.0669 - val_acc: 0.9750\n",
      "Epoch 381/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0214 - acc: 1.0000 - val_loss: 0.0667 - val_acc: 0.9750\n",
      "Epoch 382/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0213 - acc: 1.0000 - val_loss: 0.0670 - val_acc: 0.9750\n",
      "Epoch 383/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0212 - acc: 1.0000 - val_loss: 0.0669 - val_acc: 0.9750\n",
      "Epoch 384/800\n",
      "160/160 [==============================] - 0s 116us/step - loss: 0.0211 - acc: 1.0000 - val_loss: 0.0670 - val_acc: 0.9750\n",
      "Epoch 385/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0210 - acc: 1.0000 - val_loss: 0.0667 - val_acc: 0.9750\n",
      "Epoch 386/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0208 - acc: 1.0000 - val_loss: 0.0664 - val_acc: 0.9750\n",
      "Epoch 387/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0208 - acc: 1.0000 - val_loss: 0.0660 - val_acc: 0.9750\n",
      "Epoch 388/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0206 - acc: 1.0000 - val_loss: 0.0660 - val_acc: 0.9750\n",
      "Epoch 389/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.0658 - val_acc: 0.9750\n",
      "Epoch 390/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0204 - acc: 1.0000 - val_loss: 0.0659 - val_acc: 0.9750\n",
      "Epoch 391/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0203 - acc: 1.0000 - val_loss: 0.0659 - val_acc: 0.9750\n",
      "Epoch 392/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0202 - acc: 1.0000 - val_loss: 0.0657 - val_acc: 0.9750\n",
      "Epoch 393/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.0656 - val_acc: 0.9750\n",
      "Epoch 394/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0200 - acc: 1.0000 - val_loss: 0.0654 - val_acc: 0.9750\n",
      "Epoch 395/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0199 - acc: 1.0000 - val_loss: 0.0655 - val_acc: 0.9750\n",
      "Epoch 396/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.0651 - val_acc: 0.9750\n",
      "Epoch 397/800\n",
      "160/160 [==============================] - 0s 111us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 0.0647 - val_acc: 0.9750\n",
      "Epoch 398/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0196 - acc: 1.0000 - val_loss: 0.0649 - val_acc: 0.9750\n",
      "Epoch 399/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.0646 - val_acc: 0.9750\n",
      "Epoch 400/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0194 - acc: 1.0000 - val_loss: 0.0643 - val_acc: 0.9750\n",
      "Epoch 401/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.0644 - val_acc: 0.9750\n",
      "Epoch 402/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.0646 - val_acc: 0.9750\n",
      "Epoch 403/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0192 - acc: 1.0000 - val_loss: 0.0648 - val_acc: 0.9750\n",
      "Epoch 404/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0191 - acc: 1.0000 - val_loss: 0.0648 - val_acc: 0.9750\n",
      "Epoch 405/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.0644 - val_acc: 0.9750\n",
      "Epoch 406/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.0638 - val_acc: 0.9750\n",
      "Epoch 407/800\n",
      "160/160 [==============================] - 0s 142us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.0636 - val_acc: 0.9750\n",
      "Epoch 408/800\n",
      "160/160 [==============================] - 0s 187us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.0635 - val_acc: 0.9750\n",
      "Epoch 409/800\n",
      "160/160 [==============================] - 0s 116us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.0636 - val_acc: 0.9750\n",
      "Epoch 410/800\n",
      "160/160 [==============================] - 0s 125us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.0635 - val_acc: 0.9750\n",
      "Epoch 411/800\n",
      "160/160 [==============================] - 0s 109us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9750\n",
      "Epoch 412/800\n",
      "160/160 [==============================] - 0s 128us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.0636 - val_acc: 0.9750\n",
      "Epoch 413/800\n",
      "160/160 [==============================] - 0s 149us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 0.9750\n",
      "Epoch 414/800\n",
      "160/160 [==============================] - 0s 140us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9750\n",
      "Epoch 415/800\n",
      "160/160 [==============================] - 0s 133us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.0629 - val_acc: 0.9750\n",
      "Epoch 416/800\n",
      "160/160 [==============================] - 0s 106us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9750\n",
      "Epoch 417/800\n",
      "160/160 [==============================] - 0s 115us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 0.9750\n",
      "Epoch 418/800\n",
      "160/160 [==============================] - 0s 142us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9750\n",
      "Epoch 419/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 118us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.0627 - val_acc: 0.9750\n",
      "Epoch 420/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.0626 - val_acc: 0.9750\n",
      "Epoch 421/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.0627 - val_acc: 0.9750\n",
      "Epoch 422/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.0628 - val_acc: 0.9750\n",
      "Epoch 423/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.0628 - val_acc: 0.9750\n",
      "Epoch 424/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.0625 - val_acc: 0.9750\n",
      "Epoch 425/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.0623 - val_acc: 0.9750\n",
      "Epoch 426/800\n",
      "160/160 [==============================] - 0s 106us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.0622 - val_acc: 0.9750\n",
      "Epoch 427/800\n",
      "160/160 [==============================] - 0s 107us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.0619 - val_acc: 0.9750\n",
      "Epoch 428/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0615 - val_acc: 0.9750\n",
      "Epoch 429/800\n",
      "160/160 [==============================] - 0s 129us/step - loss: 0.0170 - acc: 1.0000 - val_loss: 0.0614 - val_acc: 0.9750\n",
      "Epoch 430/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 0.0611 - val_acc: 0.9750\n",
      "Epoch 431/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.0609 - val_acc: 0.9750\n",
      "Epoch 432/800\n",
      "160/160 [==============================] - 0s 105us/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.0609 - val_acc: 0.9750\n",
      "Epoch 433/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0167 - acc: 1.0000 - val_loss: 0.0610 - val_acc: 0.9750\n",
      "Epoch 434/800\n",
      "160/160 [==============================] - 0s 121us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.0611 - val_acc: 0.9750\n",
      "Epoch 435/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.0610 - val_acc: 0.9750\n",
      "Epoch 436/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.0612 - val_acc: 0.9750\n",
      "Epoch 437/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0164 - acc: 1.0000 - val_loss: 0.0610 - val_acc: 0.9750\n",
      "Epoch 438/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.0607 - val_acc: 0.9750\n",
      "Epoch 439/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.0609 - val_acc: 0.9750\n",
      "Epoch 440/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.0610 - val_acc: 0.9750\n",
      "Epoch 441/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.0608 - val_acc: 0.9750\n",
      "Epoch 442/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.0608 - val_acc: 0.9750\n",
      "Epoch 443/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.0602 - val_acc: 0.9750\n",
      "Epoch 444/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.0599 - val_acc: 0.9750\n",
      "Epoch 445/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 0.9750\n",
      "Epoch 446/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.0597 - val_acc: 0.9750\n",
      "Epoch 447/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 0.9750\n",
      "Epoch 448/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.0600 - val_acc: 0.9750\n",
      "Epoch 449/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.0600 - val_acc: 0.9750\n",
      "Epoch 450/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0155 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 0.9750\n",
      "Epoch 451/800\n",
      "160/160 [==============================] - 0s 54us/step - loss: 0.0154 - acc: 1.0000 - val_loss: 0.0597 - val_acc: 0.9750\n",
      "Epoch 452/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 0.0596 - val_acc: 0.9750\n",
      "Epoch 453/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 0.9750\n",
      "Epoch 454/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 0.0596 - val_acc: 0.9750\n",
      "Epoch 455/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.0595 - val_acc: 0.9750\n",
      "Epoch 456/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.0594 - val_acc: 0.9750\n",
      "Epoch 457/800\n",
      "160/160 [==============================] - 0s 116us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 0.0594 - val_acc: 0.9750\n",
      "Epoch 458/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 0.0592 - val_acc: 0.9750\n",
      "Epoch 459/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0149 - acc: 1.0000 - val_loss: 0.0588 - val_acc: 0.9750\n",
      "Epoch 460/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.0587 - val_acc: 0.9750\n",
      "Epoch 461/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.0588 - val_acc: 0.9750\n",
      "Epoch 462/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.0589 - val_acc: 0.9750\n",
      "Epoch 463/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0146 - acc: 1.0000 - val_loss: 0.0589 - val_acc: 0.9750\n",
      "Epoch 464/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0146 - acc: 1.0000 - val_loss: 0.0590 - val_acc: 0.9750\n",
      "Epoch 465/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 0.0588 - val_acc: 0.9750\n",
      "Epoch 466/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 0.0586 - val_acc: 0.9750\n",
      "Epoch 467/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.0586 - val_acc: 0.9750\n",
      "Epoch 468/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.0582 - val_acc: 0.9750\n",
      "Epoch 469/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.0583 - val_acc: 0.9750\n",
      "Epoch 470/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.0582 - val_acc: 0.9750\n",
      "Epoch 471/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.0580 - val_acc: 0.9750\n",
      "Epoch 472/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.0141 - acc: 1.0000 - val_loss: 0.0580 - val_acc: 0.9750\n",
      "Epoch 473/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0141 - acc: 1.0000 - val_loss: 0.0582 - val_acc: 0.9750\n",
      "Epoch 474/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 0.0579 - val_acc: 0.9750\n",
      "Epoch 475/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0139 - acc: 1.0000 - val_loss: 0.0581 - val_acc: 0.9750\n",
      "Epoch 476/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0139 - acc: 1.0000 - val_loss: 0.0581 - val_acc: 0.9750\n",
      "Epoch 477/800\n",
      "160/160 [==============================] - 0s 164us/step - loss: 0.0138 - acc: 1.0000 - val_loss: 0.0574 - val_acc: 0.9750\n",
      "Epoch 478/800\n",
      "160/160 [==============================] - 0s 117us/step - loss: 0.0138 - acc: 1.0000 - val_loss: 0.0577 - val_acc: 0.9750\n",
      "Epoch 479/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 246us/step - loss: 0.0137 - acc: 1.0000 - val_loss: 0.0575 - val_acc: 0.9750\n",
      "Epoch 480/800\n",
      "160/160 [==============================] - 0s 131us/step - loss: 0.0137 - acc: 1.0000 - val_loss: 0.0573 - val_acc: 0.9750\n",
      "Epoch 481/800\n",
      "160/160 [==============================] - 0s 148us/step - loss: 0.0137 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 0.9750\n",
      "Epoch 482/800\n",
      "160/160 [==============================] - 0s 143us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 0.9750\n",
      "Epoch 483/800\n",
      "160/160 [==============================] - 0s 109us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 0.0573 - val_acc: 0.9750\n",
      "Epoch 484/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 0.9750\n",
      "Epoch 485/800\n",
      "160/160 [==============================] - 0s 146us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.0573 - val_acc: 0.9750\n",
      "Epoch 486/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.0571 - val_acc: 0.9750\n",
      "Epoch 487/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.0568 - val_acc: 0.9750\n",
      "Epoch 488/800\n",
      "160/160 [==============================] - 0s 104us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.0569 - val_acc: 0.9750\n",
      "Epoch 489/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.0570 - val_acc: 0.9750\n",
      "Epoch 490/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.0568 - val_acc: 0.9750\n",
      "Epoch 491/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.0568 - val_acc: 0.9750\n",
      "Epoch 492/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.0565 - val_acc: 0.9750\n",
      "Epoch 493/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.0567 - val_acc: 0.9750\n",
      "Epoch 494/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.0566 - val_acc: 0.9750\n",
      "Epoch 495/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.0564 - val_acc: 0.9750\n",
      "Epoch 496/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.0567 - val_acc: 0.9750\n",
      "Epoch 497/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.0567 - val_acc: 0.9750\n",
      "Epoch 498/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.0565 - val_acc: 0.9750\n",
      "Epoch 499/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.0562 - val_acc: 0.9750\n",
      "Epoch 500/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.0561 - val_acc: 0.9750\n",
      "Epoch 501/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.0561 - val_acc: 0.9750\n",
      "Epoch 502/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.0561 - val_acc: 0.9750\n",
      "Epoch 503/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.0559 - val_acc: 0.9750\n",
      "Epoch 504/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.0560 - val_acc: 0.9750\n",
      "Epoch 505/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.0560 - val_acc: 0.9750\n",
      "Epoch 506/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.0558 - val_acc: 0.9750\n",
      "Epoch 507/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.0555 - val_acc: 0.9750\n",
      "Epoch 508/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.0554 - val_acc: 0.9750\n",
      "Epoch 509/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.0557 - val_acc: 0.9750\n",
      "Epoch 510/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.0558 - val_acc: 0.9750\n",
      "Epoch 511/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.0558 - val_acc: 0.9750\n",
      "Epoch 512/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.0556 - val_acc: 0.9750\n",
      "Epoch 513/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.0554 - val_acc: 0.9750\n",
      "Epoch 514/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.0554 - val_acc: 0.9750\n",
      "Epoch 515/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.0553 - val_acc: 0.9750\n",
      "Epoch 516/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 0.9750\n",
      "Epoch 517/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.0554 - val_acc: 0.9750\n",
      "Epoch 518/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0118 - acc: 1.0000 - val_loss: 0.0552 - val_acc: 0.9750\n",
      "Epoch 519/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0118 - acc: 1.0000 - val_loss: 0.0550 - val_acc: 0.9750\n",
      "Epoch 520/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.0549 - val_acc: 0.9750\n",
      "Epoch 521/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.0547 - val_acc: 0.9750\n",
      "Epoch 522/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.0549 - val_acc: 0.9750\n",
      "Epoch 523/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 0.9750\n",
      "Epoch 524/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 0.9750\n",
      "Epoch 525/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.0550 - val_acc: 0.9750\n",
      "Epoch 526/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.0549 - val_acc: 0.9750\n",
      "Epoch 527/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.0550 - val_acc: 0.9750\n",
      "Epoch 528/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.0550 - val_acc: 0.9750\n",
      "Epoch 529/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.0549 - val_acc: 0.9750\n",
      "Epoch 530/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.0547 - val_acc: 0.9750\n",
      "Epoch 531/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 0.9750\n",
      "Epoch 532/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 0.9750\n",
      "Epoch 533/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.0544 - val_acc: 0.9750\n",
      "Epoch 534/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 0.9750\n",
      "Epoch 535/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 0.9750\n",
      "Epoch 536/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 0.9750\n",
      "Epoch 537/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 0.0543 - val_acc: 0.9750\n",
      "Epoch 538/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.0541 - val_acc: 0.9750\n",
      "Epoch 539/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 90us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.0540 - val_acc: 0.9750\n",
      "Epoch 540/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0108 - acc: 1.0000 - val_loss: 0.0541 - val_acc: 0.9750\n",
      "Epoch 541/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0108 - acc: 1.0000 - val_loss: 0.0539 - val_acc: 0.9750\n",
      "Epoch 542/800\n",
      "160/160 [==============================] - 0s 126us/step - loss: 0.0108 - acc: 1.0000 - val_loss: 0.0539 - val_acc: 0.9750\n",
      "Epoch 543/800\n",
      "160/160 [==============================] - 0s 120us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.0540 - val_acc: 0.9750\n",
      "Epoch 544/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.0537 - val_acc: 0.9750\n",
      "Epoch 545/800\n",
      "160/160 [==============================] - 0s 107us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.0538 - val_acc: 0.9750\n",
      "Epoch 546/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.0537 - val_acc: 0.9750\n",
      "Epoch 547/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.0539 - val_acc: 0.9750\n",
      "Epoch 548/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.0539 - val_acc: 0.9750\n",
      "Epoch 549/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.0536 - val_acc: 0.9750\n",
      "Epoch 550/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.0534 - val_acc: 0.9750\n",
      "Epoch 551/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.0532 - val_acc: 0.9750\n",
      "Epoch 552/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.0536 - val_acc: 0.9750\n",
      "Epoch 553/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.0534 - val_acc: 0.9750\n",
      "Epoch 554/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.0534 - val_acc: 0.9750\n",
      "Epoch 555/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.0532 - val_acc: 0.9750\n",
      "Epoch 556/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 0.0533 - val_acc: 0.9750\n",
      "Epoch 557/800\n",
      "160/160 [==============================] - 0s 111us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 0.0532 - val_acc: 0.9750\n",
      "Epoch 558/800\n",
      "160/160 [==============================] - 0s 114us/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.0532 - val_acc: 0.9750\n",
      "Epoch 559/800\n",
      "160/160 [==============================] - 0s 130us/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.0530 - val_acc: 0.9750\n",
      "Epoch 560/800\n",
      "160/160 [==============================] - 0s 125us/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.0532 - val_acc: 0.9750\n",
      "Epoch 561/800\n",
      "160/160 [==============================] - 0s 114us/step - loss: 0.0101 - acc: 1.0000 - val_loss: 0.0531 - val_acc: 0.9750\n",
      "Epoch 562/800\n",
      "160/160 [==============================] - 0s 99us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.0529 - val_acc: 0.9750\n",
      "Epoch 563/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.0528 - val_acc: 0.9750\n",
      "Epoch 564/800\n",
      "160/160 [==============================] - 0s 112us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.0529 - val_acc: 0.9750\n",
      "Epoch 565/800\n",
      "160/160 [==============================] - 0s 120us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.0530 - val_acc: 0.9750\n",
      "Epoch 566/800\n",
      "160/160 [==============================] - 0s 110us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.0531 - val_acc: 0.9750\n",
      "Epoch 567/800\n",
      "160/160 [==============================] - 0s 118us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.0529 - val_acc: 0.9750\n",
      "Epoch 568/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 0.0530 - val_acc: 0.9750\n",
      "Epoch 569/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 0.0528 - val_acc: 0.9750\n",
      "Epoch 570/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.0527 - val_acc: 0.9750\n",
      "Epoch 571/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.0525 - val_acc: 0.9750\n",
      "Epoch 572/800\n",
      "160/160 [==============================] - 0s 111us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.0527 - val_acc: 0.9750\n",
      "Epoch 573/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 0.0526 - val_acc: 0.9750\n",
      "Epoch 574/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.0524 - val_acc: 0.9750\n",
      "Epoch 575/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.0523 - val_acc: 0.9750\n",
      "Epoch 576/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.0522 - val_acc: 0.9750\n",
      "Epoch 577/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.0523 - val_acc: 0.9750\n",
      "Epoch 578/800\n",
      "160/160 [==============================] - 0s 161us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.0525 - val_acc: 0.9750\n",
      "Epoch 579/800\n",
      "160/160 [==============================] - 0s 139us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.0522 - val_acc: 0.9750\n",
      "Epoch 580/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.0521 - val_acc: 0.9750\n",
      "Epoch 581/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.0520 - val_acc: 0.9750\n",
      "Epoch 582/800\n",
      "160/160 [==============================] - 0s 106us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.0519 - val_acc: 0.9750\n",
      "Epoch 583/800\n",
      "160/160 [==============================] - 0s 135us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 0.0519 - val_acc: 0.9750\n",
      "Epoch 584/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 0.0521 - val_acc: 0.9750\n",
      "Epoch 585/800\n",
      "160/160 [==============================] - 0s 121us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.0524 - val_acc: 0.9750\n",
      "Epoch 586/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.0523 - val_acc: 0.9750\n",
      "Epoch 587/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.0522 - val_acc: 0.9750\n",
      "Epoch 588/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.0519 - val_acc: 0.9750\n",
      "Epoch 589/800\n",
      "160/160 [==============================] - 0s 104us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.0520 - val_acc: 0.9750\n",
      "Epoch 590/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.0519 - val_acc: 0.9750\n",
      "Epoch 591/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.0517 - val_acc: 0.9750\n",
      "Epoch 592/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0517 - val_acc: 0.9750\n",
      "Epoch 593/800\n",
      "160/160 [==============================] - 0s 136us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0516 - val_acc: 0.9750\n",
      "Epoch 594/800\n",
      "160/160 [==============================] - 0s 99us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0518 - val_acc: 0.9750\n",
      "Epoch 595/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.0515 - val_acc: 0.9750\n",
      "Epoch 596/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.0512 - val_acc: 0.9750\n",
      "Epoch 597/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.0513 - val_acc: 0.9750\n",
      "Epoch 598/800\n",
      "160/160 [==============================] - 0s 134us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.0513 - val_acc: 0.9750\n",
      "Epoch 599/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 114us/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.0514 - val_acc: 0.9750\n",
      "Epoch 600/800\n",
      "160/160 [==============================] - 0s 122us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.0514 - val_acc: 0.9750\n",
      "Epoch 601/800\n",
      "160/160 [==============================] - 0s 126us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.0517 - val_acc: 0.9750\n",
      "Epoch 602/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.0514 - val_acc: 0.9750\n",
      "Epoch 603/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.0514 - val_acc: 0.9750\n",
      "Epoch 604/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.0516 - val_acc: 0.9750\n",
      "Epoch 605/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.0516 - val_acc: 0.9750\n",
      "Epoch 606/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.0514 - val_acc: 0.9750\n",
      "Epoch 607/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.0512 - val_acc: 0.9750\n",
      "Epoch 608/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.0511 - val_acc: 0.9750\n",
      "Epoch 609/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.0514 - val_acc: 0.9750\n",
      "Epoch 610/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.0514 - val_acc: 0.9750\n",
      "Epoch 611/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.0510 - val_acc: 0.9750\n",
      "Epoch 612/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.0510 - val_acc: 0.9750\n",
      "Epoch 613/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.0511 - val_acc: 0.9750\n",
      "Epoch 614/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.0511 - val_acc: 0.9750\n",
      "Epoch 615/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.0512 - val_acc: 0.9750\n",
      "Epoch 616/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.0508 - val_acc: 0.9750\n",
      "Epoch 617/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.0507 - val_acc: 0.9750\n",
      "Epoch 618/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.0507 - val_acc: 0.9750\n",
      "Epoch 619/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.0506 - val_acc: 0.9750\n",
      "Epoch 620/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.0507 - val_acc: 0.9750\n",
      "Epoch 621/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.0507 - val_acc: 0.9750\n",
      "Epoch 622/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.0509 - val_acc: 0.9750\n",
      "Epoch 623/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.0504 - val_acc: 0.9750\n",
      "Epoch 624/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.0505 - val_acc: 0.9750\n",
      "Epoch 625/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.0505 - val_acc: 0.9750\n",
      "Epoch 626/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.0504 - val_acc: 0.9750\n",
      "Epoch 627/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.0508 - val_acc: 0.9750\n",
      "Epoch 628/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.0506 - val_acc: 0.9750\n",
      "Epoch 629/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.0507 - val_acc: 0.9750\n",
      "Epoch 630/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.0506 - val_acc: 0.9750\n",
      "Epoch 631/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.0508 - val_acc: 0.9750\n",
      "Epoch 632/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.0506 - val_acc: 0.9750\n",
      "Epoch 633/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.0504 - val_acc: 0.9750\n",
      "Epoch 634/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.0502 - val_acc: 0.9750\n",
      "Epoch 635/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.0503 - val_acc: 0.9750\n",
      "Epoch 636/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.0501 - val_acc: 0.9750\n",
      "Epoch 637/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.0502 - val_acc: 0.9750\n",
      "Epoch 638/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.0501 - val_acc: 0.9750\n",
      "Epoch 639/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.0500 - val_acc: 0.9750\n",
      "Epoch 640/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.0499 - val_acc: 0.9750\n",
      "Epoch 641/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.0498 - val_acc: 0.9750\n",
      "Epoch 642/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.0502 - val_acc: 0.9750\n",
      "Epoch 643/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.0501 - val_acc: 0.9750\n",
      "Epoch 644/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0499 - val_acc: 0.9750\n",
      "Epoch 645/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0501 - val_acc: 0.9750\n",
      "Epoch 646/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.0502 - val_acc: 0.9750\n",
      "Epoch 647/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.0498 - val_acc: 0.9750\n",
      "Epoch 648/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.0499 - val_acc: 0.9750\n",
      "Epoch 649/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.0498 - val_acc: 0.9750\n",
      "Epoch 650/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.0498 - val_acc: 0.9750\n",
      "Epoch 651/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0497 - val_acc: 0.9750\n",
      "Epoch 652/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0498 - val_acc: 0.9750\n",
      "Epoch 653/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0498 - val_acc: 0.9750\n",
      "Epoch 654/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0498 - val_acc: 0.9750\n",
      "Epoch 655/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0497 - val_acc: 0.9750\n",
      "Epoch 656/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0500 - val_acc: 0.9750\n",
      "Epoch 657/800\n",
      "160/160 [==============================] - 0s 54us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0495 - val_acc: 0.9750\n",
      "Epoch 658/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0495 - val_acc: 0.9750\n",
      "Epoch 659/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 61us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0494 - val_acc: 0.9750\n",
      "Epoch 660/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0494 - val_acc: 0.9750\n",
      "Epoch 661/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0494 - val_acc: 0.9750\n",
      "Epoch 662/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0495 - val_acc: 0.9750\n",
      "Epoch 663/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0493 - val_acc: 0.9750\n",
      "Epoch 664/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0495 - val_acc: 0.9750\n",
      "Epoch 665/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0495 - val_acc: 0.9750\n",
      "Epoch 666/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0495 - val_acc: 0.9750\n",
      "Epoch 667/800\n",
      "160/160 [==============================] - 0s 54us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0496 - val_acc: 0.9750\n",
      "Epoch 668/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0493 - val_acc: 0.9750\n",
      "Epoch 669/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0492 - val_acc: 0.9750\n",
      "Epoch 670/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0488 - val_acc: 0.9750\n",
      "Epoch 671/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0490 - val_acc: 0.9750\n",
      "Epoch 672/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0490 - val_acc: 0.9750\n",
      "Epoch 673/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.0489 - val_acc: 0.9750\n",
      "Epoch 674/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.0488 - val_acc: 0.9750\n",
      "Epoch 675/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.0489 - val_acc: 0.9750\n",
      "Epoch 676/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.0491 - val_acc: 0.9750\n",
      "Epoch 677/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0488 - val_acc: 0.9750\n",
      "Epoch 678/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0488 - val_acc: 0.9750\n",
      "Epoch 679/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0492 - val_acc: 0.9750\n",
      "Epoch 680/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0492 - val_acc: 0.9750\n",
      "Epoch 681/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0488 - val_acc: 0.9750\n",
      "Epoch 682/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0489 - val_acc: 0.9750\n",
      "Epoch 683/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0491 - val_acc: 0.9750\n",
      "Epoch 684/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0490 - val_acc: 0.9750\n",
      "Epoch 685/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0487 - val_acc: 0.9750\n",
      "Epoch 686/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0488 - val_acc: 0.9750\n",
      "Epoch 687/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0490 - val_acc: 0.9750\n",
      "Epoch 688/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0487 - val_acc: 0.9750\n",
      "Epoch 689/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0483 - val_acc: 0.9750\n",
      "Epoch 690/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0484 - val_acc: 0.9750\n",
      "Epoch 691/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.0482 - val_acc: 0.9750\n",
      "Epoch 692/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 0.0482 - val_acc: 0.9750\n",
      "Epoch 693/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 0.0483 - val_acc: 0.9750\n",
      "Epoch 694/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 0.0484 - val_acc: 0.9750\n",
      "Epoch 695/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 0.0484 - val_acc: 0.9750\n",
      "Epoch 696/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.0485 - val_acc: 0.9750\n",
      "Epoch 697/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.0483 - val_acc: 0.9750\n",
      "Epoch 698/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.0484 - val_acc: 0.9750\n",
      "Epoch 699/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.0485 - val_acc: 0.9750\n",
      "Epoch 700/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.0483 - val_acc: 0.9750\n",
      "Epoch 701/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.0483 - val_acc: 0.9750\n",
      "Epoch 702/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.0482 - val_acc: 0.9750\n",
      "Epoch 703/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.0483 - val_acc: 0.9750\n",
      "Epoch 704/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.0483 - val_acc: 0.9750\n",
      "Epoch 705/800\n",
      "160/160 [==============================] - 0s 120us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.0483 - val_acc: 0.9750\n",
      "Epoch 706/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.0482 - val_acc: 0.9750\n",
      "Epoch 707/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.0480 - val_acc: 0.9750\n",
      "Epoch 708/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.0481 - val_acc: 0.9750\n",
      "Epoch 709/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.0482 - val_acc: 0.9750\n",
      "Epoch 710/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.0479 - val_acc: 0.9750\n",
      "Epoch 711/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.0480 - val_acc: 0.9750\n",
      "Epoch 712/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.0477 - val_acc: 0.9750\n",
      "Epoch 713/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0480 - val_acc: 0.9750\n",
      "Epoch 714/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0480 - val_acc: 0.9750\n",
      "Epoch 715/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0482 - val_acc: 0.9750\n",
      "Epoch 716/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0483 - val_acc: 0.9750\n",
      "Epoch 717/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0485 - val_acc: 0.9750\n",
      "Epoch 718/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0481 - val_acc: 0.9750\n",
      "Epoch 719/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 60us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0480 - val_acc: 0.9750\n",
      "Epoch 720/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0481 - val_acc: 0.9750\n",
      "Epoch 721/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0478 - val_acc: 0.9750\n",
      "Epoch 722/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0478 - val_acc: 0.9750\n",
      "Epoch 723/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0477 - val_acc: 0.9750\n",
      "Epoch 724/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.0476 - val_acc: 0.9750\n",
      "Epoch 725/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.0477 - val_acc: 0.9750\n",
      "Epoch 726/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.0476 - val_acc: 0.9750\n",
      "Epoch 727/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.0475 - val_acc: 0.9750\n",
      "Epoch 728/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.0474 - val_acc: 0.9750\n",
      "Epoch 729/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0476 - val_acc: 0.9750\n",
      "Epoch 730/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0475 - val_acc: 0.9750\n",
      "Epoch 731/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0477 - val_acc: 0.9750\n",
      "Epoch 732/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0475 - val_acc: 0.9750\n",
      "Epoch 733/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0475 - val_acc: 0.9750\n",
      "Epoch 734/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0477 - val_acc: 0.9750\n",
      "Epoch 735/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0478 - val_acc: 0.9750\n",
      "Epoch 736/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0477 - val_acc: 0.9750\n",
      "Epoch 737/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0477 - val_acc: 0.9750\n",
      "Epoch 738/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0474 - val_acc: 0.9750\n",
      "Epoch 739/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0474 - val_acc: 0.9750\n",
      "Epoch 740/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0470 - val_acc: 0.9750\n",
      "Epoch 741/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0472 - val_acc: 0.9750\n",
      "Epoch 742/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0475 - val_acc: 0.9750\n",
      "Epoch 743/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0475 - val_acc: 0.9750\n",
      "Epoch 744/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0474 - val_acc: 0.9750\n",
      "Epoch 745/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0473 - val_acc: 0.9750\n",
      "Epoch 746/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0472 - val_acc: 0.9750\n",
      "Epoch 747/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.0474 - val_acc: 0.9750\n",
      "Epoch 748/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0472 - val_acc: 0.9750\n",
      "Epoch 749/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0471 - val_acc: 0.9750\n",
      "Epoch 750/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0473 - val_acc: 0.9750\n",
      "Epoch 751/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0472 - val_acc: 0.9750\n",
      "Epoch 752/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0472 - val_acc: 0.9750\n",
      "Epoch 753/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0471 - val_acc: 0.9750\n",
      "Epoch 754/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0471 - val_acc: 0.9750\n",
      "Epoch 755/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0470 - val_acc: 0.9750\n",
      "Epoch 756/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0470 - val_acc: 0.9750\n",
      "Epoch 757/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0472 - val_acc: 0.9750\n",
      "Epoch 758/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0472 - val_acc: 0.9750\n",
      "Epoch 759/800\n",
      "160/160 [==============================] - 0s 54us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0473 - val_acc: 0.9750\n",
      "Epoch 760/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0474 - val_acc: 0.9750\n",
      "Epoch 761/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0472 - val_acc: 0.9750\n",
      "Epoch 762/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0470 - val_acc: 0.9750\n",
      "Epoch 763/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0467 - val_acc: 0.9750\n",
      "Epoch 764/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9750\n",
      "Epoch 765/800\n",
      "160/160 [==============================] - 0s 53us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0469 - val_acc: 0.9750\n",
      "Epoch 766/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9750\n",
      "Epoch 767/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9750\n",
      "Epoch 768/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9750\n",
      "Epoch 769/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0469 - val_acc: 0.9750\n",
      "Epoch 770/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9750\n",
      "Epoch 771/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 0.9750\n",
      "Epoch 772/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0469 - val_acc: 0.9750\n",
      "Epoch 773/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0469 - val_acc: 0.9750\n",
      "Epoch 774/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0469 - val_acc: 0.9750\n",
      "Epoch 775/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0469 - val_acc: 0.9750\n",
      "Epoch 776/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9750\n",
      "Epoch 777/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0469 - val_acc: 0.9750\n",
      "Epoch 778/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 0.9750\n",
      "Epoch 779/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 67us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 0.9750\n",
      "Epoch 780/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0467 - val_acc: 0.9750\n",
      "Epoch 781/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9750\n",
      "Epoch 782/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0467 - val_acc: 0.9750\n",
      "Epoch 783/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0467 - val_acc: 0.9750\n",
      "Epoch 784/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0467 - val_acc: 0.9750\n",
      "Epoch 785/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9750\n",
      "Epoch 786/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0471 - val_acc: 0.9750\n",
      "Epoch 787/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0470 - val_acc: 0.9750\n",
      "Epoch 788/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9750\n",
      "Epoch 789/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 0.9750\n",
      "Epoch 790/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0464 - val_acc: 0.9750\n",
      "Epoch 791/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 0.9750\n",
      "Epoch 792/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9750\n",
      "Epoch 793/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0467 - val_acc: 0.9750\n",
      "Epoch 794/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 0.9750\n",
      "Epoch 795/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 0.9750\n",
      "Epoch 796/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0465 - val_acc: 0.9750\n",
      "Epoch 797/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0464 - val_acc: 0.9750\n",
      "Epoch 798/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 0.9750\n",
      "Epoch 799/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0468 - val_acc: 0.9750\n",
      "Epoch 800/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0465 - val_acc: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa96d562ac8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_relu.compile(optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "nn_relu.fit(X, y, epochs=800, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa96c08dcc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_range = np.arange(-5., 5., 0.02)\n",
    "dgrid = len(grid_range)\n",
    "Xpred = np.array([(a, b) for a in grid_range for b in grid_range])\n",
    "prob_pred = nn_relu.predict(Xpred)\n",
    "ypred = np.round(prob_pred)\n",
    "ypred = ypred.reshape((dgrid, dgrid))\n",
    "ypred = np.flipud(ypred)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.imshow(ypred, extent=[-4, 4, -4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ReLU activation function is discontinuous, and it results in a discontinuous decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we could use a tanh activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_tanh = models.Sequential()\n",
    "nn_tanh.add(layers.Dense(5, input_dim=2, activation=\"tanh\"))\n",
    "nn_tanh.add(layers.Dense(5, activation=\"tanh\"))\n",
    "nn_tanh.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/800\n",
      "160/160 [==============================] - 0s 2ms/step - loss: 0.7020 - acc: 0.5187 - val_loss: 0.7393 - val_acc: 0.4000\n",
      "Epoch 2/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.6992 - acc: 0.5187 - val_loss: 0.7374 - val_acc: 0.4000\n",
      "Epoch 3/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.6962 - acc: 0.5250 - val_loss: 0.7361 - val_acc: 0.4000\n",
      "Epoch 4/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.6944 - acc: 0.5312 - val_loss: 0.7346 - val_acc: 0.4250\n",
      "Epoch 5/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.6921 - acc: 0.5375 - val_loss: 0.7334 - val_acc: 0.4500\n",
      "Epoch 6/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.6898 - acc: 0.5375 - val_loss: 0.7327 - val_acc: 0.5000\n",
      "Epoch 7/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.6879 - acc: 0.5375 - val_loss: 0.7316 - val_acc: 0.4750\n",
      "Epoch 8/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.6859 - acc: 0.5563 - val_loss: 0.7306 - val_acc: 0.4750\n",
      "Epoch 9/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.6842 - acc: 0.5563 - val_loss: 0.7299 - val_acc: 0.4750\n",
      "Epoch 10/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.6827 - acc: 0.5625 - val_loss: 0.7293 - val_acc: 0.4750\n",
      "Epoch 11/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.6810 - acc: 0.5563 - val_loss: 0.7283 - val_acc: 0.4750\n",
      "Epoch 12/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.6799 - acc: 0.5500 - val_loss: 0.7279 - val_acc: 0.4750\n",
      "Epoch 13/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.6782 - acc: 0.5687 - val_loss: 0.7268 - val_acc: 0.4750\n",
      "Epoch 14/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.6772 - acc: 0.5750 - val_loss: 0.7271 - val_acc: 0.4750\n",
      "Epoch 15/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.6754 - acc: 0.5875 - val_loss: 0.7267 - val_acc: 0.4750\n",
      "Epoch 16/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.6743 - acc: 0.5875 - val_loss: 0.7265 - val_acc: 0.4750\n",
      "Epoch 17/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.6729 - acc: 0.6000 - val_loss: 0.7261 - val_acc: 0.5000\n",
      "Epoch 18/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.6718 - acc: 0.6063 - val_loss: 0.7258 - val_acc: 0.5000\n",
      "Epoch 19/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.6706 - acc: 0.6250 - val_loss: 0.7258 - val_acc: 0.5000\n",
      "Epoch 20/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.6694 - acc: 0.6250 - val_loss: 0.7246 - val_acc: 0.5250\n",
      "Epoch 21/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.6680 - acc: 0.6312 - val_loss: 0.7244 - val_acc: 0.5250\n",
      "Epoch 22/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.6668 - acc: 0.6250 - val_loss: 0.7242 - val_acc: 0.5250\n",
      "Epoch 23/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.6656 - acc: 0.6250 - val_loss: 0.7240 - val_acc: 0.5250\n",
      "Epoch 24/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.6643 - acc: 0.6312 - val_loss: 0.7236 - val_acc: 0.5250\n",
      "Epoch 25/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.6631 - acc: 0.6375 - val_loss: 0.7231 - val_acc: 0.5500\n",
      "Epoch 26/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.6618 - acc: 0.6437 - val_loss: 0.7226 - val_acc: 0.5750\n",
      "Epoch 27/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.6605 - acc: 0.6437 - val_loss: 0.7222 - val_acc: 0.6000\n",
      "Epoch 28/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.6592 - acc: 0.6500 - val_loss: 0.7215 - val_acc: 0.6000\n",
      "Epoch 29/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.6578 - acc: 0.6562 - val_loss: 0.7208 - val_acc: 0.6000\n",
      "Epoch 30/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.6567 - acc: 0.6688 - val_loss: 0.7208 - val_acc: 0.6000\n",
      "Epoch 31/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.6551 - acc: 0.6750 - val_loss: 0.7203 - val_acc: 0.6000\n",
      "Epoch 32/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.6538 - acc: 0.6813 - val_loss: 0.7195 - val_acc: 0.5750\n",
      "Epoch 33/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.6524 - acc: 0.6813 - val_loss: 0.7195 - val_acc: 0.5750\n",
      "Epoch 34/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.6510 - acc: 0.6750 - val_loss: 0.7187 - val_acc: 0.5500\n",
      "Epoch 35/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.6496 - acc: 0.6813 - val_loss: 0.7186 - val_acc: 0.5500\n",
      "Epoch 36/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.6481 - acc: 0.6813 - val_loss: 0.7169 - val_acc: 0.5250\n",
      "Epoch 37/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.6466 - acc: 0.6750 - val_loss: 0.7167 - val_acc: 0.5250\n",
      "Epoch 38/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.6448 - acc: 0.6750 - val_loss: 0.7157 - val_acc: 0.5500\n",
      "Epoch 39/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.6434 - acc: 0.6750 - val_loss: 0.7152 - val_acc: 0.5500\n",
      "Epoch 40/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.6416 - acc: 0.6937 - val_loss: 0.7140 - val_acc: 0.5500\n",
      "Epoch 41/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.6400 - acc: 0.6875 - val_loss: 0.7126 - val_acc: 0.5500\n",
      "Epoch 42/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.6384 - acc: 0.6875 - val_loss: 0.7118 - val_acc: 0.5500\n",
      "Epoch 43/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.6367 - acc: 0.6875 - val_loss: 0.7105 - val_acc: 0.5750\n",
      "Epoch 44/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.6350 - acc: 0.6813 - val_loss: 0.7094 - val_acc: 0.5750\n",
      "Epoch 45/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.6332 - acc: 0.6813 - val_loss: 0.7088 - val_acc: 0.5750\n",
      "Epoch 46/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.6313 - acc: 0.6875 - val_loss: 0.7077 - val_acc: 0.5750\n",
      "Epoch 47/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.6294 - acc: 0.6937 - val_loss: 0.7063 - val_acc: 0.5750\n",
      "Epoch 48/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.6275 - acc: 0.7000 - val_loss: 0.7050 - val_acc: 0.5750\n",
      "Epoch 49/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.6257 - acc: 0.7000 - val_loss: 0.7038 - val_acc: 0.5750\n",
      "Epoch 50/800\n",
      "160/160 [==============================] - 0s 130us/step - loss: 0.6237 - acc: 0.7000 - val_loss: 0.7019 - val_acc: 0.5750\n",
      "Epoch 51/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.6216 - acc: 0.7125 - val_loss: 0.7004 - val_acc: 0.5750\n",
      "Epoch 52/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.6195 - acc: 0.7125 - val_loss: 0.6986 - val_acc: 0.6000\n",
      "Epoch 53/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.6174 - acc: 0.7375 - val_loss: 0.6966 - val_acc: 0.6000\n",
      "Epoch 54/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.6154 - acc: 0.7438 - val_loss: 0.6953 - val_acc: 0.6000\n",
      "Epoch 55/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.6132 - acc: 0.7438 - val_loss: 0.6935 - val_acc: 0.6000\n",
      "Epoch 56/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.6110 - acc: 0.7438 - val_loss: 0.6916 - val_acc: 0.6000\n",
      "Epoch 57/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.6088 - acc: 0.7438 - val_loss: 0.6898 - val_acc: 0.6250\n",
      "Epoch 58/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.6067 - acc: 0.7438 - val_loss: 0.6870 - val_acc: 0.6250\n",
      "Epoch 59/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.6043 - acc: 0.7375 - val_loss: 0.6852 - val_acc: 0.6250\n",
      "Epoch 60/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.6018 - acc: 0.7375 - val_loss: 0.6835 - val_acc: 0.6250\n",
      "Epoch 61/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 79us/step - loss: 0.5995 - acc: 0.7375 - val_loss: 0.6818 - val_acc: 0.6500\n",
      "Epoch 62/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.5972 - acc: 0.7375 - val_loss: 0.6800 - val_acc: 0.6500\n",
      "Epoch 63/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.5947 - acc: 0.7375 - val_loss: 0.6775 - val_acc: 0.6250\n",
      "Epoch 64/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.5923 - acc: 0.7438 - val_loss: 0.6759 - val_acc: 0.6250\n",
      "Epoch 65/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.5897 - acc: 0.7438 - val_loss: 0.6732 - val_acc: 0.6250\n",
      "Epoch 66/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.5873 - acc: 0.7500 - val_loss: 0.6707 - val_acc: 0.6250\n",
      "Epoch 67/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.5848 - acc: 0.7500 - val_loss: 0.6684 - val_acc: 0.6250\n",
      "Epoch 68/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.5824 - acc: 0.7625 - val_loss: 0.6654 - val_acc: 0.6250\n",
      "Epoch 69/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.5796 - acc: 0.7625 - val_loss: 0.6631 - val_acc: 0.6250\n",
      "Epoch 70/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.5770 - acc: 0.7625 - val_loss: 0.6609 - val_acc: 0.6250\n",
      "Epoch 71/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.5743 - acc: 0.7625 - val_loss: 0.6588 - val_acc: 0.6250\n",
      "Epoch 72/800\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.5717 - acc: 0.7625 - val_loss: 0.6567 - val_acc: 0.6750\n",
      "Epoch 73/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.5692 - acc: 0.7625 - val_loss: 0.6546 - val_acc: 0.6750\n",
      "Epoch 74/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.5664 - acc: 0.7625 - val_loss: 0.6519 - val_acc: 0.6750\n",
      "Epoch 75/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.5638 - acc: 0.7687 - val_loss: 0.6493 - val_acc: 0.6750\n",
      "Epoch 76/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.5610 - acc: 0.7750 - val_loss: 0.6469 - val_acc: 0.6750\n",
      "Epoch 77/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.5583 - acc: 0.7750 - val_loss: 0.6444 - val_acc: 0.6750\n",
      "Epoch 78/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.5556 - acc: 0.7750 - val_loss: 0.6418 - val_acc: 0.6750\n",
      "Epoch 79/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.5528 - acc: 0.7750 - val_loss: 0.6386 - val_acc: 0.6750\n",
      "Epoch 80/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.5501 - acc: 0.7750 - val_loss: 0.6349 - val_acc: 0.6750\n",
      "Epoch 81/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.5472 - acc: 0.7750 - val_loss: 0.6318 - val_acc: 0.6750\n",
      "Epoch 82/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.5444 - acc: 0.7812 - val_loss: 0.6294 - val_acc: 0.6750\n",
      "Epoch 83/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.5417 - acc: 0.7812 - val_loss: 0.6259 - val_acc: 0.6750\n",
      "Epoch 84/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.5386 - acc: 0.7875 - val_loss: 0.6233 - val_acc: 0.6750\n",
      "Epoch 85/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.5359 - acc: 0.7875 - val_loss: 0.6206 - val_acc: 0.6750\n",
      "Epoch 86/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.5329 - acc: 0.7938 - val_loss: 0.6174 - val_acc: 0.6750\n",
      "Epoch 87/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.5301 - acc: 0.8000 - val_loss: 0.6141 - val_acc: 0.6750\n",
      "Epoch 88/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.5273 - acc: 0.8000 - val_loss: 0.6114 - val_acc: 0.6750\n",
      "Epoch 89/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.5244 - acc: 0.8187 - val_loss: 0.6089 - val_acc: 0.6750\n",
      "Epoch 90/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.5215 - acc: 0.8250 - val_loss: 0.6050 - val_acc: 0.7000\n",
      "Epoch 91/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.5184 - acc: 0.8250 - val_loss: 0.6022 - val_acc: 0.7000\n",
      "Epoch 92/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.5155 - acc: 0.8312 - val_loss: 0.5992 - val_acc: 0.7250\n",
      "Epoch 93/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.5126 - acc: 0.8375 - val_loss: 0.5955 - val_acc: 0.7250\n",
      "Epoch 94/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.5096 - acc: 0.8438 - val_loss: 0.5926 - val_acc: 0.7250\n",
      "Epoch 95/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.5068 - acc: 0.8312 - val_loss: 0.5892 - val_acc: 0.7250\n",
      "Epoch 96/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.5037 - acc: 0.8375 - val_loss: 0.5861 - val_acc: 0.7250\n",
      "Epoch 97/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.5007 - acc: 0.8438 - val_loss: 0.5830 - val_acc: 0.7250\n",
      "Epoch 98/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.4976 - acc: 0.8500 - val_loss: 0.5798 - val_acc: 0.7250\n",
      "Epoch 99/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.4947 - acc: 0.8500 - val_loss: 0.5766 - val_acc: 0.7250\n",
      "Epoch 100/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.4917 - acc: 0.8500 - val_loss: 0.5729 - val_acc: 0.7250\n",
      "Epoch 101/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.4888 - acc: 0.8500 - val_loss: 0.5693 - val_acc: 0.7250\n",
      "Epoch 102/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.4856 - acc: 0.8500 - val_loss: 0.5659 - val_acc: 0.7250\n",
      "Epoch 103/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.4825 - acc: 0.8500 - val_loss: 0.5626 - val_acc: 0.7250\n",
      "Epoch 104/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.4796 - acc: 0.8500 - val_loss: 0.5594 - val_acc: 0.7250\n",
      "Epoch 105/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.4765 - acc: 0.8500 - val_loss: 0.5562 - val_acc: 0.7250\n",
      "Epoch 106/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.4734 - acc: 0.8500 - val_loss: 0.5527 - val_acc: 0.7250\n",
      "Epoch 107/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.4703 - acc: 0.8500 - val_loss: 0.5496 - val_acc: 0.7250\n",
      "Epoch 108/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.4674 - acc: 0.8563 - val_loss: 0.5455 - val_acc: 0.7250\n",
      "Epoch 109/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.4642 - acc: 0.8625 - val_loss: 0.5420 - val_acc: 0.7250\n",
      "Epoch 110/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.4612 - acc: 0.8625 - val_loss: 0.5380 - val_acc: 0.7250\n",
      "Epoch 111/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.4581 - acc: 0.8625 - val_loss: 0.5344 - val_acc: 0.7250\n",
      "Epoch 112/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.4549 - acc: 0.8625 - val_loss: 0.5314 - val_acc: 0.7250\n",
      "Epoch 113/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.4519 - acc: 0.8688 - val_loss: 0.5277 - val_acc: 0.7250\n",
      "Epoch 114/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.4487 - acc: 0.8688 - val_loss: 0.5240 - val_acc: 0.7250\n",
      "Epoch 115/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.4457 - acc: 0.8688 - val_loss: 0.5204 - val_acc: 0.7250\n",
      "Epoch 116/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.4426 - acc: 0.8688 - val_loss: 0.5171 - val_acc: 0.7250\n",
      "Epoch 117/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.4396 - acc: 0.8750 - val_loss: 0.5134 - val_acc: 0.7250\n",
      "Epoch 118/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.4364 - acc: 0.8750 - val_loss: 0.5100 - val_acc: 0.7250\n",
      "Epoch 119/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.4334 - acc: 0.8750 - val_loss: 0.5070 - val_acc: 0.7250\n",
      "Epoch 120/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.4302 - acc: 0.8750 - val_loss: 0.5031 - val_acc: 0.7250\n",
      "Epoch 121/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 93us/step - loss: 0.4270 - acc: 0.8812 - val_loss: 0.4995 - val_acc: 0.7250\n",
      "Epoch 122/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.4240 - acc: 0.8875 - val_loss: 0.4957 - val_acc: 0.7250\n",
      "Epoch 123/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.4209 - acc: 0.8875 - val_loss: 0.4917 - val_acc: 0.7250\n",
      "Epoch 124/800\n",
      "160/160 [==============================] - 0s 114us/step - loss: 0.4178 - acc: 0.8875 - val_loss: 0.4880 - val_acc: 0.7250\n",
      "Epoch 125/800\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.4148 - acc: 0.8875 - val_loss: 0.4845 - val_acc: 0.7250\n",
      "Epoch 126/800\n",
      "160/160 [==============================] - 0s 112us/step - loss: 0.4116 - acc: 0.8937 - val_loss: 0.4812 - val_acc: 0.7250\n",
      "Epoch 127/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.4085 - acc: 0.9000 - val_loss: 0.4777 - val_acc: 0.7250\n",
      "Epoch 128/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.4054 - acc: 0.9000 - val_loss: 0.4741 - val_acc: 0.7500\n",
      "Epoch 129/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.4025 - acc: 0.9000 - val_loss: 0.4714 - val_acc: 0.7500\n",
      "Epoch 130/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.3994 - acc: 0.9000 - val_loss: 0.4682 - val_acc: 0.7750\n",
      "Epoch 131/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.3963 - acc: 0.9000 - val_loss: 0.4647 - val_acc: 0.8000\n",
      "Epoch 132/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.3933 - acc: 0.9062 - val_loss: 0.4606 - val_acc: 0.8000\n",
      "Epoch 133/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.3902 - acc: 0.9062 - val_loss: 0.4573 - val_acc: 0.8000\n",
      "Epoch 134/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.3872 - acc: 0.9062 - val_loss: 0.4540 - val_acc: 0.8000\n",
      "Epoch 135/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.3844 - acc: 0.9062 - val_loss: 0.4497 - val_acc: 0.8250\n",
      "Epoch 136/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.3812 - acc: 0.9062 - val_loss: 0.4462 - val_acc: 0.8250\n",
      "Epoch 137/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.3781 - acc: 0.9062 - val_loss: 0.4433 - val_acc: 0.8250\n",
      "Epoch 138/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.3752 - acc: 0.9125 - val_loss: 0.4395 - val_acc: 0.8250\n",
      "Epoch 139/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.3724 - acc: 0.9188 - val_loss: 0.4365 - val_acc: 0.8250\n",
      "Epoch 140/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.3693 - acc: 0.9188 - val_loss: 0.4337 - val_acc: 0.8250\n",
      "Epoch 141/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.3664 - acc: 0.9188 - val_loss: 0.4299 - val_acc: 0.8500\n",
      "Epoch 142/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.3634 - acc: 0.9250 - val_loss: 0.4263 - val_acc: 0.8500\n",
      "Epoch 143/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.3604 - acc: 0.9313 - val_loss: 0.4231 - val_acc: 0.8500\n",
      "Epoch 144/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.3577 - acc: 0.9313 - val_loss: 0.4193 - val_acc: 0.8500\n",
      "Epoch 145/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.3547 - acc: 0.9313 - val_loss: 0.4159 - val_acc: 0.8500\n",
      "Epoch 146/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.3518 - acc: 0.9375 - val_loss: 0.4127 - val_acc: 0.8750\n",
      "Epoch 147/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.3489 - acc: 0.9375 - val_loss: 0.4100 - val_acc: 0.8750\n",
      "Epoch 148/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.3461 - acc: 0.9375 - val_loss: 0.4067 - val_acc: 0.8750\n",
      "Epoch 149/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.3433 - acc: 0.9375 - val_loss: 0.4042 - val_acc: 0.8750\n",
      "Epoch 150/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.3405 - acc: 0.9375 - val_loss: 0.4019 - val_acc: 0.8750\n",
      "Epoch 151/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.3376 - acc: 0.9375 - val_loss: 0.3983 - val_acc: 0.8750\n",
      "Epoch 152/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.3348 - acc: 0.9375 - val_loss: 0.3953 - val_acc: 0.8750\n",
      "Epoch 153/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.3320 - acc: 0.9375 - val_loss: 0.3921 - val_acc: 0.8750\n",
      "Epoch 154/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.3292 - acc: 0.9375 - val_loss: 0.3887 - val_acc: 0.8750\n",
      "Epoch 155/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.3265 - acc: 0.9500 - val_loss: 0.3856 - val_acc: 0.8750\n",
      "Epoch 156/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.3237 - acc: 0.9500 - val_loss: 0.3823 - val_acc: 0.9000\n",
      "Epoch 157/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.3210 - acc: 0.9562 - val_loss: 0.3790 - val_acc: 0.9000\n",
      "Epoch 158/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.3183 - acc: 0.9562 - val_loss: 0.3764 - val_acc: 0.9000\n",
      "Epoch 159/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.3156 - acc: 0.9562 - val_loss: 0.3734 - val_acc: 0.9000\n",
      "Epoch 160/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.3130 - acc: 0.9625 - val_loss: 0.3705 - val_acc: 0.9000\n",
      "Epoch 161/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.3102 - acc: 0.9625 - val_loss: 0.3679 - val_acc: 0.9000\n",
      "Epoch 162/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.3076 - acc: 0.9625 - val_loss: 0.3657 - val_acc: 0.9000\n",
      "Epoch 163/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.3050 - acc: 0.9625 - val_loss: 0.3626 - val_acc: 0.9000\n",
      "Epoch 164/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.3024 - acc: 0.9625 - val_loss: 0.3600 - val_acc: 0.9000\n",
      "Epoch 165/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.2997 - acc: 0.9625 - val_loss: 0.3571 - val_acc: 0.9000\n",
      "Epoch 166/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.2972 - acc: 0.9625 - val_loss: 0.3549 - val_acc: 0.9000\n",
      "Epoch 167/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.2946 - acc: 0.9625 - val_loss: 0.3524 - val_acc: 0.9000\n",
      "Epoch 168/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.2920 - acc: 0.9625 - val_loss: 0.3493 - val_acc: 0.9000\n",
      "Epoch 169/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.2894 - acc: 0.9688 - val_loss: 0.3464 - val_acc: 0.9000\n",
      "Epoch 170/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.2870 - acc: 0.9688 - val_loss: 0.3435 - val_acc: 0.9000\n",
      "Epoch 171/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.2844 - acc: 0.9688 - val_loss: 0.3410 - val_acc: 0.9000\n",
      "Epoch 172/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.2819 - acc: 0.9750 - val_loss: 0.3387 - val_acc: 0.9000\n",
      "Epoch 173/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.2794 - acc: 0.9750 - val_loss: 0.3364 - val_acc: 0.9000\n",
      "Epoch 174/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.2769 - acc: 0.9750 - val_loss: 0.3338 - val_acc: 0.9000\n",
      "Epoch 175/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.2745 - acc: 0.9750 - val_loss: 0.3315 - val_acc: 0.9000\n",
      "Epoch 176/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.2721 - acc: 0.9750 - val_loss: 0.3293 - val_acc: 0.9250\n",
      "Epoch 177/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.2696 - acc: 0.9750 - val_loss: 0.3265 - val_acc: 0.9250\n",
      "Epoch 178/800\n",
      "160/160 [==============================] - 0s 53us/step - loss: 0.2672 - acc: 0.9750 - val_loss: 0.3239 - val_acc: 0.9500\n",
      "Epoch 179/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.2649 - acc: 0.9813 - val_loss: 0.3221 - val_acc: 0.9500\n",
      "Epoch 180/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.2624 - acc: 0.9813 - val_loss: 0.3194 - val_acc: 0.9500\n",
      "Epoch 181/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 66us/step - loss: 0.2601 - acc: 0.9813 - val_loss: 0.3172 - val_acc: 0.9500\n",
      "Epoch 182/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.2578 - acc: 0.9813 - val_loss: 0.3144 - val_acc: 0.9500\n",
      "Epoch 183/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.2554 - acc: 0.9813 - val_loss: 0.3117 - val_acc: 0.9500\n",
      "Epoch 184/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.2531 - acc: 0.9813 - val_loss: 0.3097 - val_acc: 0.9500\n",
      "Epoch 185/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.2508 - acc: 0.9875 - val_loss: 0.3076 - val_acc: 0.9500\n",
      "Epoch 186/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.2485 - acc: 0.9875 - val_loss: 0.3058 - val_acc: 0.9500\n",
      "Epoch 187/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.2463 - acc: 0.9875 - val_loss: 0.3034 - val_acc: 0.9500\n",
      "Epoch 188/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.2440 - acc: 0.9875 - val_loss: 0.3007 - val_acc: 0.9500\n",
      "Epoch 189/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.2419 - acc: 0.9875 - val_loss: 0.2986 - val_acc: 0.9500\n",
      "Epoch 190/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.2396 - acc: 0.9938 - val_loss: 0.2966 - val_acc: 0.9500\n",
      "Epoch 191/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.2375 - acc: 0.9938 - val_loss: 0.2943 - val_acc: 0.9500\n",
      "Epoch 192/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.2352 - acc: 0.9938 - val_loss: 0.2922 - val_acc: 0.9500\n",
      "Epoch 193/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.2331 - acc: 0.9938 - val_loss: 0.2906 - val_acc: 0.9500\n",
      "Epoch 194/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.2310 - acc: 0.9938 - val_loss: 0.2884 - val_acc: 0.9500\n",
      "Epoch 195/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.2288 - acc: 0.9938 - val_loss: 0.2860 - val_acc: 0.9500\n",
      "Epoch 196/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.2266 - acc: 0.9938 - val_loss: 0.2840 - val_acc: 0.9500\n",
      "Epoch 197/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.2246 - acc: 0.9938 - val_loss: 0.2826 - val_acc: 0.9500\n",
      "Epoch 198/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.2225 - acc: 0.9938 - val_loss: 0.2811 - val_acc: 0.9500\n",
      "Epoch 199/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.2205 - acc: 0.9938 - val_loss: 0.2791 - val_acc: 0.9500\n",
      "Epoch 200/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.2184 - acc: 0.9938 - val_loss: 0.2772 - val_acc: 0.9500\n",
      "Epoch 201/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.2164 - acc: 0.9938 - val_loss: 0.2748 - val_acc: 0.9500\n",
      "Epoch 202/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.2144 - acc: 0.9938 - val_loss: 0.2728 - val_acc: 0.9500\n",
      "Epoch 203/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.2124 - acc: 0.9938 - val_loss: 0.2709 - val_acc: 0.9500\n",
      "Epoch 204/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.2103 - acc: 0.9938 - val_loss: 0.2691 - val_acc: 0.9500\n",
      "Epoch 205/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.2084 - acc: 0.9938 - val_loss: 0.2669 - val_acc: 0.9500\n",
      "Epoch 206/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.2065 - acc: 0.9938 - val_loss: 0.2651 - val_acc: 0.9500\n",
      "Epoch 207/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.2046 - acc: 0.9938 - val_loss: 0.2638 - val_acc: 0.9500\n",
      "Epoch 208/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.2027 - acc: 0.9938 - val_loss: 0.2617 - val_acc: 0.9500\n",
      "Epoch 209/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.2008 - acc: 0.9938 - val_loss: 0.2599 - val_acc: 0.9500\n",
      "Epoch 210/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.1989 - acc: 0.9938 - val_loss: 0.2582 - val_acc: 0.9500\n",
      "Epoch 211/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.1972 - acc: 0.9938 - val_loss: 0.2560 - val_acc: 0.9500\n",
      "Epoch 212/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.1952 - acc: 0.9938 - val_loss: 0.2547 - val_acc: 0.9500\n",
      "Epoch 213/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.1934 - acc: 0.9938 - val_loss: 0.2529 - val_acc: 0.9500\n",
      "Epoch 214/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.1915 - acc: 0.9938 - val_loss: 0.2511 - val_acc: 0.9500\n",
      "Epoch 215/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.1898 - acc: 0.9938 - val_loss: 0.2493 - val_acc: 0.9500\n",
      "Epoch 216/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.1880 - acc: 0.9938 - val_loss: 0.2477 - val_acc: 0.9500\n",
      "Epoch 217/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.1863 - acc: 0.9938 - val_loss: 0.2464 - val_acc: 0.9500\n",
      "Epoch 218/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.1846 - acc: 0.9938 - val_loss: 0.2450 - val_acc: 0.9500\n",
      "Epoch 219/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.1829 - acc: 0.9938 - val_loss: 0.2432 - val_acc: 0.9500\n",
      "Epoch 220/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.1812 - acc: 0.9938 - val_loss: 0.2413 - val_acc: 0.9500\n",
      "Epoch 221/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.1796 - acc: 0.9938 - val_loss: 0.2402 - val_acc: 0.9500\n",
      "Epoch 222/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.1779 - acc: 0.9938 - val_loss: 0.2389 - val_acc: 0.9500\n",
      "Epoch 223/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.1763 - acc: 0.9938 - val_loss: 0.2373 - val_acc: 0.9500\n",
      "Epoch 224/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.1747 - acc: 0.9938 - val_loss: 0.2350 - val_acc: 0.9500\n",
      "Epoch 225/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.1730 - acc: 0.9938 - val_loss: 0.2336 - val_acc: 0.9500\n",
      "Epoch 226/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.1714 - acc: 0.9938 - val_loss: 0.2322 - val_acc: 0.9500\n",
      "Epoch 227/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.1698 - acc: 0.9938 - val_loss: 0.2307 - val_acc: 0.9500\n",
      "Epoch 228/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.1682 - acc: 0.9938 - val_loss: 0.2294 - val_acc: 0.9500\n",
      "Epoch 229/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.1668 - acc: 0.9938 - val_loss: 0.2277 - val_acc: 0.9500\n",
      "Epoch 230/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.1653 - acc: 0.9938 - val_loss: 0.2262 - val_acc: 0.9500\n",
      "Epoch 231/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.1637 - acc: 0.9938 - val_loss: 0.2247 - val_acc: 0.9500\n",
      "Epoch 232/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1622 - acc: 0.9938 - val_loss: 0.2235 - val_acc: 0.9500\n",
      "Epoch 233/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.1608 - acc: 0.9938 - val_loss: 0.2222 - val_acc: 0.9500\n",
      "Epoch 234/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.1594 - acc: 0.9938 - val_loss: 0.2206 - val_acc: 0.9500\n",
      "Epoch 235/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.1579 - acc: 0.9938 - val_loss: 0.2193 - val_acc: 0.9500\n",
      "Epoch 236/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.1565 - acc: 0.9938 - val_loss: 0.2187 - val_acc: 0.9500\n",
      "Epoch 237/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.1551 - acc: 0.9938 - val_loss: 0.2176 - val_acc: 0.9500\n",
      "Epoch 238/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.1536 - acc: 0.9938 - val_loss: 0.2161 - val_acc: 0.9500\n",
      "Epoch 239/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.1523 - acc: 0.9938 - val_loss: 0.2147 - val_acc: 0.9500\n",
      "Epoch 240/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.1511 - acc: 0.9938 - val_loss: 0.2135 - val_acc: 0.9500\n",
      "Epoch 241/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 60us/step - loss: 0.1496 - acc: 0.9938 - val_loss: 0.2121 - val_acc: 0.9500\n",
      "Epoch 242/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.1484 - acc: 0.9938 - val_loss: 0.2104 - val_acc: 0.9500\n",
      "Epoch 243/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.1470 - acc: 0.9938 - val_loss: 0.2096 - val_acc: 0.9500\n",
      "Epoch 244/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.1457 - acc: 0.9938 - val_loss: 0.2080 - val_acc: 0.9500\n",
      "Epoch 245/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.1444 - acc: 0.9938 - val_loss: 0.2067 - val_acc: 0.9500\n",
      "Epoch 246/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.1431 - acc: 0.9938 - val_loss: 0.2055 - val_acc: 0.9500\n",
      "Epoch 247/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.1419 - acc: 0.9938 - val_loss: 0.2042 - val_acc: 0.9500\n",
      "Epoch 248/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.1406 - acc: 0.9938 - val_loss: 0.2029 - val_acc: 0.9500\n",
      "Epoch 249/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.1395 - acc: 0.9938 - val_loss: 0.2023 - val_acc: 0.9500\n",
      "Epoch 250/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.1383 - acc: 0.9938 - val_loss: 0.2017 - val_acc: 0.9500\n",
      "Epoch 251/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.1371 - acc: 0.9938 - val_loss: 0.2000 - val_acc: 0.9500\n",
      "Epoch 252/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.1359 - acc: 1.0000 - val_loss: 0.1988 - val_acc: 0.9500\n",
      "Epoch 253/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.1347 - acc: 1.0000 - val_loss: 0.1980 - val_acc: 0.9500\n",
      "Epoch 254/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.1336 - acc: 1.0000 - val_loss: 0.1971 - val_acc: 0.9500\n",
      "Epoch 255/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.1324 - acc: 1.0000 - val_loss: 0.1957 - val_acc: 0.9500\n",
      "Epoch 256/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.1313 - acc: 1.0000 - val_loss: 0.1944 - val_acc: 0.9500\n",
      "Epoch 257/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.1301 - acc: 1.0000 - val_loss: 0.1933 - val_acc: 0.9500\n",
      "Epoch 258/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.1291 - acc: 1.0000 - val_loss: 0.1921 - val_acc: 0.9500\n",
      "Epoch 259/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.1280 - acc: 1.0000 - val_loss: 0.1910 - val_acc: 0.9500\n",
      "Epoch 260/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.1269 - acc: 1.0000 - val_loss: 0.1905 - val_acc: 0.9500\n",
      "Epoch 261/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.1258 - acc: 1.0000 - val_loss: 0.1895 - val_acc: 0.9500\n",
      "Epoch 262/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.1248 - acc: 1.0000 - val_loss: 0.1885 - val_acc: 0.9500\n",
      "Epoch 263/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.1238 - acc: 1.0000 - val_loss: 0.1874 - val_acc: 0.9500\n",
      "Epoch 264/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.1227 - acc: 1.0000 - val_loss: 0.1860 - val_acc: 0.9500\n",
      "Epoch 265/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.1218 - acc: 1.0000 - val_loss: 0.1847 - val_acc: 0.9500\n",
      "Epoch 266/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.1208 - acc: 1.0000 - val_loss: 0.1842 - val_acc: 0.9500\n",
      "Epoch 267/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.1198 - acc: 1.0000 - val_loss: 0.1830 - val_acc: 0.9500\n",
      "Epoch 268/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.1188 - acc: 1.0000 - val_loss: 0.1820 - val_acc: 0.9500\n",
      "Epoch 269/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.1178 - acc: 1.0000 - val_loss: 0.1810 - val_acc: 0.9500\n",
      "Epoch 270/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.1168 - acc: 1.0000 - val_loss: 0.1801 - val_acc: 0.9500\n",
      "Epoch 271/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.1160 - acc: 1.0000 - val_loss: 0.1788 - val_acc: 0.9500\n",
      "Epoch 272/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.1150 - acc: 1.0000 - val_loss: 0.1784 - val_acc: 0.9500\n",
      "Epoch 273/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.1141 - acc: 1.0000 - val_loss: 0.1775 - val_acc: 0.9500\n",
      "Epoch 274/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.1132 - acc: 1.0000 - val_loss: 0.1771 - val_acc: 0.9500\n",
      "Epoch 275/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.1122 - acc: 1.0000 - val_loss: 0.1763 - val_acc: 0.9500\n",
      "Epoch 276/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.1114 - acc: 1.0000 - val_loss: 0.1750 - val_acc: 0.9500\n",
      "Epoch 277/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.1105 - acc: 1.0000 - val_loss: 0.1745 - val_acc: 0.9500\n",
      "Epoch 278/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.1096 - acc: 1.0000 - val_loss: 0.1733 - val_acc: 0.9500\n",
      "Epoch 279/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.1088 - acc: 1.0000 - val_loss: 0.1723 - val_acc: 0.9500\n",
      "Epoch 280/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.1079 - acc: 1.0000 - val_loss: 0.1717 - val_acc: 0.9500\n",
      "Epoch 281/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.1071 - acc: 1.0000 - val_loss: 0.1709 - val_acc: 0.9500\n",
      "Epoch 282/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.1062 - acc: 1.0000 - val_loss: 0.1703 - val_acc: 0.9500\n",
      "Epoch 283/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1054 - acc: 1.0000 - val_loss: 0.1690 - val_acc: 0.9750\n",
      "Epoch 284/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.1046 - acc: 1.0000 - val_loss: 0.1685 - val_acc: 0.9750\n",
      "Epoch 285/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.1038 - acc: 1.0000 - val_loss: 0.1676 - val_acc: 0.9750\n",
      "Epoch 286/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.1030 - acc: 1.0000 - val_loss: 0.1673 - val_acc: 0.9750\n",
      "Epoch 287/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.1023 - acc: 1.0000 - val_loss: 0.1662 - val_acc: 0.9750\n",
      "Epoch 288/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.1015 - acc: 1.0000 - val_loss: 0.1653 - val_acc: 0.9750\n",
      "Epoch 289/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.1007 - acc: 1.0000 - val_loss: 0.1645 - val_acc: 0.9750\n",
      "Epoch 290/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0999 - acc: 1.0000 - val_loss: 0.1636 - val_acc: 0.9750\n",
      "Epoch 291/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0992 - acc: 1.0000 - val_loss: 0.1632 - val_acc: 0.9750\n",
      "Epoch 292/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0984 - acc: 1.0000 - val_loss: 0.1625 - val_acc: 0.9750\n",
      "Epoch 293/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0977 - acc: 1.0000 - val_loss: 0.1616 - val_acc: 0.9750\n",
      "Epoch 294/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0970 - acc: 1.0000 - val_loss: 0.1607 - val_acc: 0.9750\n",
      "Epoch 295/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0963 - acc: 1.0000 - val_loss: 0.1605 - val_acc: 0.9750\n",
      "Epoch 296/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0956 - acc: 1.0000 - val_loss: 0.1598 - val_acc: 0.9750\n",
      "Epoch 297/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0949 - acc: 1.0000 - val_loss: 0.1589 - val_acc: 0.9750\n",
      "Epoch 298/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0941 - acc: 1.0000 - val_loss: 0.1581 - val_acc: 0.9750\n",
      "Epoch 299/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0935 - acc: 1.0000 - val_loss: 0.1569 - val_acc: 0.9750\n",
      "Epoch 300/800\n",
      "160/160 [==============================] - 0s 52us/step - loss: 0.0928 - acc: 1.0000 - val_loss: 0.1569 - val_acc: 0.9750\n",
      "Epoch 301/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 69us/step - loss: 0.0921 - acc: 1.0000 - val_loss: 0.1560 - val_acc: 0.9750\n",
      "Epoch 302/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0915 - acc: 1.0000 - val_loss: 0.1550 - val_acc: 0.9750\n",
      "Epoch 303/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0908 - acc: 1.0000 - val_loss: 0.1543 - val_acc: 0.9750\n",
      "Epoch 304/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0901 - acc: 1.0000 - val_loss: 0.1537 - val_acc: 0.9750\n",
      "Epoch 305/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0895 - acc: 1.0000 - val_loss: 0.1534 - val_acc: 0.9750\n",
      "Epoch 306/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0889 - acc: 1.0000 - val_loss: 0.1529 - val_acc: 0.9750\n",
      "Epoch 307/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0882 - acc: 1.0000 - val_loss: 0.1523 - val_acc: 0.9750\n",
      "Epoch 308/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0877 - acc: 1.0000 - val_loss: 0.1513 - val_acc: 0.9750\n",
      "Epoch 309/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0870 - acc: 1.0000 - val_loss: 0.1510 - val_acc: 0.9750\n",
      "Epoch 310/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0864 - acc: 1.0000 - val_loss: 0.1501 - val_acc: 0.9750\n",
      "Epoch 311/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0858 - acc: 1.0000 - val_loss: 0.1497 - val_acc: 0.9750\n",
      "Epoch 312/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0852 - acc: 1.0000 - val_loss: 0.1487 - val_acc: 0.9750\n",
      "Epoch 313/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0846 - acc: 1.0000 - val_loss: 0.1480 - val_acc: 0.9750\n",
      "Epoch 314/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0841 - acc: 1.0000 - val_loss: 0.1479 - val_acc: 0.9750\n",
      "Epoch 315/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0835 - acc: 1.0000 - val_loss: 0.1474 - val_acc: 0.9750\n",
      "Epoch 316/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0829 - acc: 1.0000 - val_loss: 0.1464 - val_acc: 0.9750\n",
      "Epoch 317/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0823 - acc: 1.0000 - val_loss: 0.1459 - val_acc: 0.9750\n",
      "Epoch 318/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0818 - acc: 1.0000 - val_loss: 0.1456 - val_acc: 0.9750\n",
      "Epoch 319/800\n",
      "160/160 [==============================] - 0s 54us/step - loss: 0.0812 - acc: 1.0000 - val_loss: 0.1451 - val_acc: 0.9750\n",
      "Epoch 320/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0807 - acc: 1.0000 - val_loss: 0.1444 - val_acc: 0.9750\n",
      "Epoch 321/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0802 - acc: 1.0000 - val_loss: 0.1440 - val_acc: 0.9750\n",
      "Epoch 322/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0796 - acc: 1.0000 - val_loss: 0.1437 - val_acc: 0.9750\n",
      "Epoch 323/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0791 - acc: 1.0000 - val_loss: 0.1433 - val_acc: 0.9750\n",
      "Epoch 324/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0785 - acc: 1.0000 - val_loss: 0.1422 - val_acc: 0.9750\n",
      "Epoch 325/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0780 - acc: 1.0000 - val_loss: 0.1416 - val_acc: 0.9750\n",
      "Epoch 326/800\n",
      "160/160 [==============================] - 0s 237us/step - loss: 0.0775 - acc: 1.0000 - val_loss: 0.1414 - val_acc: 0.9750\n",
      "Epoch 327/800\n",
      "160/160 [==============================] - 0s 106us/step - loss: 0.0770 - acc: 1.0000 - val_loss: 0.1405 - val_acc: 0.9750\n",
      "Epoch 328/800\n",
      "160/160 [==============================] - 0s 120us/step - loss: 0.0765 - acc: 1.0000 - val_loss: 0.1401 - val_acc: 0.9750\n",
      "Epoch 329/800\n",
      "160/160 [==============================] - 0s 106us/step - loss: 0.0760 - acc: 1.0000 - val_loss: 0.1396 - val_acc: 0.9750\n",
      "Epoch 330/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0755 - acc: 1.0000 - val_loss: 0.1396 - val_acc: 0.9750\n",
      "Epoch 331/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0750 - acc: 1.0000 - val_loss: 0.1389 - val_acc: 0.9750\n",
      "Epoch 332/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0745 - acc: 1.0000 - val_loss: 0.1383 - val_acc: 0.9750\n",
      "Epoch 333/800\n",
      "160/160 [==============================] - 0s 122us/step - loss: 0.0741 - acc: 1.0000 - val_loss: 0.1373 - val_acc: 0.9750\n",
      "Epoch 334/800\n",
      "160/160 [==============================] - 0s 112us/step - loss: 0.0736 - acc: 1.0000 - val_loss: 0.1371 - val_acc: 0.9750\n",
      "Epoch 335/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0731 - acc: 1.0000 - val_loss: 0.1366 - val_acc: 0.9750\n",
      "Epoch 336/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0727 - acc: 1.0000 - val_loss: 0.1367 - val_acc: 0.9750\n",
      "Epoch 337/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0722 - acc: 1.0000 - val_loss: 0.1363 - val_acc: 0.9750\n",
      "Epoch 338/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0718 - acc: 1.0000 - val_loss: 0.1352 - val_acc: 0.9750\n",
      "Epoch 339/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0713 - acc: 1.0000 - val_loss: 0.1346 - val_acc: 0.9750\n",
      "Epoch 340/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0708 - acc: 1.0000 - val_loss: 0.1344 - val_acc: 0.9750\n",
      "Epoch 341/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0704 - acc: 1.0000 - val_loss: 0.1337 - val_acc: 0.9750\n",
      "Epoch 342/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0701 - acc: 1.0000 - val_loss: 0.1328 - val_acc: 0.9750\n",
      "Epoch 343/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0695 - acc: 1.0000 - val_loss: 0.1327 - val_acc: 0.9750\n",
      "Epoch 344/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0691 - acc: 1.0000 - val_loss: 0.1328 - val_acc: 0.9750\n",
      "Epoch 345/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0687 - acc: 1.0000 - val_loss: 0.1326 - val_acc: 0.9750\n",
      "Epoch 346/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0682 - acc: 1.0000 - val_loss: 0.1322 - val_acc: 0.9750\n",
      "Epoch 347/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0678 - acc: 1.0000 - val_loss: 0.1317 - val_acc: 0.9750\n",
      "Epoch 348/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0674 - acc: 1.0000 - val_loss: 0.1312 - val_acc: 0.9750\n",
      "Epoch 349/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0670 - acc: 1.0000 - val_loss: 0.1305 - val_acc: 0.9750\n",
      "Epoch 350/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0666 - acc: 1.0000 - val_loss: 0.1298 - val_acc: 0.9750\n",
      "Epoch 351/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0661 - acc: 1.0000 - val_loss: 0.1296 - val_acc: 0.9750\n",
      "Epoch 352/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0658 - acc: 1.0000 - val_loss: 0.1290 - val_acc: 0.9750\n",
      "Epoch 353/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0654 - acc: 1.0000 - val_loss: 0.1289 - val_acc: 0.9750\n",
      "Epoch 354/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0650 - acc: 1.0000 - val_loss: 0.1294 - val_acc: 0.9750\n",
      "Epoch 355/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0646 - acc: 1.0000 - val_loss: 0.1290 - val_acc: 0.9750\n",
      "Epoch 356/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0642 - acc: 1.0000 - val_loss: 0.1284 - val_acc: 0.9750\n",
      "Epoch 357/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0639 - acc: 1.0000 - val_loss: 0.1277 - val_acc: 0.9750\n",
      "Epoch 358/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.0635 - acc: 1.0000 - val_loss: 0.1273 - val_acc: 0.9750\n",
      "Epoch 359/800\n",
      "160/160 [==============================] - 0s 130us/step - loss: 0.0630 - acc: 1.0000 - val_loss: 0.1271 - val_acc: 0.9750\n",
      "Epoch 360/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0627 - acc: 1.0000 - val_loss: 0.1265 - val_acc: 0.9750\n",
      "Epoch 361/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 69us/step - loss: 0.0624 - acc: 1.0000 - val_loss: 0.1261 - val_acc: 0.9750\n",
      "Epoch 362/800\n",
      "160/160 [==============================] - 0s 125us/step - loss: 0.0620 - acc: 1.0000 - val_loss: 0.1253 - val_acc: 0.9750\n",
      "Epoch 363/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0616 - acc: 1.0000 - val_loss: 0.1251 - val_acc: 0.9750\n",
      "Epoch 364/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0612 - acc: 1.0000 - val_loss: 0.1247 - val_acc: 0.9750\n",
      "Epoch 365/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0609 - acc: 1.0000 - val_loss: 0.1243 - val_acc: 0.9750\n",
      "Epoch 366/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0605 - acc: 1.0000 - val_loss: 0.1242 - val_acc: 0.9750\n",
      "Epoch 367/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0602 - acc: 1.0000 - val_loss: 0.1234 - val_acc: 0.9750\n",
      "Epoch 368/800\n",
      "160/160 [==============================] - 0s 172us/step - loss: 0.0600 - acc: 1.0000 - val_loss: 0.1241 - val_acc: 0.9750\n",
      "Epoch 369/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0595 - acc: 1.0000 - val_loss: 0.1233 - val_acc: 0.9750\n",
      "Epoch 370/800\n",
      "160/160 [==============================] - 0s 143us/step - loss: 0.0592 - acc: 1.0000 - val_loss: 0.1234 - val_acc: 0.9750\n",
      "Epoch 371/800\n",
      "160/160 [==============================] - 0s 114us/step - loss: 0.0588 - acc: 1.0000 - val_loss: 0.1229 - val_acc: 0.9750\n",
      "Epoch 372/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.0585 - acc: 1.0000 - val_loss: 0.1223 - val_acc: 0.9750\n",
      "Epoch 373/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0582 - acc: 1.0000 - val_loss: 0.1222 - val_acc: 0.9750\n",
      "Epoch 374/800\n",
      "160/160 [==============================] - 0s 118us/step - loss: 0.0579 - acc: 1.0000 - val_loss: 0.1218 - val_acc: 0.9750\n",
      "Epoch 375/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.0575 - acc: 1.0000 - val_loss: 0.1211 - val_acc: 0.9750\n",
      "Epoch 376/800\n",
      "160/160 [==============================] - 0s 115us/step - loss: 0.0572 - acc: 1.0000 - val_loss: 0.1209 - val_acc: 0.9750\n",
      "Epoch 377/800\n",
      "160/160 [==============================] - 0s 109us/step - loss: 0.0569 - acc: 1.0000 - val_loss: 0.1206 - val_acc: 0.9750\n",
      "Epoch 378/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0565 - acc: 1.0000 - val_loss: 0.1205 - val_acc: 0.9750\n",
      "Epoch 379/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0564 - acc: 1.0000 - val_loss: 0.1204 - val_acc: 0.9750\n",
      "Epoch 380/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0560 - acc: 1.0000 - val_loss: 0.1196 - val_acc: 0.9750\n",
      "Epoch 381/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0557 - acc: 1.0000 - val_loss: 0.1188 - val_acc: 0.9750\n",
      "Epoch 382/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0554 - acc: 1.0000 - val_loss: 0.1193 - val_acc: 0.9750\n",
      "Epoch 383/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0550 - acc: 1.0000 - val_loss: 0.1186 - val_acc: 0.9750\n",
      "Epoch 384/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0547 - acc: 1.0000 - val_loss: 0.1181 - val_acc: 0.9750\n",
      "Epoch 385/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0545 - acc: 1.0000 - val_loss: 0.1182 - val_acc: 0.9750\n",
      "Epoch 386/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0541 - acc: 1.0000 - val_loss: 0.1180 - val_acc: 0.9750\n",
      "Epoch 387/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0538 - acc: 1.0000 - val_loss: 0.1177 - val_acc: 0.9750\n",
      "Epoch 388/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0536 - acc: 1.0000 - val_loss: 0.1169 - val_acc: 0.9750\n",
      "Epoch 389/800\n",
      "160/160 [==============================] - 0s 148us/step - loss: 0.0533 - acc: 1.0000 - val_loss: 0.1163 - val_acc: 0.9750\n",
      "Epoch 390/800\n",
      "160/160 [==============================] - 0s 110us/step - loss: 0.0530 - acc: 1.0000 - val_loss: 0.1167 - val_acc: 0.9750\n",
      "Epoch 391/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0527 - acc: 1.0000 - val_loss: 0.1164 - val_acc: 0.9750\n",
      "Epoch 392/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0524 - acc: 1.0000 - val_loss: 0.1159 - val_acc: 0.9750\n",
      "Epoch 393/800\n",
      "160/160 [==============================] - 0s 106us/step - loss: 0.0522 - acc: 1.0000 - val_loss: 0.1164 - val_acc: 0.9750\n",
      "Epoch 394/800\n",
      "160/160 [==============================] - 0s 110us/step - loss: 0.0518 - acc: 1.0000 - val_loss: 0.1158 - val_acc: 0.9750\n",
      "Epoch 395/800\n",
      "160/160 [==============================] - 0s 165us/step - loss: 0.0515 - acc: 1.0000 - val_loss: 0.1153 - val_acc: 0.9750\n",
      "Epoch 396/800\n",
      "160/160 [==============================] - 0s 107us/step - loss: 0.0513 - acc: 1.0000 - val_loss: 0.1151 - val_acc: 0.9750\n",
      "Epoch 397/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0510 - acc: 1.0000 - val_loss: 0.1144 - val_acc: 0.9750\n",
      "Epoch 398/800\n",
      "160/160 [==============================] - 0s 141us/step - loss: 0.0508 - acc: 1.0000 - val_loss: 0.1146 - val_acc: 0.9750\n",
      "Epoch 399/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0505 - acc: 1.0000 - val_loss: 0.1149 - val_acc: 0.9750\n",
      "Epoch 400/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0502 - acc: 1.0000 - val_loss: 0.1144 - val_acc: 0.9750\n",
      "Epoch 401/800\n",
      "160/160 [==============================] - 0s 146us/step - loss: 0.0500 - acc: 1.0000 - val_loss: 0.1140 - val_acc: 0.9750\n",
      "Epoch 402/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0498 - acc: 1.0000 - val_loss: 0.1140 - val_acc: 0.9750\n",
      "Epoch 403/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0495 - acc: 1.0000 - val_loss: 0.1135 - val_acc: 0.9750\n",
      "Epoch 404/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0492 - acc: 1.0000 - val_loss: 0.1132 - val_acc: 0.9750\n",
      "Epoch 405/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0490 - acc: 1.0000 - val_loss: 0.1134 - val_acc: 0.9750\n",
      "Epoch 406/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0487 - acc: 1.0000 - val_loss: 0.1126 - val_acc: 0.9750\n",
      "Epoch 407/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0484 - acc: 1.0000 - val_loss: 0.1126 - val_acc: 0.9750\n",
      "Epoch 408/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0482 - acc: 1.0000 - val_loss: 0.1117 - val_acc: 0.9750\n",
      "Epoch 409/800\n",
      "160/160 [==============================] - 0s 131us/step - loss: 0.0479 - acc: 1.0000 - val_loss: 0.1120 - val_acc: 0.9750\n",
      "Epoch 410/800\n",
      "160/160 [==============================] - 0s 109us/step - loss: 0.0477 - acc: 1.0000 - val_loss: 0.1114 - val_acc: 0.9750\n",
      "Epoch 411/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0475 - acc: 1.0000 - val_loss: 0.1114 - val_acc: 0.9750\n",
      "Epoch 412/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0472 - acc: 1.0000 - val_loss: 0.1108 - val_acc: 0.9750\n",
      "Epoch 413/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0470 - acc: 1.0000 - val_loss: 0.1106 - val_acc: 0.9750\n",
      "Epoch 414/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.0467 - acc: 1.0000 - val_loss: 0.1104 - val_acc: 0.9750\n",
      "Epoch 415/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0465 - acc: 1.0000 - val_loss: 0.1100 - val_acc: 0.9750\n",
      "Epoch 416/800\n",
      "160/160 [==============================] - 0s 143us/step - loss: 0.0463 - acc: 1.0000 - val_loss: 0.1103 - val_acc: 0.9750\n",
      "Epoch 417/800\n",
      "160/160 [==============================] - 0s 111us/step - loss: 0.0460 - acc: 1.0000 - val_loss: 0.1103 - val_acc: 0.9750\n",
      "Epoch 418/800\n",
      "160/160 [==============================] - 0s 104us/step - loss: 0.0458 - acc: 1.0000 - val_loss: 0.1097 - val_acc: 0.9750\n",
      "Epoch 419/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0456 - acc: 1.0000 - val_loss: 0.1100 - val_acc: 0.9750\n",
      "Epoch 420/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0454 - acc: 1.0000 - val_loss: 0.1096 - val_acc: 0.9750\n",
      "Epoch 421/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 77us/step - loss: 0.0452 - acc: 1.0000 - val_loss: 0.1095 - val_acc: 0.9750\n",
      "Epoch 422/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0449 - acc: 1.0000 - val_loss: 0.1095 - val_acc: 0.9750\n",
      "Epoch 423/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0447 - acc: 1.0000 - val_loss: 0.1090 - val_acc: 0.9750\n",
      "Epoch 424/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0445 - acc: 1.0000 - val_loss: 0.1080 - val_acc: 0.9750\n",
      "Epoch 425/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0443 - acc: 1.0000 - val_loss: 0.1079 - val_acc: 0.9750\n",
      "Epoch 426/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0441 - acc: 1.0000 - val_loss: 0.1085 - val_acc: 0.9750\n",
      "Epoch 427/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0438 - acc: 1.0000 - val_loss: 0.1086 - val_acc: 0.9750\n",
      "Epoch 428/800\n",
      "160/160 [==============================] - 0s 144us/step - loss: 0.0437 - acc: 1.0000 - val_loss: 0.1081 - val_acc: 0.9750\n",
      "Epoch 429/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0434 - acc: 1.0000 - val_loss: 0.1077 - val_acc: 0.9750\n",
      "Epoch 430/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0432 - acc: 1.0000 - val_loss: 0.1079 - val_acc: 0.9750\n",
      "Epoch 431/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0431 - acc: 1.0000 - val_loss: 0.1081 - val_acc: 0.9750\n",
      "Epoch 432/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0428 - acc: 1.0000 - val_loss: 0.1074 - val_acc: 0.9750\n",
      "Epoch 433/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0426 - acc: 1.0000 - val_loss: 0.1066 - val_acc: 0.9750\n",
      "Epoch 434/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0424 - acc: 1.0000 - val_loss: 0.1063 - val_acc: 0.9750\n",
      "Epoch 435/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0422 - acc: 1.0000 - val_loss: 0.1061 - val_acc: 0.9750\n",
      "Epoch 436/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0420 - acc: 1.0000 - val_loss: 0.1064 - val_acc: 0.9750\n",
      "Epoch 437/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0418 - acc: 1.0000 - val_loss: 0.1063 - val_acc: 0.9750\n",
      "Epoch 438/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0416 - acc: 1.0000 - val_loss: 0.1066 - val_acc: 0.9750\n",
      "Epoch 439/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0414 - acc: 1.0000 - val_loss: 0.1065 - val_acc: 0.9750\n",
      "Epoch 440/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0412 - acc: 1.0000 - val_loss: 0.1058 - val_acc: 0.9750\n",
      "Epoch 441/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0410 - acc: 1.0000 - val_loss: 0.1058 - val_acc: 0.9750\n",
      "Epoch 442/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0408 - acc: 1.0000 - val_loss: 0.1050 - val_acc: 0.9750\n",
      "Epoch 443/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0406 - acc: 1.0000 - val_loss: 0.1049 - val_acc: 0.9750\n",
      "Epoch 444/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0404 - acc: 1.0000 - val_loss: 0.1046 - val_acc: 0.9750\n",
      "Epoch 445/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0402 - acc: 1.0000 - val_loss: 0.1046 - val_acc: 0.9750\n",
      "Epoch 446/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0401 - acc: 1.0000 - val_loss: 0.1048 - val_acc: 0.9750\n",
      "Epoch 447/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0399 - acc: 1.0000 - val_loss: 0.1043 - val_acc: 0.9750\n",
      "Epoch 448/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0397 - acc: 1.0000 - val_loss: 0.1043 - val_acc: 0.9750\n",
      "Epoch 449/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0395 - acc: 1.0000 - val_loss: 0.1040 - val_acc: 0.9750\n",
      "Epoch 450/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0393 - acc: 1.0000 - val_loss: 0.1036 - val_acc: 0.9750\n",
      "Epoch 451/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0391 - acc: 1.0000 - val_loss: 0.1037 - val_acc: 0.9750\n",
      "Epoch 452/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.0390 - acc: 1.0000 - val_loss: 0.1032 - val_acc: 0.9750\n",
      "Epoch 453/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0388 - acc: 1.0000 - val_loss: 0.1034 - val_acc: 0.9750\n",
      "Epoch 454/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0386 - acc: 1.0000 - val_loss: 0.1035 - val_acc: 0.9750\n",
      "Epoch 455/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0385 - acc: 1.0000 - val_loss: 0.1030 - val_acc: 0.9750\n",
      "Epoch 456/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0383 - acc: 1.0000 - val_loss: 0.1025 - val_acc: 0.9750\n",
      "Epoch 457/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0381 - acc: 1.0000 - val_loss: 0.1029 - val_acc: 0.9750\n",
      "Epoch 458/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0380 - acc: 1.0000 - val_loss: 0.1024 - val_acc: 0.9750\n",
      "Epoch 459/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0378 - acc: 1.0000 - val_loss: 0.1032 - val_acc: 0.9750\n",
      "Epoch 460/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0376 - acc: 1.0000 - val_loss: 0.1029 - val_acc: 0.9750\n",
      "Epoch 461/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0375 - acc: 1.0000 - val_loss: 0.1019 - val_acc: 0.9750\n",
      "Epoch 462/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0373 - acc: 1.0000 - val_loss: 0.1018 - val_acc: 0.9750\n",
      "Epoch 463/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0371 - acc: 1.0000 - val_loss: 0.1017 - val_acc: 0.9750\n",
      "Epoch 464/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0370 - acc: 1.0000 - val_loss: 0.1008 - val_acc: 0.9750\n",
      "Epoch 465/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0368 - acc: 1.0000 - val_loss: 0.1007 - val_acc: 0.9750\n",
      "Epoch 466/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0366 - acc: 1.0000 - val_loss: 0.1014 - val_acc: 0.9750\n",
      "Epoch 467/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0365 - acc: 1.0000 - val_loss: 0.1021 - val_acc: 0.9750\n",
      "Epoch 468/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0363 - acc: 1.0000 - val_loss: 0.1019 - val_acc: 0.9750\n",
      "Epoch 469/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0361 - acc: 1.0000 - val_loss: 0.1014 - val_acc: 0.9750\n",
      "Epoch 470/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0359 - acc: 1.0000 - val_loss: 0.1009 - val_acc: 0.9750\n",
      "Epoch 471/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0358 - acc: 1.0000 - val_loss: 0.1008 - val_acc: 0.9750\n",
      "Epoch 472/800\n",
      "160/160 [==============================] - 0s 52us/step - loss: 0.0357 - acc: 1.0000 - val_loss: 0.1003 - val_acc: 0.9750\n",
      "Epoch 473/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0355 - acc: 1.0000 - val_loss: 0.1006 - val_acc: 0.9750\n",
      "Epoch 474/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0353 - acc: 1.0000 - val_loss: 0.1007 - val_acc: 0.9750\n",
      "Epoch 475/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0352 - acc: 1.0000 - val_loss: 0.1005 - val_acc: 0.9750\n",
      "Epoch 476/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0350 - acc: 1.0000 - val_loss: 0.1002 - val_acc: 0.9750\n",
      "Epoch 477/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0349 - acc: 1.0000 - val_loss: 0.1003 - val_acc: 0.9750\n",
      "Epoch 478/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0347 - acc: 1.0000 - val_loss: 0.0997 - val_acc: 0.9750\n",
      "Epoch 479/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0346 - acc: 1.0000 - val_loss: 0.0997 - val_acc: 0.9750\n",
      "Epoch 480/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0345 - acc: 1.0000 - val_loss: 0.1003 - val_acc: 0.9750\n",
      "Epoch 481/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 74us/step - loss: 0.0343 - acc: 1.0000 - val_loss: 0.1001 - val_acc: 0.9750\n",
      "Epoch 482/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0341 - acc: 1.0000 - val_loss: 0.0997 - val_acc: 0.9750\n",
      "Epoch 483/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0340 - acc: 1.0000 - val_loss: 0.0991 - val_acc: 0.9750\n",
      "Epoch 484/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0338 - acc: 1.0000 - val_loss: 0.0990 - val_acc: 0.9750\n",
      "Epoch 485/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0338 - acc: 1.0000 - val_loss: 0.0986 - val_acc: 0.9750\n",
      "Epoch 486/800\n",
      "160/160 [==============================] - 0s 53us/step - loss: 0.0335 - acc: 1.0000 - val_loss: 0.0989 - val_acc: 0.9750\n",
      "Epoch 487/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0334 - acc: 1.0000 - val_loss: 0.0989 - val_acc: 0.9750\n",
      "Epoch 488/800\n",
      "160/160 [==============================] - 0s 54us/step - loss: 0.0333 - acc: 1.0000 - val_loss: 0.0987 - val_acc: 0.9750\n",
      "Epoch 489/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0332 - acc: 1.0000 - val_loss: 0.0987 - val_acc: 0.9750\n",
      "Epoch 490/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0330 - acc: 1.0000 - val_loss: 0.0987 - val_acc: 0.9750\n",
      "Epoch 491/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0328 - acc: 1.0000 - val_loss: 0.0985 - val_acc: 0.9750\n",
      "Epoch 492/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0327 - acc: 1.0000 - val_loss: 0.0981 - val_acc: 0.9750\n",
      "Epoch 493/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0326 - acc: 1.0000 - val_loss: 0.0975 - val_acc: 0.9750\n",
      "Epoch 494/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0324 - acc: 1.0000 - val_loss: 0.0972 - val_acc: 0.9750\n",
      "Epoch 495/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0323 - acc: 1.0000 - val_loss: 0.0972 - val_acc: 0.9750\n",
      "Epoch 496/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0322 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 0.9750\n",
      "Epoch 497/800\n",
      "160/160 [==============================] - 0s 52us/step - loss: 0.0320 - acc: 1.0000 - val_loss: 0.0975 - val_acc: 0.9750\n",
      "Epoch 498/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0319 - acc: 1.0000 - val_loss: 0.0972 - val_acc: 0.9750\n",
      "Epoch 499/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0318 - acc: 1.0000 - val_loss: 0.0978 - val_acc: 0.9750\n",
      "Epoch 500/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0316 - acc: 1.0000 - val_loss: 0.0977 - val_acc: 0.9750\n",
      "Epoch 501/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0315 - acc: 1.0000 - val_loss: 0.0972 - val_acc: 0.9750\n",
      "Epoch 502/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0314 - acc: 1.0000 - val_loss: 0.0971 - val_acc: 0.9750\n",
      "Epoch 503/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0313 - acc: 1.0000 - val_loss: 0.0974 - val_acc: 0.9750\n",
      "Epoch 504/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0311 - acc: 1.0000 - val_loss: 0.0971 - val_acc: 0.9750\n",
      "Epoch 505/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0310 - acc: 1.0000 - val_loss: 0.0975 - val_acc: 0.9750\n",
      "Epoch 506/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0308 - acc: 1.0000 - val_loss: 0.0971 - val_acc: 0.9750\n",
      "Epoch 507/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0307 - acc: 1.0000 - val_loss: 0.0964 - val_acc: 0.9750\n",
      "Epoch 508/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0306 - acc: 1.0000 - val_loss: 0.0961 - val_acc: 0.9750\n",
      "Epoch 509/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0305 - acc: 1.0000 - val_loss: 0.0965 - val_acc: 0.9750\n",
      "Epoch 510/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0304 - acc: 1.0000 - val_loss: 0.0960 - val_acc: 0.9750\n",
      "Epoch 511/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0302 - acc: 1.0000 - val_loss: 0.0959 - val_acc: 0.9750\n",
      "Epoch 512/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0301 - acc: 1.0000 - val_loss: 0.0955 - val_acc: 0.9750\n",
      "Epoch 513/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0300 - acc: 1.0000 - val_loss: 0.0951 - val_acc: 0.9750\n",
      "Epoch 514/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0299 - acc: 1.0000 - val_loss: 0.0954 - val_acc: 0.9750\n",
      "Epoch 515/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0298 - acc: 1.0000 - val_loss: 0.0950 - val_acc: 0.9750\n",
      "Epoch 516/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0297 - acc: 1.0000 - val_loss: 0.0945 - val_acc: 0.9750\n",
      "Epoch 517/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0295 - acc: 1.0000 - val_loss: 0.0952 - val_acc: 0.9750\n",
      "Epoch 518/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0294 - acc: 1.0000 - val_loss: 0.0954 - val_acc: 0.9750\n",
      "Epoch 519/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0292 - acc: 1.0000 - val_loss: 0.0951 - val_acc: 0.9750\n",
      "Epoch 520/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0291 - acc: 1.0000 - val_loss: 0.0951 - val_acc: 0.9750\n",
      "Epoch 521/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0290 - acc: 1.0000 - val_loss: 0.0948 - val_acc: 0.9750\n",
      "Epoch 522/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0289 - acc: 1.0000 - val_loss: 0.0946 - val_acc: 0.9750\n",
      "Epoch 523/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0288 - acc: 1.0000 - val_loss: 0.0953 - val_acc: 0.9750\n",
      "Epoch 524/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0287 - acc: 1.0000 - val_loss: 0.0950 - val_acc: 0.9750\n",
      "Epoch 525/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0286 - acc: 1.0000 - val_loss: 0.0952 - val_acc: 0.9750\n",
      "Epoch 526/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0285 - acc: 1.0000 - val_loss: 0.0947 - val_acc: 0.9750\n",
      "Epoch 527/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0283 - acc: 1.0000 - val_loss: 0.0950 - val_acc: 0.9750\n",
      "Epoch 528/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0282 - acc: 1.0000 - val_loss: 0.0945 - val_acc: 0.9750\n",
      "Epoch 529/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0281 - acc: 1.0000 - val_loss: 0.0939 - val_acc: 0.9750\n",
      "Epoch 530/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0280 - acc: 1.0000 - val_loss: 0.0944 - val_acc: 0.9750\n",
      "Epoch 531/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0279 - acc: 1.0000 - val_loss: 0.0940 - val_acc: 0.9750\n",
      "Epoch 532/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0279 - acc: 1.0000 - val_loss: 0.0941 - val_acc: 0.9750\n",
      "Epoch 533/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0277 - acc: 1.0000 - val_loss: 0.0944 - val_acc: 0.9750\n",
      "Epoch 534/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0276 - acc: 1.0000 - val_loss: 0.0941 - val_acc: 0.9750\n",
      "Epoch 535/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0275 - acc: 1.0000 - val_loss: 0.0933 - val_acc: 0.9750\n",
      "Epoch 536/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0274 - acc: 1.0000 - val_loss: 0.0929 - val_acc: 0.9750\n",
      "Epoch 537/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0273 - acc: 1.0000 - val_loss: 0.0927 - val_acc: 0.9750\n",
      "Epoch 538/800\n",
      "160/160 [==============================] - 0s 54us/step - loss: 0.0271 - acc: 1.0000 - val_loss: 0.0929 - val_acc: 0.9750\n",
      "Epoch 539/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0271 - acc: 1.0000 - val_loss: 0.0937 - val_acc: 0.9750\n",
      "Epoch 540/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0270 - acc: 1.0000 - val_loss: 0.0934 - val_acc: 0.9750\n",
      "Epoch 541/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 62us/step - loss: 0.0268 - acc: 1.0000 - val_loss: 0.0933 - val_acc: 0.9750\n",
      "Epoch 542/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0267 - acc: 1.0000 - val_loss: 0.0931 - val_acc: 0.9750\n",
      "Epoch 543/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0266 - acc: 1.0000 - val_loss: 0.0935 - val_acc: 0.9750\n",
      "Epoch 544/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0265 - acc: 1.0000 - val_loss: 0.0932 - val_acc: 0.9750\n",
      "Epoch 545/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0264 - acc: 1.0000 - val_loss: 0.0931 - val_acc: 0.9750\n",
      "Epoch 546/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0263 - acc: 1.0000 - val_loss: 0.0929 - val_acc: 0.9750\n",
      "Epoch 547/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0263 - acc: 1.0000 - val_loss: 0.0934 - val_acc: 0.9750\n",
      "Epoch 548/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0261 - acc: 1.0000 - val_loss: 0.0932 - val_acc: 0.9750\n",
      "Epoch 549/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0260 - acc: 1.0000 - val_loss: 0.0931 - val_acc: 0.9750\n",
      "Epoch 550/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0259 - acc: 1.0000 - val_loss: 0.0931 - val_acc: 0.9750\n",
      "Epoch 551/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0258 - acc: 1.0000 - val_loss: 0.0931 - val_acc: 0.9750\n",
      "Epoch 552/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0258 - acc: 1.0000 - val_loss: 0.0921 - val_acc: 0.9750\n",
      "Epoch 553/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0256 - acc: 1.0000 - val_loss: 0.0923 - val_acc: 0.9750\n",
      "Epoch 554/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0255 - acc: 1.0000 - val_loss: 0.0925 - val_acc: 0.9750\n",
      "Epoch 555/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0255 - acc: 1.0000 - val_loss: 0.0921 - val_acc: 0.9750\n",
      "Epoch 556/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0253 - acc: 1.0000 - val_loss: 0.0916 - val_acc: 0.9750\n",
      "Epoch 557/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0252 - acc: 1.0000 - val_loss: 0.0917 - val_acc: 0.9750\n",
      "Epoch 558/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0251 - acc: 1.0000 - val_loss: 0.0917 - val_acc: 0.9750\n",
      "Epoch 559/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0251 - acc: 1.0000 - val_loss: 0.0913 - val_acc: 0.9750\n",
      "Epoch 560/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0249 - acc: 1.0000 - val_loss: 0.0913 - val_acc: 0.9750\n",
      "Epoch 561/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0249 - acc: 1.0000 - val_loss: 0.0914 - val_acc: 0.9750\n",
      "Epoch 562/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0248 - acc: 1.0000 - val_loss: 0.0916 - val_acc: 0.9750\n",
      "Epoch 563/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0247 - acc: 1.0000 - val_loss: 0.0918 - val_acc: 0.9750\n",
      "Epoch 564/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0246 - acc: 1.0000 - val_loss: 0.0919 - val_acc: 0.9750\n",
      "Epoch 565/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0245 - acc: 1.0000 - val_loss: 0.0909 - val_acc: 0.9750\n",
      "Epoch 566/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0244 - acc: 1.0000 - val_loss: 0.0914 - val_acc: 0.9750\n",
      "Epoch 567/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0243 - acc: 1.0000 - val_loss: 0.0904 - val_acc: 0.9750\n",
      "Epoch 568/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0242 - acc: 1.0000 - val_loss: 0.0908 - val_acc: 0.9750\n",
      "Epoch 569/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0241 - acc: 1.0000 - val_loss: 0.0914 - val_acc: 0.9750\n",
      "Epoch 570/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0240 - acc: 1.0000 - val_loss: 0.0917 - val_acc: 0.9750\n",
      "Epoch 571/800\n",
      "160/160 [==============================] - 0s 54us/step - loss: 0.0239 - acc: 1.0000 - val_loss: 0.0913 - val_acc: 0.9750\n",
      "Epoch 572/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0239 - acc: 1.0000 - val_loss: 0.0910 - val_acc: 0.9750\n",
      "Epoch 573/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0238 - acc: 1.0000 - val_loss: 0.0912 - val_acc: 0.9750\n",
      "Epoch 574/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0237 - acc: 1.0000 - val_loss: 0.0904 - val_acc: 0.9750\n",
      "Epoch 575/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0236 - acc: 1.0000 - val_loss: 0.0902 - val_acc: 0.9750\n",
      "Epoch 576/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0235 - acc: 1.0000 - val_loss: 0.0907 - val_acc: 0.9750\n",
      "Epoch 577/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0234 - acc: 1.0000 - val_loss: 0.0905 - val_acc: 0.9750\n",
      "Epoch 578/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0234 - acc: 1.0000 - val_loss: 0.0900 - val_acc: 0.9750\n",
      "Epoch 579/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0233 - acc: 1.0000 - val_loss: 0.0901 - val_acc: 0.9750\n",
      "Epoch 580/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0232 - acc: 1.0000 - val_loss: 0.0897 - val_acc: 0.9750\n",
      "Epoch 581/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0231 - acc: 1.0000 - val_loss: 0.0895 - val_acc: 0.9750\n",
      "Epoch 582/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0230 - acc: 1.0000 - val_loss: 0.0892 - val_acc: 0.9750\n",
      "Epoch 583/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0230 - acc: 1.0000 - val_loss: 0.0904 - val_acc: 0.9750\n",
      "Epoch 584/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0228 - acc: 1.0000 - val_loss: 0.0905 - val_acc: 0.9750\n",
      "Epoch 585/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0228 - acc: 1.0000 - val_loss: 0.0908 - val_acc: 0.9750\n",
      "Epoch 586/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0226 - acc: 1.0000 - val_loss: 0.0906 - val_acc: 0.9750\n",
      "Epoch 587/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0226 - acc: 1.0000 - val_loss: 0.0905 - val_acc: 0.9750\n",
      "Epoch 588/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0225 - acc: 1.0000 - val_loss: 0.0897 - val_acc: 0.9750\n",
      "Epoch 589/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0224 - acc: 1.0000 - val_loss: 0.0897 - val_acc: 0.9750\n",
      "Epoch 590/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0223 - acc: 1.0000 - val_loss: 0.0895 - val_acc: 0.9750\n",
      "Epoch 591/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0223 - acc: 1.0000 - val_loss: 0.0894 - val_acc: 0.9750\n",
      "Epoch 592/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0222 - acc: 1.0000 - val_loss: 0.0895 - val_acc: 0.9750\n",
      "Epoch 593/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0221 - acc: 1.0000 - val_loss: 0.0888 - val_acc: 0.9750\n",
      "Epoch 594/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0220 - acc: 1.0000 - val_loss: 0.0895 - val_acc: 0.9750\n",
      "Epoch 595/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0219 - acc: 1.0000 - val_loss: 0.0898 - val_acc: 0.9750\n",
      "Epoch 596/800\n",
      "160/160 [==============================] - 0s 53us/step - loss: 0.0219 - acc: 1.0000 - val_loss: 0.0894 - val_acc: 0.9750\n",
      "Epoch 597/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0218 - acc: 1.0000 - val_loss: 0.0897 - val_acc: 0.9750\n",
      "Epoch 598/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0217 - acc: 1.0000 - val_loss: 0.0889 - val_acc: 0.9750\n",
      "Epoch 599/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0216 - acc: 1.0000 - val_loss: 0.0890 - val_acc: 0.9750\n",
      "Epoch 600/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0216 - acc: 1.0000 - val_loss: 0.0885 - val_acc: 0.9750\n",
      "Epoch 601/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 71us/step - loss: 0.0215 - acc: 1.0000 - val_loss: 0.0884 - val_acc: 0.9750\n",
      "Epoch 602/800\n",
      "160/160 [==============================] - 0s 51us/step - loss: 0.0214 - acc: 1.0000 - val_loss: 0.0887 - val_acc: 0.9750\n",
      "Epoch 603/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0213 - acc: 1.0000 - val_loss: 0.0886 - val_acc: 0.9750\n",
      "Epoch 604/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0213 - acc: 1.0000 - val_loss: 0.0884 - val_acc: 0.9750\n",
      "Epoch 605/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0212 - acc: 1.0000 - val_loss: 0.0880 - val_acc: 0.9750\n",
      "Epoch 606/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0211 - acc: 1.0000 - val_loss: 0.0885 - val_acc: 0.9750\n",
      "Epoch 607/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0210 - acc: 1.0000 - val_loss: 0.0887 - val_acc: 0.9750\n",
      "Epoch 608/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0209 - acc: 1.0000 - val_loss: 0.0886 - val_acc: 0.9750\n",
      "Epoch 609/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0209 - acc: 1.0000 - val_loss: 0.0882 - val_acc: 0.9750\n",
      "Epoch 610/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0208 - acc: 1.0000 - val_loss: 0.0885 - val_acc: 0.9750\n",
      "Epoch 611/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0207 - acc: 1.0000 - val_loss: 0.0888 - val_acc: 0.9750\n",
      "Epoch 612/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0207 - acc: 1.0000 - val_loss: 0.0882 - val_acc: 0.9750\n",
      "Epoch 613/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0206 - acc: 1.0000 - val_loss: 0.0892 - val_acc: 0.9750\n",
      "Epoch 614/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.0893 - val_acc: 0.9750\n",
      "Epoch 615/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.0881 - val_acc: 0.9750\n",
      "Epoch 616/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0203 - acc: 1.0000 - val_loss: 0.0883 - val_acc: 0.9750\n",
      "Epoch 617/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0203 - acc: 1.0000 - val_loss: 0.0880 - val_acc: 0.9750\n",
      "Epoch 618/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0202 - acc: 1.0000 - val_loss: 0.0885 - val_acc: 0.9750\n",
      "Epoch 619/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.0885 - val_acc: 0.9750\n",
      "Epoch 620/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.0874 - val_acc: 0.9750\n",
      "Epoch 621/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0200 - acc: 1.0000 - val_loss: 0.0879 - val_acc: 0.9750\n",
      "Epoch 622/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0199 - acc: 1.0000 - val_loss: 0.0879 - val_acc: 0.9750\n",
      "Epoch 623/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0199 - acc: 1.0000 - val_loss: 0.0871 - val_acc: 0.9750\n",
      "Epoch 624/800\n",
      "160/160 [==============================] - 0s 52us/step - loss: 0.0199 - acc: 1.0000 - val_loss: 0.0880 - val_acc: 0.9750\n",
      "Epoch 625/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.0877 - val_acc: 0.9750\n",
      "Epoch 626/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0196 - acc: 1.0000 - val_loss: 0.0878 - val_acc: 0.9750\n",
      "Epoch 627/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0196 - acc: 1.0000 - val_loss: 0.0871 - val_acc: 0.9750\n",
      "Epoch 628/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.0868 - val_acc: 0.9750\n",
      "Epoch 629/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.0872 - val_acc: 0.9750\n",
      "Epoch 630/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0194 - acc: 1.0000 - val_loss: 0.0870 - val_acc: 0.9750\n",
      "Epoch 631/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.0874 - val_acc: 0.9750\n",
      "Epoch 632/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.0869 - val_acc: 0.9750\n",
      "Epoch 633/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.0867 - val_acc: 0.9750\n",
      "Epoch 634/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0191 - acc: 1.0000 - val_loss: 0.0862 - val_acc: 0.9750\n",
      "Epoch 635/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.0863 - val_acc: 0.9750\n",
      "Epoch 636/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.0867 - val_acc: 0.9750\n",
      "Epoch 637/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.0872 - val_acc: 0.9750\n",
      "Epoch 638/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.0880 - val_acc: 0.9750\n",
      "Epoch 639/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.0879 - val_acc: 0.9750\n",
      "Epoch 640/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.0876 - val_acc: 0.9750\n",
      "Epoch 641/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.0865 - val_acc: 0.9750\n",
      "Epoch 642/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.0867 - val_acc: 0.9750\n",
      "Epoch 643/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.0861 - val_acc: 0.9750\n",
      "Epoch 644/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.0867 - val_acc: 0.9750\n",
      "Epoch 645/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.0863 - val_acc: 0.9750\n",
      "Epoch 646/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.0864 - val_acc: 0.9750\n",
      "Epoch 647/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.0865 - val_acc: 0.9750\n",
      "Epoch 648/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.0857 - val_acc: 0.9750\n",
      "Epoch 649/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.0850 - val_acc: 0.9750\n",
      "Epoch 650/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.0860 - val_acc: 0.9750\n",
      "Epoch 651/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.0860 - val_acc: 0.9750\n",
      "Epoch 652/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.0866 - val_acc: 0.9750\n",
      "Epoch 653/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.0871 - val_acc: 0.9750\n",
      "Epoch 654/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.0867 - val_acc: 0.9750\n",
      "Epoch 655/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.0870 - val_acc: 0.9750\n",
      "Epoch 656/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.0860 - val_acc: 0.9750\n",
      "Epoch 657/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.0851 - val_acc: 0.9750\n",
      "Epoch 658/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.0854 - val_acc: 0.9750\n",
      "Epoch 659/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.0855 - val_acc: 0.9750\n",
      "Epoch 660/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.0859 - val_acc: 0.9750\n",
      "Epoch 661/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 61us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.0862 - val_acc: 0.9750\n",
      "Epoch 662/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.0857 - val_acc: 0.9750\n",
      "Epoch 663/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.0855 - val_acc: 0.9750\n",
      "Epoch 664/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.0859 - val_acc: 0.9750\n",
      "Epoch 665/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.0861 - val_acc: 0.9750\n",
      "Epoch 666/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0860 - val_acc: 0.9750\n",
      "Epoch 667/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0853 - val_acc: 0.9750\n",
      "Epoch 668/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0170 - acc: 1.0000 - val_loss: 0.0855 - val_acc: 0.9750\n",
      "Epoch 669/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0170 - acc: 1.0000 - val_loss: 0.0853 - val_acc: 0.9750\n",
      "Epoch 670/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0170 - acc: 1.0000 - val_loss: 0.0861 - val_acc: 0.9750\n",
      "Epoch 671/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 0.0861 - val_acc: 0.9750\n",
      "Epoch 672/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 0.0854 - val_acc: 0.9750\n",
      "Epoch 673/800\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.0167 - acc: 1.0000 - val_loss: 0.0852 - val_acc: 0.9750\n",
      "Epoch 674/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0167 - acc: 1.0000 - val_loss: 0.0852 - val_acc: 0.9750\n",
      "Epoch 675/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.0855 - val_acc: 0.9750\n",
      "Epoch 676/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.0848 - val_acc: 0.9750\n",
      "Epoch 677/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.0845 - val_acc: 0.9750\n",
      "Epoch 678/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.0848 - val_acc: 0.9750\n",
      "Epoch 679/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0164 - acc: 1.0000 - val_loss: 0.0849 - val_acc: 0.9750\n",
      "Epoch 680/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.0850 - val_acc: 0.9750\n",
      "Epoch 681/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.0852 - val_acc: 0.9750\n",
      "Epoch 682/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.0852 - val_acc: 0.9750\n",
      "Epoch 683/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.0844 - val_acc: 0.9750\n",
      "Epoch 684/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.0846 - val_acc: 0.9750\n",
      "Epoch 685/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.0838 - val_acc: 0.9750\n",
      "Epoch 686/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.0842 - val_acc: 0.9750\n",
      "Epoch 687/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.0849 - val_acc: 0.9750\n",
      "Epoch 688/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.0845 - val_acc: 0.9750\n",
      "Epoch 689/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.0846 - val_acc: 0.9750\n",
      "Epoch 690/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.0846 - val_acc: 0.9750\n",
      "Epoch 691/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.0848 - val_acc: 0.9750\n",
      "Epoch 692/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.0843 - val_acc: 0.9750\n",
      "Epoch 693/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.0848 - val_acc: 0.9750\n",
      "Epoch 694/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.0843 - val_acc: 0.9750\n",
      "Epoch 695/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.0850 - val_acc: 0.9750\n",
      "Epoch 696/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0155 - acc: 1.0000 - val_loss: 0.0845 - val_acc: 0.9750\n",
      "Epoch 697/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0155 - acc: 1.0000 - val_loss: 0.0842 - val_acc: 0.9750\n",
      "Epoch 698/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0154 - acc: 1.0000 - val_loss: 0.0846 - val_acc: 0.9750\n",
      "Epoch 699/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 0.0843 - val_acc: 0.9750\n",
      "Epoch 700/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 0.0837 - val_acc: 0.9750\n",
      "Epoch 701/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 0.0842 - val_acc: 0.9750\n",
      "Epoch 702/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 0.0849 - val_acc: 0.9750\n",
      "Epoch 703/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.0844 - val_acc: 0.9750\n",
      "Epoch 704/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.0839 - val_acc: 0.9750\n",
      "Epoch 705/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 0.9750\n",
      "Epoch 706/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 0.0836 - val_acc: 0.9750\n",
      "Epoch 707/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0149 - acc: 1.0000 - val_loss: 0.0839 - val_acc: 0.9750\n",
      "Epoch 708/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0149 - acc: 1.0000 - val_loss: 0.0838 - val_acc: 0.9750\n",
      "Epoch 709/800\n",
      "160/160 [==============================] - 0s 53us/step - loss: 0.0149 - acc: 1.0000 - val_loss: 0.0838 - val_acc: 0.9750\n",
      "Epoch 710/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.0838 - val_acc: 0.9750\n",
      "Epoch 711/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.0846 - val_acc: 0.9750\n",
      "Epoch 712/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.0843 - val_acc: 0.9750\n",
      "Epoch 713/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.0835 - val_acc: 0.9750\n",
      "Epoch 714/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0146 - acc: 1.0000 - val_loss: 0.0829 - val_acc: 0.9750\n",
      "Epoch 715/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0146 - acc: 1.0000 - val_loss: 0.0826 - val_acc: 0.9750\n",
      "Epoch 716/800\n",
      "160/160 [==============================] - 0s 143us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 0.0837 - val_acc: 0.9750\n",
      "Epoch 717/800\n",
      "160/160 [==============================] - 0s 109us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 0.0837 - val_acc: 0.9750\n",
      "Epoch 718/800\n",
      "160/160 [==============================] - 0s 99us/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.0836 - val_acc: 0.9750\n",
      "Epoch 719/800\n",
      "160/160 [==============================] - 0s 112us/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.0840 - val_acc: 0.9750\n",
      "Epoch 720/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.0839 - val_acc: 0.9750\n",
      "Epoch 721/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 78us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.0841 - val_acc: 0.9750\n",
      "Epoch 722/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.0836 - val_acc: 0.9750\n",
      "Epoch 723/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 0.9750\n",
      "Epoch 724/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.0831 - val_acc: 0.9750\n",
      "Epoch 725/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0141 - acc: 1.0000 - val_loss: 0.0839 - val_acc: 0.9750\n",
      "Epoch 726/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 0.0837 - val_acc: 0.9750\n",
      "Epoch 727/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 0.0833 - val_acc: 0.9750\n",
      "Epoch 728/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 0.0838 - val_acc: 0.9750\n",
      "Epoch 729/800\n",
      "160/160 [==============================] - 0s 51us/step - loss: 0.0139 - acc: 1.0000 - val_loss: 0.0833 - val_acc: 0.9750\n",
      "Epoch 730/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0139 - acc: 1.0000 - val_loss: 0.0830 - val_acc: 0.9750\n",
      "Epoch 731/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0138 - acc: 1.0000 - val_loss: 0.0830 - val_acc: 0.9750\n",
      "Epoch 732/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0138 - acc: 1.0000 - val_loss: 0.0830 - val_acc: 0.9750\n",
      "Epoch 733/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0137 - acc: 1.0000 - val_loss: 0.0827 - val_acc: 0.9750\n",
      "Epoch 734/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0137 - acc: 1.0000 - val_loss: 0.0824 - val_acc: 0.9750\n",
      "Epoch 735/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 0.0829 - val_acc: 0.9750\n",
      "Epoch 736/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 0.0835 - val_acc: 0.9750\n",
      "Epoch 737/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 0.0834 - val_acc: 0.9750\n",
      "Epoch 738/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 0.0837 - val_acc: 0.9750\n",
      "Epoch 739/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 0.9750\n",
      "Epoch 740/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.0831 - val_acc: 0.9750\n",
      "Epoch 741/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.0826 - val_acc: 0.9750\n",
      "Epoch 742/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.0834 - val_acc: 0.9750\n",
      "Epoch 743/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.0827 - val_acc: 0.9750\n",
      "Epoch 744/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.0822 - val_acc: 0.9750\n",
      "Epoch 745/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.0818 - val_acc: 0.9750\n",
      "Epoch 746/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.0829 - val_acc: 0.9750\n",
      "Epoch 747/800\n",
      "160/160 [==============================] - 0s 53us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.0835 - val_acc: 0.9750\n",
      "Epoch 748/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.0829 - val_acc: 0.9750\n",
      "Epoch 749/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.0825 - val_acc: 0.9750\n",
      "Epoch 750/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.0828 - val_acc: 0.9750\n",
      "Epoch 751/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.0821 - val_acc: 0.9750\n",
      "Epoch 752/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.0828 - val_acc: 0.9750\n",
      "Epoch 753/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.0837 - val_acc: 0.9750\n",
      "Epoch 754/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.0828 - val_acc: 0.9750\n",
      "Epoch 755/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.0827 - val_acc: 0.9750\n",
      "Epoch 756/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.0828 - val_acc: 0.9750\n",
      "Epoch 757/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.0820 - val_acc: 0.9750\n",
      "Epoch 758/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.0821 - val_acc: 0.9750\n",
      "Epoch 759/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 0.9750\n",
      "Epoch 760/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.0827 - val_acc: 0.9750\n",
      "Epoch 761/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.0824 - val_acc: 0.9750\n",
      "Epoch 762/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.0820 - val_acc: 0.9750\n",
      "Epoch 763/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.0825 - val_acc: 0.9750\n",
      "Epoch 764/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.0827 - val_acc: 0.9750\n",
      "Epoch 765/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.0830 - val_acc: 0.9750\n",
      "Epoch 766/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.0831 - val_acc: 0.9750\n",
      "Epoch 767/800\n",
      "160/160 [==============================] - 0s 115us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.0829 - val_acc: 0.9750\n",
      "Epoch 768/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.0821 - val_acc: 0.9750\n",
      "Epoch 769/800\n",
      "160/160 [==============================] - 0s 105us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.0822 - val_acc: 0.9750\n",
      "Epoch 770/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.0824 - val_acc: 0.9750\n",
      "Epoch 771/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.0820 - val_acc: 0.9750\n",
      "Epoch 772/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.0822 - val_acc: 0.9750\n",
      "Epoch 773/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.0816 - val_acc: 0.9750\n",
      "Epoch 774/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.0824 - val_acc: 0.9750\n",
      "Epoch 775/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.0823 - val_acc: 0.9750\n",
      "Epoch 776/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.0818 - val_acc: 0.9750\n",
      "Epoch 777/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.0814 - val_acc: 0.9750\n",
      "Epoch 778/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.0809 - val_acc: 0.9750\n",
      "Epoch 779/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.0817 - val_acc: 0.9750\n",
      "Epoch 780/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0118 - acc: 1.0000 - val_loss: 0.0820 - val_acc: 0.9750\n",
      "Epoch 781/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 72us/step - loss: 0.0118 - acc: 1.0000 - val_loss: 0.0828 - val_acc: 0.9750\n",
      "Epoch 782/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.0821 - val_acc: 0.9750\n",
      "Epoch 783/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 0.9750\n",
      "Epoch 784/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.0817 - val_acc: 0.9750\n",
      "Epoch 785/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.0810 - val_acc: 0.9750\n",
      "Epoch 786/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.0816 - val_acc: 0.9750\n",
      "Epoch 787/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.0818 - val_acc: 0.9750\n",
      "Epoch 788/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.0822 - val_acc: 0.9750\n",
      "Epoch 789/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 0.9750\n",
      "Epoch 790/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 0.9750\n",
      "Epoch 791/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 0.9750\n",
      "Epoch 792/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 0.0824 - val_acc: 0.9750\n",
      "Epoch 793/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.0820 - val_acc: 0.9750\n",
      "Epoch 794/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 0.9750\n",
      "Epoch 795/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.0813 - val_acc: 0.9750\n",
      "Epoch 796/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.0816 - val_acc: 0.9750\n",
      "Epoch 797/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.0815 - val_acc: 0.9750\n",
      "Epoch 798/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.0816 - val_acc: 0.9750\n",
      "Epoch 799/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.0820 - val_acc: 0.9750\n",
      "Epoch 800/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.0818 - val_acc: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa9273a0668>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_tanh.compile(optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "nn_tanh.fit(X, y, epochs=800, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa96408a320>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzsnXd4HNW5uN+Z2Z1ddcmWZFmW3CQ3ueJubGODAZsODoSaQEIgpOemQMiP1HtvSG4SUiAkIUAIJRC6Mc1gY2Pce5d7lSXZlqyu3Z3ZmfP7Y3ZlSTsrraRVs/d9Hh5gd/bMWe2c73znq5IQghgxYly4yN09gRgxYnQvMSEQI8YFTkwIxIhxgRMTAjFiXODEhECMGBc4MSEQI8YFTtSEgCRJiiRJWyVJejdaY8aIEaPziaYm8B2gMIrjxYgRowuIihCQJCkHuAZ4OhrjxYgRo+twRGmcPwIPAknhLpAk6X7gfoCEeGnSyHw1SreOESNGc46e0Ck7a0iRXNthISBJ0rXAaSHEZkmS5oa7TgjxFPAUwOTxbrFhSW5Hbx0jRowwTJ1/IuJro3EcmAlcL0nSUeAV4DJJkl6MwrgxYsToAjosBIQQDwshcoQQg4HbgE+EEHd1eGYxYsToEmJxAjFiXOBEyzAIgBBiBbAimmPGiBGjc4lpAjFiXODEhECMGBc4MSEQI8YFTkwIxIhxgRMTAjFiXODEhECMGBc4MSEQI8YFTkwIxIhxgRPVYKEYMWJAcUk6H38yldJTfckbWsS8uRtJS63t7mmFJSYEYsSIItt2DOevTy/E71cwTYWDh3P5ZMUUHv7hc+QOON3d07MldhyIccFz9HgWy1ZMZuPmUei60u5xDEPi6eduQNNUTNMax+934PG6+NeL10RrulEnpgnEuGDRdYU///VW9h8ciBASimwiKybf//ZLDB1c3Obxjh7PxjDt9lWJI8ey8fmcuFx6xyceZWKaQIwLlsXvz2bfgUFomoquO/H6XNTXx/GHx+/Ab7R9aciyCWFae0qAJPXMvp8xIRDjgmXFZ5PQdWfI635DpnDvkDaPNyi3FFUN3eklyWRY/nFU1d+ueXY2MSEQo1dR73GxdPlk/vXS1SxbMZl6j6vdY3l9YepcCom6enebx5NlwdfuewOXquFwWMJAVTUSEjx86Qs9txJ/zCYQo9dwsjiDX/3uHvx+BU1TUVWNt96Zy49/+BzZ/cvaPF7+0BMU7huCpayfw28oDMuLvEZfY0YOP8ajv/gLn66+iNLSvuQNKWLmxTuIj/O1a7yuIKYJxOg1/O3phdTXu9A0awfXNJW6ejd/f/amdo136+eWoqo6kmQ2vKaqGrMv3krfPtXtnmdaWg03XruSB77yFlfM29ijBQDEhECMHoRpwqEjA9i5O4/auqbqePnZZE6d7kPoIytTXJJBRWVim+83aGApjzz0LBPG7icxsY7+WWe44/Mf8oXbP2j/l+iFRKPkuBtYCbgC470uhPhZR8eNcWFxsjiDx564nbq6OGRZ4PcrXHXlam68diWSBIahIMn21nVJEhj+9vn3cwec5ttff7UjU+/1REMT8AGXCSHGAxOABZIkTY/CuDEuEPx+md889kXOnk3B53Ph8bjRdScffnwxGzaNBiAjvYKEeK/t55OT6ujbt6orp3xeEY2S40IIEQyMdgb+6ZkO0RjtQgjYs3cIz710NS+8vIADh3KiOv62ncPRdQfNDXSapvLekpkASBLce/ciVFVDlg0AZNlAVTXuvfsdpIh67cSwIyreAUmSFGAzkA/8RQixPhrjxog+uq6wfdcwqqsTyBtykkEDS1u83jThr09/jp27h+HzOZEkwaq1E5h98Tbuuu3DqMypoiI5bHBOZeW5znajRx3hZw8/zYcfz+DEyUwG5pSy4Ip19M8qj8o8LlSiIgSEEAYwQZKkVOAtSZLGCCF2Nb6mcS/CgQNinsnu4Oix/vzuz3diGLJ1xpYEI4Yd49tf+w8Oh2n7mS3bRrJzdz6+gE9dCAlNU/lszQSmTd7NsPz2udIakzugFMOwO9MLcnOaCqns/mV8+YuLG/6/qjqB9z68mNNn0sgbepJpU3bh6qFBOT2VqHoHhBCVwHJggc17TwkhJgshJmf0bX+SRoz2YRgSjz1+B3V18Xi91plb01T27h/M4vdnh/3cZ2sm4POFBuTouoO1G8ZGZW6VVWH72JKVFd7/v//AQB76ybdY9N4cVq6exL9fnc+PfvJNzlaEHy9GKB0WApIkZQQ0ACRJigOuAPZ2dNwY0aVw3xB0f6gGputOPvl0MmcrkkLccgBGGDVdCBl/Oy3yzdldOBQh7BNvioqybD9jmhJPPHUzPp/aEPrr87moqk7k+Zd6bsZeTyQaenl/4F8Bu4AMvCqE6LkxkhcodXVxYc21tXXx/Oin30QIifyhJ7j/S2+TllYDwPSpuzh4aCA+rWmIrUvVmDJpT1Tmlpxch6IYNkcCQXJSne1njhzLto37F0Jm5548/IaMQ7E/4sRoSjS8AzuEEBcJIcYJIcYIIX4ZjYnFiC75eSfC7uogoetO/H4H+w8O5H9/+6UGQ930KbvIzSlFVbWGq12qRsHIw4wedch2NCGss3qkcf2zL95mZeA1Q1V15l260fYzVlxAOCeUhDBj7oJIiVnoLhD69qlm1sXbWb1uXEPY7blFdG7BmKZCXV0c23cMY9JF+3A4TB763vOs2zCWNevHoigmsy7expSJhcg2MmX3nqE899I1VFYlIQSMGHaMr9z9ToNmYUdWv7N88Y73eP7f1wTScSUMU+baqz5j5PBjtp8ZMvgkzV2Kwe80ZPBJnE4jor9LDJCE6HqX/uTxbrFhSW6X3/dCRwhYueoiPlw6g5raeDTNaatSS5LJTdet4LqrV7Vp/KPHs3j0t/eg6eeODrJskJZaw6//+4lW1fPaOjc7dg7DbyiMHX2w1bp86zcW8Ozz16P7HQgh41D8OJwGP/7BP8nN6ZmlvLqKqfNPsGm7NyJ1KKYJXEBIEsyZvZU5s7cC8Ls/3cnuwqE031Fdqk6/zLNtHn/x+7NDjI9BzWLb9uFMntiyvTgxwcvF03dGfL9pU/bQr99ZliydzukzfcgfeoIr563vUPLPhUhMCHQSFRVJfLR8Kvv3DyIjvZIr561j6JC2l6zqTK67+jMOHMptdDywtACXS+Oi8fvaPN6Jon62Vn6vT6W4JIPOcBoNHljKV7/8dtTHvZCICYFOoKS0L//9m3vRdQd+v4Mjx7LZumM4d932AbMv3t7d02tgxLDjfOmud3jxP1cHquPKZGed4ev3v9GuM3VWv3LOlKURolm4NDLSK6I0a3tqauPYun0EhiEzbszBmDbQBmJCoBN48ZUFeDwqQeeLEDKapvLiK1cxddKeHlVscvrUPUyZVEjpqXTcbl+HFs+1C1Y31OwLIkkmqtPP5ImF0ZiuLZ+tGc8LL1+NLJkIIfHya/O56oo13HT9p512z/OJWD2BKCMEgWo1oX9aRTY5cCi8QbS2No4lS6fx3IvXsHzlRLzeUKNdZ6AoggHZZzq8ew4fdpx77lxMfLwHt8uH06mT3b+Mh3/wXKdZ60tP9eGFl69G1534NBeabgUPfbh0BnvaUSfwQiSmCXQCsiwwbJ55ATgU+8Vw5Fh/fvuHL+A3ZHQ9UDpr8aU88uAzZGZUdu6E24BpSpSVpxAX5yMp0RPy/oxpu5kyuZDikgzcLo3MjM49BqxaOx7Tpsy3pjlZunwKBSOPdOr9zwdiQiDKSBJMHL+XzdtGNjSgCCLLgnybhBsh4MmnbsbjPRe2a5XBdvDM8zfw8Pf/1enzjoS1G0bz8qsL8GlOTFNmWN5xvvrlt0hJaRrV51BMBuac6pI5VdckhEk+kqitje+SOfR2YseBTuDO2z4kNaUGl8uqLedw6Kiqxtfve93WV15ckk5NTULI60LIHDqcE7AvdC+7C4fw3IvXUVObgKap+P0O9h0YxKO/vwezG6NzxxYcavg7N8bp1Bk3dn+HxhYCdu7O4/G/3cJvHvsCS5dPxufrmiNaVxLTBDqBlOQ6Hv3Fk2zYXMCBg7mkp1cya8b2sMEvwbRee0SYrjZdy9vvzmli8AMrBqCqKpE9e4cypuBwt8zrogn76PfBWUpK09H91gJVFIPEBA+XXbK5Q2O/8voVrPhsUsP3Pnx0AMuWT+WnDz9NXJzWyqd7DzEh0EaCVXbWbhiLMCWmTt7N2NEHQ0JoVdXPrBk7mDVjR6tj5gw4jcPhB5uU3f79yklMCC2rpWkOduzKp64ujhHDj5HVr+3BPW2h9FRf29cNQ6b0VN9uEwIOxeThHzzH+0tmsnrdeAxDZvLEQq6/eiXx8e2v8ltcks7ylZObRFRqmkrZ2RSWLJvOjdeujMb0ewQxIdAGhIBnnr+eTZsL8GlOQGLztpEUjDzCN7/6qm0sfSTIsuDLX1zM355ZiK5bIbCKYuBwGNxj07Ri/8Fc/vjE7QjANGWEkJgycQ/33r2o3XNojX6ZZ6mtDT2yKIrZrujCaOJ26yy8YQULb1gRtTG37RiOaZOE5Pc7Wbdh7HklBLpfz+xFFO4bHBAAKsGAGJ/PxZ69Q9mybWSHxr5o/H4eefBZZkzbwZDBJ5kzawu/fOTv5A052eQ6n8/JH564A4/Xjdfrbuijt2nrKJavnNShObTEDdesRHU2VYFl2SApqY7Ro7pHC+hMFMUMe0Szy3jszcSEQBtYt2FsQANois+nsnrd+A6Pn5tzmvvueYef/ugZvnD7B7buta07httm0GqaysefTOvwHMIxdvQhvnjH+yTE1+NyaTgcfvKHFvHwD55DDlMKvDczccJe2xxF1akze+bWLp9PZxI7DgSorEpkydJp7N6TR0pKLVfOW8/Y0U3z5U0hYZ++iq3qCLB+UwGLFs+lvCKZzPQKbrp+ORMntN9qXVcXF9ZQWF8f1+5xI2HmjB1Mn7qT02f6EB/nDXENnk9kpFdy0/XLeXvxpeh+BSFkXC4fOdmnuXyufY2D3soFLQRME2QZzpSl8otffQVfwPV14iTsPziQOTM3k5jowaepjB9zkKmTdrNpS0FD0c0gLlXj4mmh2W8fL5vC64vmNViXi4r78fdnF/KF29+PyGBox7D8E0g2qoAkmYwYfrRdY7YFRREXTHXfq65cx+iCI6xaM576ejcXjd/HhHH7UZTzS/O54ISAriu8tXguy1dOxutVGZB9hsSEOuo97iYZcJqm8vHy6SiyiWHKLFs+lVEjDzN61EF278lvKLflcvnIG3KSyRP3hNznzcWXhbjVNE3lP29cwcXTdrZLjR6Yc4oxBYfYVZjXMLYkmbhUnYXXr2jzeDFaZmDOKe74/EfdPY1O5YITAn/9x+fYVZjX4Po5WZyJdci2U+cljEDUn09TKdw7lNs//yEzp+9k1drxCFNi+rSdTL6oMGR3OHW6T9jqV5rPSUVlUrtj9b9+/+t8tGway1ZMxeN1MWLYMW6+8ZNeu0NXVCSxd/8gXC6NsaMPtZhnUH42mfp6N1n9ymPVg6JENHoR5gLPA/2wHvunhBB/6ui4nUFJad8mAuAckdWj82kqn66axE9/9AwTJ7Scb5+Y6Alb088UMnEd6FSrKIKrrlzHVVeua/cYPQEh4LW35rH0k6koigkBa/y3HniVgpFHm1xbUZHEX/5xM8ePZ6EEeiTcfOMy5s3d1NXTPu+IhnfAD3xfCFEATAe+IUlSQRTGjTpHj/VHCeveiUw112y8A3akptSSn1eE0ixhyOHwM27MgR7frror2LRlFMtWTEH3O/H6XHgDbs8/P3kbdY3Kn5sm/PqxL3LkaLZ1rdeF1+vi1TcvZ8u2Ed34Dc4PolFtuEQIsSXw3zVAITCgo+N2Bmmp4YtdKorZEIOuKH7shILTqTNl0u6I7/fAvW+Q1a8Ml8tn/aNq5Aw4xZe/sLj1D18ALFk6PcRmAtZffsPmc/vI3v1DqK5ODEnI0jSVRe9e0tnTPO+Jqk1AkqTBwEVASC/CntCGbPiwYyQkePBpziZGQFXVWHj9J2SkV7LvwCCSk2o5cHgge/cOaTAAOh06Kcm1XHHphojvl5xcz3//5O8cPJzDqdN9yO5fxpBBxbHmmQGqbZKmAHTN0SSh6tTptLAu2LLy1E6Z24VE1FajJEmJwBvAd4UQIRYvIcRTwFNgVRuO1n3bgizDg//1PI89fieVVYnIkkD3O5g1YxtXXLYBWabBh2+aa1m3cSzLV07C51OZMmkP8+ZubLMaL0kwLK+IYXlFnfGVejWjRhyl/GxKyA6vunTy886lXA/oX4YUxpOS1a93GkN7EtHqSuzEEgAvCSHejMaYnUVmRiWP/uIvHD3en8rKJFyqRmpqbcjuLMtw8bSdtv7/GNHh2qtWsXFzAV6f1KCZOR06uQNOM2rE0YbrhuUfJzOjguKSdAzj3COrqho3xdyiHSYa3gEJeAYoFEI81vEptYxhSCxdPpVlK6bg8booGHmE2RdvZcXKyezeOwSXqjNn9hauXbAqrAtJkqCqKpF/vnAdmu5ACInk5Dq+ft/rDBlU0tlf4bxjfvaEVq9ZUrwt5LWM9Ep+8qNneO2teezZOwTV6Wf2zK3ccM3KJkJZkuDB777AM89fz649eUiSIC7Oyx23LOm27MXziQ43H5EkaRbwGbATCJrefyyEeD/cZzrSfOTxv93Crj1NA2WEkLDMSYHdxKmTN6SIB//rBdvzd9HJDP77N/eGGKXi3F7+738eJ9GmbFaMc0Sy6FvCTiBEisej4vW6SEmp6bSMyfOBLm0+IoRYRaSO9g5y/ES/JgLAur9MYwEAVqfdI0cHcODgQIYPOx4yzocfz7DtqGsYMqvXjWP+5SF2zRicW/yZ6OThw4mgCJWjqJhteATmZ09otyCIi9OiWtDj0JEBrN84GtOUmDqpkGH5xy84w22vihjcd2BQGCtx6Gu6X+HAoVxbIVBSmh5ijALQdJXikvRoTLVdnK1IoqQkg4yMik4v0NkWGu/846lnBF5kLLHbD50RePmYZPxtFATQMa2gNXbvGcp/3ryc4pIMEhPqufLydSy4fC2ybAUqvfSfBXy2ZkKghgOsWnMREycUct+XFl1QgqBXCYGEeA+KYuL3t36tw2GQmFhv+97gQSUcPd4/RBC4VI1BA0ujMdU2oesKTz17E9t3DsPhNPD7FfKHFvHNB17t1qCi5mp/EgYj8DZ5aJyB14fjZQ9tz2LsiFbQEjt35/HE3z6PFogOrapOYtG7czhzJo2773yf/QcH8tmaCU20Sp+msmX7KLbv3MOEcQdsxy0rT8EwZDIzKs4bQdGrTlUXTdhHuE60zZGAKc2SeoLMv3xdiNFQkkxUVWdGN3gD/v3afLbvGobud+LxuNF1JwcO5fL3ZxZ2+VyC2J37c9Bs//oOYAjtF1YdtTHY8crrVzYIgCCaprJq7QQqqxJZs24cmha6B/p8Kp+tCZ3PiaJM/t8vvsaPf/51fvo/X+UHP/4Oe/YOjvq8u4NeJQTi3Brf/toruFy+QHMLDadTJ6tfGQ5Ft153e3G7fXz766+ErTGXmVHBD7/zAgP6n8ah+FEUg/yhRTzy0LPEuUPPm7W1cXy66iKWLp9C6ak+Uf1Ouq6weu34kHwGv99B4d7BVFQmRvV+rTE/e0IUF6WgPxrTqWU6tWShE2l4dkcwTcIe65wOP8eO90fXFcI9/rreVDjU1rl59Pf3UFySjq470TSVsxUp/OnJ2ygpta+92JvoVccBgIKRR/nT/z3G1h3Dqa93M3L4MbL7l1F+NpnCvUNwx/kYN/ogqtrymSFv6En+52d/o6Y2DkU2wwqMdRsKePaFG5AlgWFKvPrm5cyasY0v3P5BVNTB+np32PccDoPKqqRWW3RHg0gW/glUxhLqOfEDh2leJFUwk1qy0XFiLf1cNIpQWUsCzTW6aNoIZBni3L4mfRyCmEIiJbmWKZMK2bJ9lG1tiOlTdzV5bfXa8YFksKZz9vsVPlo2jbvvDOsI6xX0OiEA4HLpTJ/SNIa/b59qZrWj2addF50gZyuSePaFG0J26TXrxjNqxFGmTGpbfz3TlDhTlorbrZGSbFXlSUqqR3XqNpmN4DcUsjI7PyIu0p2/FoU9uCloZBjUgWoU9tN0wWWjNwgAsJaPE+tI0R+VEjq3l8Jlczbx8SfTmhwJJMkkLbWGQQNLGJhbyrC84xw4OLAhNFxVNXJzSpk6uemzVXQy0zbHwTQVik7269Tv0RX0SiHQVazfOAa7MAqfprJ0+dQ2CYGNm0fxwstXN3TvGTL4JA/c+yZ90mq48boVvPbW5U0eNFXVuPSSTZ1e376tqv8u4ilBZSheXAhOoHLCxkU4BA27fEsnMBgtrBCIlqHwxutWcOp0H7bvGoaiGAghkZJcx/e/9RKSBJIk+O43Xmb9pjF8tnoCppBJSqzj8JFsvvPD7zNy+NGGGg25OadQVS1EEMiyQW5O1xuSo01MCDTC53Ny6nQfkpPrSE2ppa7ejd9v/yeqrYvcEr7vwECefu4GNP3cQ3TocA6/+u2X+M1/P87ll27C4TB4a/FcamoSiIvzcdUVa7h6/uoOf6dwdOTcX46Dclq2VdiVQAvSFYYoh8PkG199ndNn0jh2PCuQ2n2iyRFOUURDaPifnryVnbvyG36jrdtHsKdwKD99+GlmzdjOonfnoGE2mb3DYXDlvN4fUxITAlg+43fem837S2YhKwZ+v4MRw45x6SUbcbk0fM2agjgcfiaMi7xY6KJ3L2kiAMBSJevq49i+cxgTJ+xn7uytzJm1Fb9fweEwOtX91BnWeIAU/IzGSxp+dCT8hD5gOnC0laNANN2GmRHEXBw7nsWewiFNfiMhZHyak5f+s4DkpDr69q1EqUrC43EjSYLUlBruvfudTm/60hXEhACwfOUk3v9opnV+DJwh9+4fhMejMmRQMYeODEAPPCCKYhAf52X+vMir+pSU2luqdd3BqdPnrMuSRKeXzOosAZCJzlxqGmwFwfjxxoJAB07h5KTtQaH72H9wYKCSdFOEkNldOBRJEggh43D4cTj8fOuBVxk14mgsTuB8YvH7s0POe4bhoKi4H5//3MfceO2nZGacJTWlmjmztvCLR54iOdk+EMmO/llltq87HX76dYHhL0hzAeBA0A+dvtgXUYkcwTTqcHDugQoKAwkowsEJnKwjkc9IJJIo884SVnYkJtRb5c1sOZfh6Pc78HpV3v1g9nkjACCmCQBQVW1/vlVkk4qKFK6ev5ar569t9/jXX/MZhw7nNFE3ZdkgPsHD+LH2kWnRpvmiGomHcXgwsZakjsSnJFHRjkfCjSAe+0UkY7kWjxDeFdrSnDszrDjIReP38fy/r7F5x64Arcze/Vb4+vnSdCWmCQB9+1TZvu43FLL7n+nw+COHH+PeuxeRlFiHqlrde/KGnOTHP3iuS2rYNxcA2WiMw4MDULEs9vEI5lGD0g6NwKDlvX0MXuLCCImegNut891vvhwINPNav5HiR5btj2ayJOiKoKeuIqYJAAuvX85zL13b5EjgdOoMz49et9+pkwuZPHFvSJxAZ2MJAMEwfBTgxY2JwP6HlxDkonE0JPCnZXRkPEgkhAnfTsTkOir5lCROtWAPUDEZhIYbkzIclODsMm1gxLDj/On/HrM6PdfHkZ93nEd/+yXq6pv+pWTZYPy4/edVGnNMCAAzpu3C63PyxqJ5aJqzocvvF++IbiSYLAv6ZXYsO7B4pYvCp5OoL3XQ/xIPo++vIS4zdJdtvPtPwMPwZok/digQVq1vjT24mYwnTPcG60GbSS1vkYqwuaofOpdQgxSYhx8rCGkpye2aT3tQVT+TJ+5t+P+v3fcGj//1VgxTwu934nL5iI/zcddtH3TZnLqCmBAIcOklW5kzaxtVVYnEx3txufTunlIIOx9PYscfU/B7rG2ocr+TA/9O5LolpSTm2quuTsyIBABYav3Zdj4Sh3AzEh8JmGHPmDKCvvgpa6YNyAguoabJq04gBYNxRG6AjTajRx3h0V8+wcrVF1FWlsqw/BNMm7ILVysh6b2NmBBohCwL0tLClyXvTjxlMtsfS8XwndtFTU1C88ts/t9U5vztnJehsRaQhhHR3m5ghQWXtvORMJH4iGQmUsdg9LA2ArvX+6PbnrAdwFC0LjsS2JGWWssN13zWLffuKmJCIAJ8moP9BwYBMHL40W5pf1XymRvJIcDXbBmZEkXLrOhFO7eaFznszhz0DJjAMVQ2E0/jZRqHyTjqGYCOgcQhVPYQF7aKkA+ZtSQRTxUZGDb3lSi3eeQctgcEi/YYKmO0jWhVG34WuBY4LYQYE40xewrrNhTw3IvXIwc6F5lC4iv3vM3ki1puQxZtFLcI65uWnSKsX70ahWoUUpstymDm32biA8us6eBuTK6iChUR+JygAC/98fMxSSHXN2YjiVxJNQoCBUvImMB64m0FyCmctoLKBEoCh4Tu1AbOd6Jl43wOWBClsUIoPdWH/7w5j6eevZFVa8cFcsFbprIqkdffvpRHf3c3T//reo4XtT3bq+hkBv984Xp8morH68bjdePzufjHswujXlegNbLneBE2er2smuR9rmVPw6ckUYOMDmhYAuA0DrYSH9iDQxfmSLw4GwSAhQNIxU9/WraXVKPwLinsxc0pHBxG5SOSORHG6+BFZg/uJqOagB+JbcS3eK8YHScqmoAQYmWg+1DUWbN+DM+9eB2GIWOaClu2jeDdD2bzk4eeISHBa/uZktK+/M9vvoymO/D7rSo9GzcX8OUvvMO0KfbVhuz4ZMUUdJsEIsOUWfHZRG67eWm7v1dbccYLLnmynE8f6IswwdRkHAkmibl+fvHMoBY/60HmPVJIx08CJhU4qKZlQZqNbnuFA8jE32oqsBeZ7W1YwDuJ5ywORgZiCkpxsIc46luZZ4yO02XeTkmS7pckaZMkSZvOlEd2pq73uPjXi9eh686GeoA+zUVZeQpvLZ4b9nPP//tq6j0u/H5LlRRCRtNUnnvxuoi0iCBnylObtCsLYhgKZWVd3/4q90oPN60uYfz3qxj55RoK7q9m1954LqOaMdSjtmgClCjDyTFcrQoAAF8Ydd9o4T2wzvcZ6KS0IxT5JCqWLeRWAAAgAElEQVTLSOZdUtlEYogA6MpQ4guJLhMCQoinhBCThRCTM/pGthB37spvOIs3xjAcrN9ob3owDIl9BwZh99UkSXDwcOT9DiwjYKjqq6oaI4Yfi3icaJLQ32Dct2pwpRls+UMqA/CTjkEBXq6hKmqRefuaqeeNORZGrS+gnoVUMIca5lPNNVSRSNcbUWO0jR4d92Sa4bPSwzWoDBaMsENAC63JQ5kzewtul9YkfFSSTOLcPmbNaHsVo+bUFimsfSiNN6Zls3h+Pw69lmBbxKQ5dSUKW36fioNzp3kH4EIwNkp+9SKcHMSNHyv7T8eyJawhAU+zx0ZCMIdqxgfiEdTAfJIwuZxqm9oCgj74SUdHjln/u50e7SIcXXAY0wyVU7JsMHHCXptPWL7+cWMOsn3nsBBVXpFN8oaesP2cHYkJXn76o6d56dUF7NyVD5JgwtgD3HHrhx2u+FN7QmHxlVnodTLCL8EJB+sedlK63sXM37UcqvyDScOZRF2IUi8DuehE3je5JSS2Es9+XGQFXIQncaIjk4KfdPx4kSnGyVTqyLbpOiBjHQ+y0TkZsCGkozObWhyIBq/EBuI53sZQ5RjRI1ouwpeBuUC6JElFwM+EEM90dNzkpHpuvnEZbyy6LNAgQkZ16sTFe/ncjcvDfu4Lt7/PkWNfwetx4dNUHA4/smzytfveaHPCTnp6Fd/5+n8aduhopZBu/b8UtFoZjHMD+utljrwRz5ivVZOSZx+VNj97AoNbKO8dbeW7DoVDAXETjOzLCgT3NG7+Fu7PIgMJgSOKC5NLm0UGgmA6ddSgRJTBGHMVRp9oeQduj8Y4dlw5bwNDhxSzbPlkKquSGDv6IHNmbQ3rGQDok1bDr3/xBGs3jOXgoVwyMiq4ZOZW+nQgGjDa+ePFK+KaCIBzN4KSVW5S8sJXGD6Jk6k2rweDfkIRuBD4kTA60DFuNB6y0Js8NJGI1IqAEBkaxqSoAFOoYynJbWpnFiM69OjjQJD8oUXkDy1q02fcbp1LL9nCpZds6aRZdQxHvAnloQZSSQFnor3dImgd15HZShyTA+W/g8tGwkrEaZwHn4OPydTjCizXIlQ2EI8esTlIkBFQ/0fa5CC0tGQFVszAmcCnEjHCZC9CHwyuooqPSG7D3M4fTFPC71daLZXfGfQKIdASPp+Tjz6Zypp14xFCYsbUHVx5+XrbJiI9ieFfqGX7H1IwPM0eeNNyBTanuXssGRMTmtgFJCxjXDY6xahkoXNxoOJPkBw0EjFYQjKtVfiREVxKDX3wo7R6dVOCGoIfKMCDghVWrINtMrGMlXJcgLfV+ILz6Ujg9Tr596vzWbthHKYhk5l5ljtv/bBLW673aiGg6wq/+u09lJxKb6jb/96SWWzcPJqfPvx0iFT1aQ4+WjadVWsmYJoSUybt4Zr5q1s8WnQWBffXULrGzekNLgyfhKxay2bOU2WoSa0r2VlhgnmcQDp+ilEZR33ID6wAyRj0xbCN42/MFOrIwN8GncGicQxiBgbpgUOIn3O5CnZjKljtzNoSZNSbEQJ+/+e7OHo8q6GqdempdB7/6618/zsvMjw/ciN2R+jVetfGLQWcOt23SeMOXXdypjyVdc3iCAxD4te/v5vF78/m9Jk+lJWn8fEn0/jFo/fh8XZuIww7ZAeM/U41Y75ZRcH9NUz5eSW3bComZ16oQLILkmnupgvih0BrEGuxhyOVltXOfugMRWvzA9JcW5A4ZzgM/koeOl6X53wIHDp8dAAnivo1BLUF0XQnby66tMvm0auFwNbtIxq6xzRG01Q2bx0Zcm1JaXoTgeH3O6iqSmDlqq59oKqPOnjz4v4suyuD3U+msPfZRE6tV3EmtVwcpDF7Az58O4LGwfoWft7aVqIGJ1HXZvU/Es+EA9CQbUOaDMIZNs9Pjp/IChsXcqILOxv16uNAfJwHSTJtQntNhJB4/G+3UFySwYDs0xh+JaR/AICmq2zdPpL5l0fHu94aQsDHt2VQW+SARgFPxz+IZ2een/H/VR3ROMWo7MHNaLyYnDMFfkYSvsDi30Uc05vZBASW2t2SxURGkNLGyEO7kpzhSMDkMCpDAppGsJ2ZF5nd7Whv3lvpk1aFrAjsQjNTU7qurkWvFgKXzNrGug1jQxp7OBwmhXsHY5gKQsicOt0HSRJhBIYgISF8P8Joc2aTirdcaSIAAAyPTOEzSU2EQGsq7y7iOYibYXgZhEY8JtOooxAX+3FzHBc5aAxqVOQjeF6/jFreJtXWZRgsoxmuCXzz1w3gMCoGEvn4WnyoBI1bkTnQkXEhKMbBYdzN5iNIxMRACnv86Yn4DZnCvUOoq3czLO8EffvYC/YxBYeIc/vw+ZxNnktV1bh2waqumm7vFgJ5Q05yzYJVvPvh7IBaJSFJAlky8Wnndn0h5MD7obqXS9W57JLNXTVlPGeUsDEHWtW5ByHSM28KBqMaue4cmEzAQxImm0kgJYznXUYwAK1dkXrBY4iMJQBqUNhKAn6gCoUxeIhrVCik8f2D/+0EsvDzKUmctvEXDEBjKnU4A+NUoLCaRJs4yZ7F4SPZPPbEHRiGAsKqWH3JzK3cdVtoF2tFETz0vX/xx7/cTkVFMrJi4vcrXH3l6pDOyJ1JrxYCANdfs4oZ03axZdtIhIDcnFL+/NfbbK+VZQNZtkpGmwGhkZlRztP/ugFV1ZkzazNXzluPw9F55bH7jtMwdXspkDaq7XUNJzZT98H6UfPxsZs43GHU+qCXoOlrAieixWoBOrCKROIQxGFSjoNTjbIYilCZ0Cx+IZxWoQCD8YUIgb74mUltk+/VB4MrqWYRqQ1irae5CjXNwe/+fBceT9MeC6vWjmfQwGIumRmab9Ivs4Jf/fxJik5mUlsXz6CBJcTHhY8I7Qx6vRAAyEivZP7lVluwispE23wDsPrWP/LgMxw4NJC6OjcffzKN4tJMS2oDi96dw+7CPH7wnRebSG0h4GRxBl6fi4E5pR0K6EjMMRh0fR3H3o1vEiOguE0m/riC2hMKt06LtDiTIDXMIjeAPvgpw8EAm/AbCSjAixeZo7iYTB2D0BBYxTy8YOuok7EakoYL6BlDPY5mxUhaCim2G2WsTRWBYB5CLlrYLMbuZuuO4QibxDZNU1mydIatEAArGjU353RnTy8s54UQaExaai052ac5diKryTlLkkxyc0oZNPAUgwae4pXXL8frUxsEAFhGwkNHBrBv/yBGjjiGELBpyyj+/ep86j1uFNnEFBK3fu4jLr1ka7vnOPOxs6QM1Sl8JhlfpUzaSJ30i7x8en8GwoTPUcFxVDaQ0EqYr4QOtvZ0BRiPhxokW798sAz4ROoZgo80jIaF50BgYAmSxotRB/YQZysAHAjmUNOmuAI/lgfDEQhpthD0s0lGIjDfltye3U1FRTKabr+kamp7buxD77G2tIEHvvIGSYn1uFyWWuVy+UhKqueBe99suGbHzmEYRugP5vOpFO4bTPnZZP7fLx7gyX/cTGVVElqgxJjP5+KV1+eze8/Qds9PVmDcd2q4dcdJvnj8BMPvquHQ64notTL+ehkFyEVjFuHzB4IctHEVBpN60jDICbwbTneRgL424bwK4EXiFA58SFQis4GEsNb7ydSR3sbAIhkYjpeFVDAokBTVtxWx17wgSk+JFyg91YfFH8y21UIlyWRYXtcE/rSH804TAOuc9dv//TObtoyi5FRf+meVM2XiniZVguPDRAk6nX7i4738/s93UnqqL3bKrKapvPvhTEZHKbRz+2Oh4cMOrICdBIwWjWE7iCMZP/0Du2fzioFBlTtccE7jDsLNiUOwKILmHzKCQWgRm+yCNgKJc1rMNOqowEFSoER6OGFyoofGETz5j5vxeFyEPi8CVfWz8PoV3TCryDgvhQBY3WQunr4z7PuXX7qBoqJ+tsFGWVllnK1Iti0tFuRMeVpU5mn6LY+B7XtIJLciBATWQgr6/8MRbnf1t/BepG45Zwvxf5HGD8hAPl6OoIa9az30yCzDsvIUSk/1DfO8CL77jX8zILvjPS07i/PyOBAJ0ybvZvq0HTidOg6Hjkv14XTq3HfP2xh+R6DppD2SZDI4tyQq85Ad4O4TrqOvoKaV/XUgGn3wt0ua+4GyQEny5idtHdgdYSdhH1KLdQcbjx3sddCcYAJRvzAHFwG4gfHd2JEoHD6f07YMHoDTaZCZ0bHWc53NeasJtIYkwT13vs/8eevZXTgUVdWZOGEviQleyspT8BvhF5/T6ee6q6PXlWbst6rY8pvUJkcCP3AGR6vhvQPR2vUjeoGdxHEQNyqCWdTSF3+DKl6Im4MRW+ElthAfEp2oA/txI7C6DHmQcWKSaXPut8KOBSMbMh+a38HSdEbg5SgqVT3o0e2fVY7T4beNSE1KrCMtteXoP01z4PWpJCXWR71uRST0nL9kI/yGjCKbUf+D1NTGsWbdOMrOpjB0cDGTLyqkf1Y5/bPKm1yX3reKieP3snX7SDS9sQ9b0CetmvvueZtBA0ujNq9R99XyzM+zKcDbYDArwclaElv9rBGo4BfuT2X3ng5sI4HDgUXuQ2IZycRh4sakBiWMfT48x3HhR2I8HpIxqEdmF26OYJ2Ti9EZgY+MMIY/CUjFCIiM8MhYgm9no0c3kngBU4fSNW70eol+U324+0YvFkSWBV+84z2e/teNaJoDkJEkE6fTzz13vRf2OfZ4VZ7/99Vs2lIAQEK8h9tuXsL0qZGXxY8GPUoI7C4cwr9fXUBJaToOh59ZM7Zx680fR6UB5P6DuTz2+B2YpoyuWx1m31x0KY889Kxtm/CvfGkR77xbwdIVU/F4XPTLLOem65czbXJhh+fSHEmyQoALiSMRAy9yQ/x/S6iYFONgUJhMALvlZGIJDrtEHQ9yh8Jzi1Epthk3Dy+TAulMQSOlnSaQ1Cy+IFqc3qiy7O4Mq5ajBIYmMfYbVUz4QWR5GpEwZdJe0tJe5N0PZlJSmk7OgNNcd/VnDG5hs/jjE7dz+Gh2QxpxVXUS/3zheuLidMaPPRC1ubWGJCIpb9vaIJK0APgTlsb2tBDi1y1dP3m8W2xY0rT09/6Dufz+T3c2yQNwOnSGDj3Jj773fIfmZ5oS333we9TUJjR5XVEMJo7fy9fvf6OVz9Op/egjd3MJEjBxIJhEPRn4GxZUS3X+BOc8AFWB8NvWbA3RwoFgYUTVAyPDD3xEMpXNRgynCWg1Eq9NHIC/rpn3Jd5k9uPlDLyq6/JGGnP8RD/+97dfQrMxTOfmlPLLR55q8lrpqT54fSo52acjimidOv8Em7Z7I1LnOvzbSJKkAH8BrgCKgI2SJL0jhGiTTvPG25eFJALpfidHjmZz9HhWixK1NQ4dzrHvJGQobNk+stVF3pkCIFIax9IHl2/jX7hxJqFdTr8XiY9I6fJEnH7oYV2QwfTjcFWLGhcpsSIZrf6JzQVASxxbHG+rEvnrZXb9LanbhEBRcWbY0viWa9qipLQvT/z9FsrKUpEV64h8xy1LmHVxx0veB4nGEzEVOCiEOCyE0IBXgBvaOsiJFnoFHjvev/2zA3S/w6b2vYVpSgjR89xOjUlHZya1xCEaovTtyntbLjR73IhuycRrac8ysQyQ4XIVBLCBeE7i5Dgqn5EU6JwcOZ4zCv4wG6KntPtOwxl9K8O+FzQk6rrCo7+/h5LSdDRdxet14/G4eeGVq9izd3DU5hKNp2IA0DgcqijwWhNaa0OWkmIfHSdLljGuI+QNPYFhm08gGJZ3os1lyLua0XgjUt4TCP+D1nSTN/iUbUVBSwM4jspu4sIeY86icAg3n5LEahIpxUnbKh1C+gQNR1zo7yspgsxpXV9WLkh+3gn6pFU3aWwDVhrxdVetBGDztpFomiMk/kDTVN79YFbU5tJj2pBdPX81qtrUwCVJJnFxPkaP6lhknkv1c+etHwbGt/YmRTFwuzTuuu2DDo0dCVq1xK6/JbHk85l89q0+nN7Utqi3lAgLhdv3F7bU6B1dXLfPgaAADwuowofckIsAlnfCg8yWQKvyDYE05KDWYASu2UiCzchto/9sLyn5OrLaWCcRKG7BuO9EzzDYViQJHvyvFxgyqBinUyfO7UV16ly7YBUzZ+wA4MyZNDTNXoieLoteV+xo6EMngcZWvpzAa21i1oztnDrdh4+WTsfhNDBNmdSUGr77jZeR5Y7v1JfM3Eb/fuV8uHQ6Z8rSGJZ3nAVXrCMjPbxaFg08Z2TeXZCFr0LG8MogCY69H89FD1Uy+v7WcwMAqpFJaEesnMBy/20mnqIuDLdVEMynKmDEtLBqH0qU4uR0QL0PfqNjuKhBYRRekjAow0Eh7qjUDpBkmP/6aTb/KpXDryVgeCUyp3uZ8vPKsA1euorUlFoeeeifnClLpaYmnuz+Z3C7zx2OBmSfxqXqeJvFH0iSycAB0XNRd9g7IEmSA9gPzMNa/BuBO4QQu8N9xs47EKS2zs2xY9kkJNYzKLe0W4InosmaH6Zx8D+JlnuqEYpL8LmNJ7lx3LhWxyignvF4m9XcsfBDk56EjdGAtSRwsotTb4fh4SI8ITuMAM6gNCmBFi16Ul2BaGEYEg//7JuUn01u6MoNoDo1Hv7Bvxg8KHzUalu8Ax3+JYQQfuCbwBKgEHi1JQHQGokJXkYXHGbwwN4vAACOvx8fIgAAJIegeHkk9fQE+Wi2Fn8T+IQkqsL8jApwNsyZvDPJbdalKIiEVYJ8PtUtNCIVpKOTi0ZCD04b7goURfD/fvgsYwoOoyh+HA4/mRln+fbXXm1RALSVaLUhex94PxpjnXe0JGYjOObIQHzYwiESDmALCVxCTZMf0w8cRe0Wj4DewsFFwupJOBCNo800lAQMLqMGN2ZDQtQJVNaS0KhY2YVFSkod//XNl/F6neh+B4kJnqhvjj3AA35+M/TGumZGKQthQM681n3UVoSfPXLA7VeKk09JoibgCA2KFg9yWNdoZ3IQV4slypxARsgVgsuoIQETJ1ZmpILVMWk03ePL70m43TpJidEXABATAp3O+O9XkTDAsHoPYrmmFLfJtF9V4EoNXaAKgniMRuqyFLZwiIZEbeAndGI2FPcMVg0ajZebqeBiakhppdlINCnByWFcDQFMzfFDiNGvLwZuzJAH0oFVBi2pFx4NSkr7snL1BLZsG46u99wCqT0qd+B8xJUquGFZCUcWJVD8qRt3psHwO2pJHd50UcoIJlHHkEAegMAq5bUbN9sDhUOyAwv5XEEOwVxq+IQkW0NcMFZ/EDq56KwkkZJGXgInJjnoKAhKcEaxkq/EZhIoQ2EG9baKvIlgKD5O4sSHTFzgCGCHAlxFFZ+SFDbuoCdhmhJPPXsjW7aPRJYEkmyiyILvffslhg4u7u7phRATAl2A4ob8W+vIvzU0USnINOrIbZYWPBoPBrCXOGpREM3KdzmwKvNmopPYQmxeMA13BrW8SRogkYuPGdQ1SeY5gJutxBFpQI4UqAcYj8lZlJBw3iJc1OIhqdnyloBxgYzJKcBm4inGGVYEBTWbGYFeCSARh8lwvPRDpwaFsu0q6eN7RhPaj5ZNZev2EU26XQE89uc7+cNvHmtS4aonEDsO9ADcAUOZXenwMXiREPQPU9/XAWTib9EYF0TFKkWeic6MQO6/MzCGVabcS06Lp/lzJGFwA5XMooZJ1HEl1cxtZvUfg4f4RkeU4D9K4H7OwH9PpB4ngmOoLR5aVARJmCRjcA1VjMRLOgZDZB8fLszk0Gs9o5jn0uXTQvJgAAxTZufuvG6YUcvEhEAbqa93sXL1BD74aAZHjnUspyFIIkbYE6+CwIGwcRJaGFgtv/fhavXULwEXUc9lYXIInViFP1vHOoa4EaicEySZ+JtU/slrpRtREDlw7XoS2NtKRQETq6ipo1EiFaaE4ZFZ93Af/J7u9yLU19tXZBKmRG1dzxBUjYkdB9rAzt15PPH3W5Akgd+voCgmBSOO8M0HXm13/sGS4m3cmD0urCpsYvUB2IebFOpsT8THUdGQSMBkcEBc2OXrN+4MHA5XBN6Elox4+fjYGgj3dUTomZCxEpwEEjuIJxudVIwm4wus8uT1SGSGqWosKVbtgOxLurZ5R3Py804EdvymszSF1COrDsc0gQjxeFSe+PstaJqKz+fCMBxomsqefUP4+JNpHRsbmWKcITu5HyvLTgSKgBzGhRF4XedcjcBrqeJmKhiAhifweuPlF2mxTwMojsDwloo/rNByNNzRSh6KpH6PAaSjcQ2VjKKedSSgIzX8PazvK7GaxJbjBQTIPaAY8S03LcPlapwNYUX5XTR+X0gVq55ATBOIkK07RtgWH9U0lU8+ncyCK9Z1aPw1JDKdOnLQMLCk837c7Gqo829Z3PfhJgsdCcE4vGQ22zFdnEvACRdOHKSxcDCxFtreMMVFHQiG42UwPpJbyGOobFQdYBtxZAZsDME5Nq4REPx/q8ioNYsJeBmFj/dJIhedPhhUonAYV6CHMRThJMeuq5JDkDm5e7UAsLoJ/fiH/+T1ty7jwKFcEuK9zLt0A/PnrW/TOB6Pyqo149lVmEeftGounbOZgTmnoj7fmBCIEI/HFSYdGby+jm8/RmCnUzGJw6QuTJ2/WhQOogR6ENqX41KwFvVG4jiCi89Rabu/m1hJPTJwEic7ibeN6VcQXEkViY0SguwQ0CTfvwoHH5LCeOrJDQiDxos/+O/mLctcCC6mnmVheh5sIoG+VKEicAKyaiIpMPfvZcg95IkemHOK733r5XZ/vqoqgV88eh919W40TUWWDdasG8ddt3/A7CgWFIHzQAiUlaVQW2dlYHWkR2BrjBpx1PZ1STIZ08FU5yXF2xpKjGnIDTteS/RHb9Gr7wRy8HOIOHYSx1g8TQSBH9hOHPvCdBRqzBB8rQoAAB+ENBetQaEGBbPZfIO5D+EqC2fib9ae7BxeZN4llT89dpDTG10kDfSTf2sd8Vk9y/XWEV57ax5V1QkNiUOmqaCZCi+8fDWTLtob1aalvVYInK1I4om/30LRyX4oioEwZW66fjnzL2+byhUp2f3LmDJpN5u2FDTUhZNlqybBjdet6PD4jQVBJHiRSWnhxG1CQy+AvbjRkBiLl3hM6pHZgZsjEfYVGBKBlV8A5WHsCX3D2BDCFR0NjufGDFty3UBi2G11DLstfOxFb2bztlFNMgeDKIrB7j1DmTIpegVve6UQME34zWN3U1aegmkqDUEZb75zKX3SqqP6B2rMvV98h+H5x1n6yTTqPG5GjzrMDVevJD29Kirjt0UQ7MNFegsGOhMa9Q2QOIybwxEu+uaES2BqjAGN7BdNqUYJb9EPM54JgfrE3YcQcOJkP6qqEhmYW2pblbrrib4LtFcKgb37B1PdSFUKomkqi967pNOEgCzDnFnbmDOr83LXIxUEqs3CDJ6zTWA3cZRFKcS2tTZjAKdQwpYw24eboW2oIGACe3C3WEals+sHlJ9N5g9P3E5ZWRqyYqLrDmZfvJW7bvugSwrPTppQyLqNY5t0zQYwDDlqPTCD9EoX4ZkzaZhhioOePZvS6fcXwooZePq563n6X9eze89QolC5vYFIHvACfCFaQPCcvYH4sN2D20J/NC5vpddPMCYhG4MbqWzoLtyYOmR2BgqKhq8iYM1dw7JVhNMqoPMFgBDw+z/dSUlpOj5NxeNx4/c7WL1uPB8undGp9w5yy8JlpCTXNpTckyQT1anzxTvei6o9AHqpJjBgwJmwe0RWVlmn3ts04W/PLGTHruH4fFYr0E1bCpgwbh9f/fJbUUv1DD7o4bSCcHF1za3t7SXYMCT4gLQWaxCM759GHWU4GpKRcvExPZCj0BoG8C7JeLv5sTx8dABnK5NtNc0lH8/g6ivXdvocUpLr+J+f/ZU168axe89Q0tJquPSSTeQMiH5j014pBPKGFJGVVcbJ4syG7i0AqlPv9BbQO3YNY8fO4Y26GUv4fCrbdoxg5+58xo05GNX7hTsenEUhK0yg8NlmP6uKySi8DAzEIBzCxYEW1G0ZwcRGAgCauvZaEwZD8LGLeFLwN+QoNMZujKAQuYxa3icl7F26ooxYRUVS2Ia0XRn2G+fWmDd3E/PmburU+/RKISBJ8OB3X+CfL17Lth0jkBAkJHi44/NLGNOO85JhSPgNBZ9XZc36sRw7kUV2Vjnz5q4nPr5pQs2a9eNs25n7fCpr14+NuhAAe61gB3Gk21QTKsPRpN+PE5OrqMaN2XB8GI+HAeh8QhIuBCPw0h8dLzL7Ap6EcDt3a4qOwjktZUSY5qLhkIAUTPpiUN6Nj+bA3NKwDWmz+pWxZdsITp3uQ/+scsaOPtDjS9a3Rof+0pIk3QL8HBgFTBVCdK7IakR8vI9v3P8GPp8Tr08lOamuzaq4x6vy0isLWL9pDIYhhzQheWvxXG6/eQlXzNvY8Jpphr9JS+9Fg8a74PzsCawgiUnUk4qBH2uH396stPgwfLgaCQA4l4I8CI1JgQw+632DTHQO42r3kUKHQH8Aq9V4OI9ASxrFWOrZSEJIfYOuKiaamVHJ+DEH2L5rWJN0YKdTp7omgX/880Z0v4LTYZCYWM+Pf/hP0lIjqxzdE+no8XEXsBBYGYW5tAuXSyclue0CQAj43R/vYv2mMfj9wQYPTZNehZB55Y0r2X9gYMPnpk/dhcsVaphxuXxMm7KrI1+lTSwp3sYLxbv5gBReJo3XSGMLCSEdCprXKAhipSl7UBtn42EFGeXjow45orj/xnugH8sdeDIgBM6ESSFqbd/Mws+1VDGyG8uKffXeN5k3d0PgtxZkZpTTt08l9fVuvIHcEa/PxdmKZJ569qZum2c06JAmIIQoBJB6YVngg4dzQmwKdpimzAcfz2D4sOMATBy/j+H5x9l/cFDAMGgJgJHDjzFh3P5On3dzmmsHzQlXZ8AEEsLs1CaWzSEYHxAcIdyv7EXCwOoTuKehyJmV3TjGJks3f/kAACAASURBVDU5qAmEsw0EXxuHh1KcVOLo8pLiDofJrZ9bxucXLsM0JaprEnjwkW+HGAtNU+Hg4Vxqa+NITOydtRC77OAlSdL9wP0AAwd0vynixImssG7GpkiUl59zO8qy4LvfeIXNW0eyZt04kAQzp+9g4oR9rfqPTxRl8umqiVTXJDBuzAGmTd4d1SozzRfK/OwJ7MdNX2pDIgasZqCS7V7twCobbtf41I63SbU1MpqBTMBw0Qp+zpVAs7uHjGVk/HVx12lYzZEkq/S31+tCUQzbTUOWTbxetUcIgU1bR/Cf168EHoz4M62uRkmSlgJZNm/9PyHEokhvJIR4CngKrOYjEc+wk+jTpwpFNiOooyMYlt80B1yWBVMmFbYpKGnpism89ubl6Lp19NixM5/3l8zkkYeejbrfN8iS4m0IAeseSmPPi0kNOzBYiT5JmIwI0+cwUjFdiRzWy1AbaD9mJwSCO/46EhhPPYk2wkjGqibUE8jMOIvDYeCz+ani3D769IlO1GhH2Lp9OP94diGa3rYgsVZtAkKIy4UQY2z+iVgA9ETGjj6I2+1Dklo6+Qpcqs5VV6zp0L2qqhN49fUr0DS1obmkT3NxpiyVxVFsLGmHJMGM/6tg4fISpj5SwcW/PMtiUjkUSFOuQmkQhAaR1x6wip1YGX3hEEhsJT7sMjaxsiL32lRTBsvI+JVn2tzRrkXKylJYsnQaH3w0g9JTkffzUxTBnbd+ENIvU3XqXRZF2BqvvXl5mwUA9FIXYTRQFMFD33uePz15GxUVyciyiU9z4HAYDQlCQ4cUcc9d73U4N2D7zmHIiqD5k+73O1m3fiy3LlzWofEjIW2ETtoIa7m//ZUdDa8vyB7PAHT6oePCZFAEupGJVVb8CCp9MEjE5ASqbcbfEVxMot62E6ICVKBQicJwfMQ3612YPclHzhXRU7HfXzKDt9+dixAghMRbi+dyxaXruWXhJxF9fsbU3aSm1PHOe7MpPdWX7P5l3HDNygZ7UXdTerp9TUo76iK8CXgcyADekyRpmxBifkfG7CjlZ5P58OMZ7DswiD5pVSy4fB0jRxyzvTar31l+9fMnKSrOpK42jkEDS4iL0wIPCVGT7sKUwoYVN3dLdjUfFp/LTd/zj0Q2/syqRhwOP1adggH4mU5dQ6jyZOpYaVMS3AGtFjI3kFhCMiPwMhiN3GFe8m+rY9SXaqJWH+Do8SwWvTu3icvPMGDpiqmMLjhMwcijEY0zasTRsGnl3U1igoea2rZ3cu7QYy6EeEsIkSOEcAkh+nW3ACguSecnv3yA5Ssn8f/bO/P4qMqrj3+fmbkz2UMWskCAsCUQdghBkH0RBBQsWEUWcSmttdW2+rZaP7WL1rfqq/Z1qX2tVqtilVYsCiibiKDsS1hC2MnGkkD2kNmf949JYiaZgYTMlsn9fj58PkzuzL3nztx77nnOc57fKShMIvtQGi+9toANm0e6/YwQ0K1rMf3S8wgNNTf8zZPh3eBBJ7C7ECTRaq1kZV5320aPI+3C7RVR3+zkAGEoQJe6foNaHGN+BepaoTl7u/CriKjagdC6GQgLGg4Txmo6MXfLBQY+UIX2+hY9umTrN0Ox2pqfnNms8NXW4Z47kB+ZPnV7s+FKSwiAkYzn+GDFdGqNemy2+seHwGzW86+VU6mt9Z/4XEynam67ZTN6vbkhB6HXm4npVMWtM7f6za6mJN9oRKt3EbJoJb1uq+a+vDyOE0LaVfQFUnC+CGvRXLWfQOOawnXnDnhtKvDKlRCX6/NBcOVK2xdbBQI33/QtY0dno+haJhtfT1DlBI4eS8WVX9NpbRw70YOhg0/43KZ6Zk7fTlrfAjZvGUFlVThDBh1n7Ohsp370/iZ2oIVu02spXB+KtdbxPQqdRB9pZ8QTFWgUx4360eCuGC81v6HquyI1xoyGAvSkNClasgJn0LP2nGelstwxdMhx9h9Mx2RyboKq15sZMcw7S88bU1YewarV4zlwMB1FsTJh3D6mT9nh0SlijQYWL/icObO2MPX2ln8uqJyAVmt36e0loCi+68Xnjj69CunTq9DfZlyV8a9d5vgH4Rx7OxJzlYaUKbUMeqiS8OTvLtbk8bWc+U84NCmTNoTa+cOaU8T0c3Zs1iuCrQ/FUbQpFI1eYjML+s6sYemLpT45J4ARw3L5Yv0Yis53bsgL6HQWYmMqGXPDwWt8um1UVIbz26d/SE2jaOSzNeM5fKQ3v/rFux5vMhoVdeXab2pEUDmBrMwj7Ng1sNFwwIFGSNLTXCcHVZwRGkhfVEP6IvcqOkMfraRwQxiWGhocgS7UTspNtc0cAIAuTDLpzUtcuaihOl9HZKqV0M4tKUr2HDqtnccffYcVK6ewbftQrFYtnePLuHvhGgxe1KYE+GLjDdQaDU4PKLNF4Wx+MkeP9SSj3xmvHv9aBFVO4M55G4iPq2io7VcUC3q9mdvmbObFlxfy45//kseefJAt24Z6VASkoxGVauWW9efpdVsNoQk2ovpYGPGbcsa9enVN/bBEOwkjzT53APXk5PZi6zfDMJsVbDYdFy7G8+IrCzl4uI9Xj3vwUF+XlYYmk56c3FSvHrslBFUkEBFRy9O/fZ39B9I5caobcbEVJCZc5vU35zfM/dfWhvDBihmcO9+ZBbdv8LPF7ZfIHjbGveK7cL6t2O3w9vuznHoESqnBbNbzzvuzeeG//+zxsLye8HDXtQ46nY0IN9t8SVBFAuAI+0aOOMpd31/P9Kk7WbVmYoMDqMds1rN5SyYVla2fU1VpnxSXxGI0Glxuq6kJdVof4mmmTdrlcupOCMmoAJgiDjon0JS8fFfLHhxe+PSZrj62xn/UnNey5YE4lvdOYXnfFLY9HEvtpaD/+Rsw6C2OOggX2KVA8WJeIHP4UcaOPoCiWFAUCwaDCb1i4f67/0NMTJXXjttSgmo44AqDweLyCSClCIhQzBeYygWrZyRhKtUgbY4b4fQn4Vz4NoQ5W86jhAV/giQmpoouXUrIL0hqWL/hwE5Kl4sNcuJSwu69GWzakkntlRCGDT3GtMk7iQhvSbdm1wgBixd8wbTJuzic0xuDwdGXsC379CRB/yiYcONeFKVpxloSFmakd6/A6xDrDU58EIGlSjQ4AABpFZjKNJz+OPBaZXuLH923krAwI86yJoLiS7GcvxAHwDvvz+bv797K8ROpFBQlsXbdGJ586kdUVrX9e0pKLGXqpN2MG5MdMA4AOoATmDd3M2l98tHrzegVMyEhRqIia/jFT5cHxMovX3Du6xBsxuYna72i4fxWD9bmBjhJiaV0ji+jqROorTXw+pvzKCzqzI5dg5w0JK1WharqMD5fN8bn9vqKoB8OKIqNRx9eTl5+EmfyutApuoqBA06h0/pnmsofhHexITSy2ZhY6CThXYKnf9+1KCsPp6AwiabPPik1XLgQx87dA7C5yBtYrTr27O/PHfM3+shS3xL0TqCeHt0v0KP7BX+b4Rf6La3izKowbLXOF7hGJ0lb1H4FMlvDufPxPPfSImwuFhEBCI1EaCQajcTmwi8GQsWpt+ggAbF7rlwxsGtPBjv3ZFBTc32hcXVNCBWV4QFbgBQ32ELWU2VoQ+woEY5/2lA7Y14sJbpP8F7c9dhsguf+vJiKygjcLZMOCzUxcdxel9v0erPbbcFAh4kEXLH12yG898+ZaLV2kGCza1j4/c+ZMM6xks1q1bA/O52LxXEkJ5UwZPAJp2FEcUkMb7w9l7N5yQgB8bHl3LPkM9L6BF7CMe2uGlJnX+H8thCEgOTxRpTwAPVaHiYnt1edKKyrZ55Er7ewdNFqYmOqWbxgLe/9cyZ2u8Bm02IwmOnds4jJXm4A4k86rBMoLOrM+/+cicWiYGk0efDBipvpmXqesFAjf3z+HoxGAyazgkFvISzMyBO//DuxMVUYjQpPP3sv1TWhDVNOF4rjeeHlhfz+iTdISgy8ajp9lKTHzI4xLdqY8ooItz0hNBo7T/zX23TvdhGAcWOy6Z92lh27B3KlNoSBGafon37Wa9WEgUCHdQKbv87EYm2+4tBq0/DllkzyC5OoqIxouMGNJgNmi443/n4bjz3yLjt2D8RkVprMOYPVquXz9aO5Z/Ean5yHyrXplXrOrYJT394FDQ6gnvj4Cmbf/I0vTAsIOqwTKC+PbHYDg0NHvrgkhsLChGbb7XYtp86kUFUdSkFBUrNy5Pr35BUke83uQKasLJKvvxlKcUksfXoXMDrrUEDoJXTtUkJGvzPk5PZ0khfTK2bm39YyfcFgpk2JQSHE80KIXCHEQSHEJ0KITp4yzNsM6H/KZT23Xm+md89ChzCoCzRCYjLqSU6+5KYe3E7XZM93jg10cnJTeey3D7L6i7F8u3MIH/77Jh777U8oLYv0t2kAPLjsX9w0ZQdhYbVoNHZSe5zjkYc+CHh9B1/Q1khgA/C4lNIqhHgWeBz4VdvN8j5jbjjI2vU3Ul6hadAf0GpthIcZmTFtO19tHdHQYagxYWG1xMZWMGbUQVZ+OommIt2KYmXGNM+1ri4uiWHdplHkFyTRretFbpqyM+DyDTab4PW/zXeKjMxmPRaLjvf+OZOHf/yRH61zoCg25s/dzPy5m/1tSsDR1jZk6xu93AHMb5s5viMkxMKTj7/Jv/8zmT37MgAYMewo8+d+SXi4icUL1vLWu3MwmxXqG2fpFQtL7lqLRuNoiPrYI//g9TfmUVoehRCOHgX3LP6MbinFbo9rt8OO3YP46uvhmMx6Ro44wpQJexpEThtz/GQ3Xnh5IVarFrtdy+kzXflmxxB+9uCHAaV4e/pMissuvlJqOHi4D3a7QKPx/kyEzSa4UhtCWKix3XcK9iVCemhyWwjxGfCRlPJ9N9sbtyEbcWZPqkeO602On+jOp2vHcf5iHF2TS7h11tZm4aOUcLE4FqtVS5fkkquWIksJf/nbPA4d7ttQmqooFmI6VfG7X7/h5AikhF/95ieUXGquJR8bU8H/PPO/AZOxzj3eg5f/cge1xuZ1FkLYeePVZ7xaoWm3w6drx7Nu42isVi2KzhGNzb55a4cpDW9K1vQC9mQbW3SFeKQNmRDiCRzakcvd7SfQ2pC1hLS++Tz6sNtTAhwrxJISSykrj+Afy2ex/2A6is7G2DEHmDX9G/SNlqiePNXNyQEAWCwKZeWRbPxqJLc0ykiXlkVRXuF6PF1dE0pxSQyJCWVtPEPP0LtnoZvsu520vvleL9H+eNVkNm7OahiOWK061qy7EatVy/fmfOXVYwcDbW5DJoRYCswGFkpPhRXtjMqqMH73x2Vs2z6UqqoISsui+Xz9GJ59aYnT/HT24T6YzM3bRFksCrv3DnD6m1ZrdzutJaVwFDgFCIpiY+niz5wk1XU6K6GhZpYsWOvVY5vMOicHUI/ZrGf9phswm6/+nKtvNNORaWsHohk42p9OkFK2TuI0SDCZFF54+S4qq8JpnCC0WBSKznXm0JE+DBnkkDrXKxa0WlszIdT6bY3pFF1NcuIlCooScPbVkvi4cuLj/N8AszGjMnNITrzM+k2jKC6JJa1PPlMm7SKmk3fXJpSWRqMRbu5iISkrj3QZMVVUhrP8oxnsO9APu13QP/0sC+/4gi7Jl5zeV1MTwsXiWGJjK+kUHZzrLNo6O/AqYAA2CMcAdYeU8kdttqqdICW8+Mpd5Bck4aom3WQykJOb2uAEsjJzWP3FuGYLVPR6MxPHN69N/+F9n/DM80ux2rSYzXr0ihmtzs6P7l/pjdMBHOd0Nj+ZCxfiSEq6TGr38y3OPXTvdpH7l37qNdtcER1d7TIpCWC3aYiKaq6abDbr+MOf7qO8PLJBATgnN5Wnn7uXPz75OjExVdhsguUrZrDt26HotDasVi0Z/U/zw/s+ITSk9V1+Apm2zg54V6Y1wDmT14Wz+cm4G1XpdFYiI74LkJISS5k350s+XjUZu12DzabBoLcwoP8pxow61OzzXbuU8NzTr7Bt+xAKChNJ6VrM2NHZRER4p/S3uiaEF15eyLnzndEIiV0KuiRd4pGH3w8oEYzGhIWayBpxmN37BjgVAimKhVGZh13esLv3ZVBTE9qkR4UGs1nH+i+zuGPeJj75bCLfbB9SV1bu2O+Ro734vze/x89+8qG3T8undNiKQU+Ql5+E277bOIQkRze5uadP3cmQQSfYuWcAJpOeoYNO0LdPvtunbXi4kelTd3rQavf87e25FBQmOg1XCooS+Nvbc/l5AF/4SxetwWrTse9APxTFisWiY8TQXJbc5TofcfxEt2adiABsNh3HT/TAZhMu8wxWq0JObk9KyyKJDQBtQE+hOoE2EBtTiUZrx3U3b8l9S1YRF1vZbEtSYilzZgVOD0KA6upQcnJ7NctX2Gw6cnJ7UV0d6rUIpK0oio0H7l9JRWU4ly51Ij6+vEEz0BWd48vR6SxYrc5JWiHsxMeVYzQasLkZYugUG5dLo4PKCXTQWVTPMDDjFCGG7zLi9Wi1VpbctYZRI3P8ZFnrqa4JRat1rTKk1dqorgn8pp3RUTX07lV0VQcAMHZMtsviJUWxctPUnYSGGjG46e5rtWhJTAisis22ojqBNqDVSn71i3dJ6FyGwWAiNMSIoliYOf1bJo7b52/zWkV8fLnbqj6NRhIfX+5ji7xHp+hqHv7xR4SF1RISYiQkxIheb2bRnZ/Tu2cRGg3cOuvrZmtD9IqZUSMPExUZXBNhHqsYbA2ZQ0LkrnXdfH5cbyEl5BckUV0TSmr384QHaBLtWny5ZQQffTzNaSys15v5/vc2MiUARTUsFi1btg1j67fDsNs13DDyEFMn7cZgaNnKRatNw8mT3bDatPTtXeD0OSlh/aYsPl07AYtZh9BIJo7by+3f29Qu9ClbUzGoOgEVJ3btyeCTzyZy+XI0cXEVzL3lK0ZlBt6wxm4XPPviEs7mJzc4LUWxkNC5lCcfe8upUrOtx6muCSUs1IhOF/g3fz0eLRtW6VhkZeaQFYA3fVP2Z6eRV5DsFLVYLAoll2L4ZsdgJo33zHBMo5FBF/43RXUCAcaVWgNWq5bIiCs+XyB0pdbA+k2j2LVnAIrOyoSx+xg/bn9Ahr/7s9NdLvU2m/Xs2ZfhMSfQEVCdQIBwuTSKt/4xh+MnuyEExMVWsHTRavql5fnk+LW1en7/zA8oK49sKI75aOU09mX3C8hGLSEhjlkZV+pQISEmP1jUfgmwn7ZjYrFoefrZezl2ojs2mw6rVcfF4jheenUBRec6+8SGzV+PoKw8wqnqzmzWc/J0Ckdze/nEhtYwdnQ2iq75uN+gNzNh7H4/WNR+UZ1AALBnX3+MRkOTMlaHaOmaL270jQ37M7BYmofXJpOeAwf7ev34FRXhbP56BBs3j6S45Noqdak9zjNz+jcodYuyhLCj15vJGnmYs/lJPPn0Mp5+7h62bR/sVmlYxYE6HAgACooSMbooY3WIlrpurX692O2CbduH8OWWTIxGPcOHHmPGtO1uQ2iNxk5IqPfCaynh41WTWLdhNBqNHYlgxcqpTJu8k9uvIQI6Z/ZWRo08wp59/bHZNfRPP8Ob78ylvDwSS101YEFhIgcOpvHgsn9fV47FZNZRWJRIeFhtwMm6eQrVCbQBk0mhtCyKTp2q2rSyLDGhFIPehMnc1BHYSUq83DYjGyElvP7mPA4e7tOQVd/wZSe27xzEbbdu5vTpFCfBE3DoGrha3OQJ8vKTeOnVBd91BmpUsLhxcxb9088yMOP0VfeRlFjaIA/+2edjnRwAOIY0h4/04eTpFPr2bp2o6LqNWXzy6WQ0Gjs2m4aEhFIeemAFnYOocArU4cB1YbMJPlhxEw89+ii//+/7efi/HuEfy2ditV7f15mVeQStzk7T1Uh6vZVZ0z2nf3/mbBcONXIA4FDhqa4J4+LFOEZmHmkQBtFqrSiKhflzN5Gc5DlHVI/RqPDcS4upqIzE1TJss1nPl19ltmqfu/dmODmAekxmhexDrRvS7N2fzspPJ2My66k1hmC26Ck6l8CfXrg76IYXaiRwHfzrkyls2TYcc6Mk2rc7BmOXgnsWtb7pSGiImccfeYfX3rid0rIoNEKi1dq5e+FqevU85zG7Dx/tjdnS/Ce3WnXsPdCPP/3hL0yduJvsw31RdFYyhx/12lNv974MbParO83qmrBW7VOvd10pqNXaMLjZ5o5P145vtopQSg1XakM4dKQ3QwadbNX+AhnVCbQSs1nH5i0jnRwAgNmiZ/vOwdwxbyNh1zGGTulawjO/+wsXi2MxmxW6din2uGJuiMGMVmtzGbGE1A1nfNW9+XJptMt5/noUxcKwIcdatc+J4/ZSUJjY7ObVaCRZmUdabZ8rbDYNly63m/YaLUIdDrSSyspwhBs5K63WRmlpFBaLlsM5vThwsC+1RvcXelPqRUu7d7voFcnskcNzXCbH9Hozkyf4dm1Aj24XCDG4zqMIYScqsoYJrVyENWbUIQYNOFm3AlA2DGlun7ux1aKsKV1cy8ZrNHZSurqXlG+PqJFAK4mOrnarI2Kzajl/IZ4/Pn8P4Bjp2mxa7rz9CyaNd567tli0nDjVHZtNQ1rfPAweqnW/GjExVSy5aw3vfjALKQU2mwZFsTJowEnGjs72+vEbM3jgCTp1qqLkkraJhoFkzKiD3Dl/Q6siqqPHUvno46kUFiUSYjDRM7WI/ulnGDXyyHWpMt9261e8+MpdTlGFTmclMaGUtD75rd5fINOmBURCiKeAOYAdKAaWSimvOYht7wuIPvp4Cl9uGdlstd3wIbnsy+7XLBzV68388mfv0btXEQDZh/ryf2/d1rDdbtewZOFqxow67BP7y8oi2bU3A5NJz8CMU/RMPeeXHgbV1aG898+b2XugP3a7oHvKRRYvWNvwPbWUnNye/O9rd2C2OP8eozIPc++S1ddtX/ahvrz34c1UlEcAMGzIMe5euKZdrBL12SpCIUSUlLKy7v8PARktERpt707AbhesWDmFzVtGIjR27HYNY0cfIDKyhrXrbnSpWDNyeA4P/GAlly5F88TvH3C6YMFx0TZuke1rLlyMpaAwkbi4Cnr28K1TsNkEdrsGRXEtanItnnx6GQWFzespdDorzz71SptUgKSEmishGPSW67bPH/hsFWG9A6gjnKsq7gUPGo3kzvkbue3Wrygri6JTdBUhIRb+/u7sZg4AHFnl+kTTlm3DXWbFrVYtGzdnce+Sz7xuf2MsFi2vvTGfnNxeaLU2pBTEx1Xw4A9XcOp0ClVV4fTuVUjf3gVecwxarXSratQSCosSXP5dp7NyNq8LsTGtSzA2RggCVmTVU7Q5JyCE+COwBKgAJrXZonaEQW91qiJLT8tn194BzUQsdToL/dLPAlByqZPLvgN2u5aSS77POn/472nk5PZyUtU9dz6eJ373Y/R6C1arFp3WTo8e53jkpx94bJ2+JwkLNVJzpfl0opTCpeS4ijPXnB0QQmwUQhx28W8OgJTyCSllNxwtyH5ylf0sE0LsEULsKbncfsKq1pA14giREVfQar+7URw17VamTdoFQHpansuW5opiId1HKwbrsdsF274d5rRoCByRi5QaTCYDNpsOk1nPmbNdWbVmvE/taylTJu5Grzh/p0LYiYqqoXdPtfX4tWhzG7JGLAfmXWU/b0gpM6WUmZ3jXCu5tncUxcZvfvUWWSOONCxsGTzwBE8+9ibR0Y4n0uisQ4SFGtFovnOEQtjRKxYmj/ftNJ3ZrMNqa9ksscWi8PW24V626Pq4ZdZWhg4+jqJYHJqBBhNxsRU88tPlAdO0NZBpaxuyvlLKE3Uv5wC5bTepfRMVdYVl965iGU19pIP6lugffDSD/QfTkVIwoP8pFt35BVFRvlWwMRgsdIquprTMdWFMU8wu+igGAjqtnQd+sJLikhjO5CUTE11Nn975AaeBEKi0NSfwJyFEOo4pwjygw7Qgawsxnap58If/bmiE6a+nlRBw5/z1vPnOnCazFZLm9fyStL6+Ha60loTOZSR0DoxOze2Jts4OuA3/Va5NIISqI0ccRVFs/OuTKVy4GEtUZA2JiZc4cyalwTFoNDYUxcod8zb42VoVb6BWDKowdPBxhg4+3vBaStixeyDrNt5AZWUEaX3zmDPra6+sJlTxP6oTUGmGEDA66zCjs3xTwajiX9TUiYpKB0d1AioqHRzVCaiodHDUnICKihvKKyLYsnUYhecS6NH9AuNv3B+U3YhUJ6Ci4oJTZ7ry/J8XYbNpsFoVsg+lsXbdjfz60bdJ6Vrib/M8ijocUFFpgpTw17e+h8lkaFgVarEo1NYa+Ns7c/1snedRnYCKShMuFsdSWRnuYovg3PnOVFa1TgA10FGdQAswmXXU1rZcK1ClfSPl1Us5r7W9vaHmBK7C5dIo3n7vFnKPp4KELl1KWOphGXCVwCMp8TLhYbXNZOIAEjqXEh1kGgVqJOAGk1nHU8/ex9FjqdhsWmx2LQWFSTz30hIuFsf42zwVLyIELLvnPxj05gZtCJ3OSojBxP1LXa8Obc+okYAbdu0ZgNGob9Yk1GLV8sWG0dy9cK2fLFPxBf3S8/jDb/7Kpq9GUliUSGqPc0yesIe42Mprf7idoToBN+TlJTeTCQOHDNiZs139YJGKr0noXM6C24N/5aQ6HHBDQkKpSxkwIewkerBJqIqKv1GdgBvG3HAQjaa5eLKiszJj2nY/WKSi4h1UJ+CGiHAjv/z5u8TGlGMwmAgJMREWVst9d6+iZ4/z/jZPRcVjqDmBq9Czx3n+55mXKSxKwGLR0b37BXRau7/NUlHxKKoTuAZCQLeU4GpAqaLSGI8MB4QQjwghpBAi3hP7U1FR8R1tdgJCiG7ATUBwtWpVUekgeCISeAn4JR2kD6GKSrDR1uYjc4AiKWW2uIZ+thBiGbCs7qVJm3wyGFUs44FL/jbCSwTruQXreaW39I3XbE0uhNgINO/7DE8AvwZuDIyTgAAAAotJREFUklJWCCHOAplSymt+oUKIPVLKzJYa2V4I1vOC4D039bxaEAlIKae6OcggoCdQHwWkAPuEEFlSygutsFdFRcWPXPdwQEp5CGhoDN+aSEBFRSVw8FfF4Bt+Oq63CdbzguA9tw5/XtfMCaioqAQ36toBFZUOjuoEVFQ6OH53AsFWciyEeF4IkSuEOCiE+EQI0cnfNrUFIcQMIcQxIcRJIcRj/rbHUwghugkhNgshcoQQR4QQD/vbJk8ihNAKIfYLIVZf671+dQJBWnK8ARgopRwMHAce97M9140QQgu8BtwMZAALhBAZ/rXKY1iBR6SUGcANwINBdG4ADwNHW/JGf0cCQVdyLKVcL6W01r3cgaN+or2SBZyUUp6WUpqBD4E5frbJI0gpz0sp99X9vwrHDRMUunFCiBRgFvBmS97vNyfQuOTYXzb4gHuBz/1tRBvoChQ0el1IkNwojRFCpALDgJ3+tcRj/BnHw7VF4hde1RNoScmxN4/vLa52XlLKVXXveQJHyLncl7aptA4hRATwMfAzKWW7lxIWQswGiqWUe4UQE1vyGa86gWAtOXZ3XvUIIZYCs4Epsn0XYhQB3Rq9Tqn7W1AghFBwOIDlUsqV/rbHQ9wI3CqEmAmEAFFCiPellIvcfSAgioWCqeRYCDEDeBGYIKVs1+1rhRA6HMnNKThu/t3AXVLKI341zAMIx9PnH0CplPJn/rbHG9RFAo9KKWdf7X3+TgwGI68CkcAGIcQBIcRf/W3Q9VKX4PwJsA5H4mxFMDiAOm4EFgOT636nA3VPzw5HQEQCKioq/kONBFRUOjiqE1BR6eCoTkBFpYOjOgEVlQ6O6gRUVDo4qhNQUengqE5ARaWD8/8mk0MIspCMSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_range = np.arange(-5., 5., 0.02)\n",
    "dgrid = len(grid_range)\n",
    "Xpred = np.array([(a, b) for a in grid_range for b in grid_range])\n",
    "prob_pred = nn_tanh.predict(Xpred)\n",
    "ypred = np.round(prob_pred)\n",
    "ypred = ypred.reshape((dgrid, dgrid))\n",
    "ypred = np.flipud(ypred)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.imshow(ypred, extent=[-4, 4, -4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tanh activation function is continuous and it results in a smooth decision boundary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
