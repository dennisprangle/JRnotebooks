{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on concentric circles data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200 # Number of points in dataset\n",
    "np.random.seed(0)\n",
    "r1 = np.random.gamma(10, 0.1, n/2)\n",
    "r2 = np.random.gamma(30, 0.1, n/2)\n",
    "phi1 = np.random.uniform(-np.pi,np.pi,n/2)\n",
    "phi2 = np.random.uniform(-np.pi,np.pi,n/2)\n",
    "r = np.hstack((r1, r2))\n",
    "phi = np.hstack((phi1, phi2))\n",
    "group = np.hstack((np.repeat(0,n/2), np.repeat(1,n/2)))\n",
    "x0 = r*np.cos(phi)\n",
    "x1 = r*np.sin(phi)\n",
    "X = np.stack((x0,x1), axis=1)\n",
    "y = group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we fit a model with ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_relu = models.Sequential()\n",
    "nn_relu.add(layers.Dense(5, input_dim=2, activation=\"relu\"))\n",
    "nn_relu.add(layers.Dense(5, activation=\"relu\"))\n",
    "nn_relu.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/800\n",
      "160/160 [==============================] - 0s 933us/step - loss: 0.7751 - acc: 0.6250 - val_loss: 0.9152 - val_acc: 0.0000e+00\n",
      "Epoch 2/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.7683 - acc: 0.6250 - val_loss: 0.8995 - val_acc: 0.0000e+00\n",
      "Epoch 3/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.7618 - acc: 0.6250 - val_loss: 0.8849 - val_acc: 0.0000e+00\n",
      "Epoch 4/800\n",
      "160/160 [==============================] - 0s 126us/step - loss: 0.7559 - acc: 0.6250 - val_loss: 0.8707 - val_acc: 0.0000e+00\n",
      "Epoch 5/800\n",
      "160/160 [==============================] - 0s 107us/step - loss: 0.7496 - acc: 0.6250 - val_loss: 0.8577 - val_acc: 0.0000e+00\n",
      "Epoch 6/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.7448 - acc: 0.6250 - val_loss: 0.8438 - val_acc: 0.0000e+00\n",
      "Epoch 7/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.7389 - acc: 0.6250 - val_loss: 0.8318 - val_acc: 0.0000e+00\n",
      "Epoch 8/800\n",
      "160/160 [==============================] - 0s 111us/step - loss: 0.7338 - acc: 0.6250 - val_loss: 0.8214 - val_acc: 0.0000e+00\n",
      "Epoch 9/800\n",
      "160/160 [==============================] - 0s 136us/step - loss: 0.7285 - acc: 0.6312 - val_loss: 0.8122 - val_acc: 0.0500\n",
      "Epoch 10/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.7244 - acc: 0.6437 - val_loss: 0.8021 - val_acc: 0.0500\n",
      "Epoch 11/800\n",
      "160/160 [==============================] - 0s 118us/step - loss: 0.7200 - acc: 0.6625 - val_loss: 0.7927 - val_acc: 0.0500\n",
      "Epoch 12/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.7155 - acc: 0.6500 - val_loss: 0.7843 - val_acc: 0.0500\n",
      "Epoch 13/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.7113 - acc: 0.6500 - val_loss: 0.7769 - val_acc: 0.0500\n",
      "Epoch 14/800\n",
      "160/160 [==============================] - 0s 130us/step - loss: 0.7073 - acc: 0.6437 - val_loss: 0.7703 - val_acc: 0.0750\n",
      "Epoch 15/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.7035 - acc: 0.6437 - val_loss: 0.7640 - val_acc: 0.1000\n",
      "Epoch 16/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.7002 - acc: 0.6500 - val_loss: 0.7576 - val_acc: 0.1000\n",
      "Epoch 17/800\n",
      "160/160 [==============================] - 0s 118us/step - loss: 0.6964 - acc: 0.6437 - val_loss: 0.7523 - val_acc: 0.1500\n",
      "Epoch 18/800\n",
      "160/160 [==============================] - 0s 117us/step - loss: 0.6930 - acc: 0.6437 - val_loss: 0.7473 - val_acc: 0.1500\n",
      "Epoch 19/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.6900 - acc: 0.6375 - val_loss: 0.7417 - val_acc: 0.1500\n",
      "Epoch 20/800\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.6862 - acc: 0.6562 - val_loss: 0.7383 - val_acc: 0.1750\n",
      "Epoch 21/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.6834 - acc: 0.6500 - val_loss: 0.7343 - val_acc: 0.2000\n",
      "Epoch 22/800\n",
      "160/160 [==============================] - 0s 103us/step - loss: 0.6808 - acc: 0.6437 - val_loss: 0.7292 - val_acc: 0.2000\n",
      "Epoch 23/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.6775 - acc: 0.6500 - val_loss: 0.7259 - val_acc: 0.2000\n",
      "Epoch 24/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.6749 - acc: 0.6562 - val_loss: 0.7231 - val_acc: 0.2250\n",
      "Epoch 25/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.6722 - acc: 0.6625 - val_loss: 0.7205 - val_acc: 0.2250\n",
      "Epoch 26/800\n",
      "160/160 [==============================] - 0s 158us/step - loss: 0.6695 - acc: 0.6625 - val_loss: 0.7185 - val_acc: 0.2250\n",
      "Epoch 27/800\n",
      "160/160 [==============================] - 0s 110us/step - loss: 0.6673 - acc: 0.6625 - val_loss: 0.7155 - val_acc: 0.2250\n",
      "Epoch 28/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.6647 - acc: 0.6625 - val_loss: 0.7132 - val_acc: 0.2250\n",
      "Epoch 29/800\n",
      "160/160 [==============================] - 0s 114us/step - loss: 0.6621 - acc: 0.6625 - val_loss: 0.7114 - val_acc: 0.2250\n",
      "Epoch 30/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.6596 - acc: 0.6688 - val_loss: 0.7102 - val_acc: 0.2250\n",
      "Epoch 31/800\n",
      "160/160 [==============================] - 0s 112us/step - loss: 0.6570 - acc: 0.6750 - val_loss: 0.7084 - val_acc: 0.2250\n",
      "Epoch 32/800\n",
      "160/160 [==============================] - 0s 117us/step - loss: 0.6547 - acc: 0.6813 - val_loss: 0.7063 - val_acc: 0.2500\n",
      "Epoch 33/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.6521 - acc: 0.6750 - val_loss: 0.7054 - val_acc: 0.2750\n",
      "Epoch 34/800\n",
      "160/160 [==============================] - 0s 153us/step - loss: 0.6496 - acc: 0.6750 - val_loss: 0.7039 - val_acc: 0.2750\n",
      "Epoch 35/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.6470 - acc: 0.6750 - val_loss: 0.7023 - val_acc: 0.2750\n",
      "Epoch 36/800\n",
      "160/160 [==============================] - 0s 107us/step - loss: 0.6447 - acc: 0.6750 - val_loss: 0.6997 - val_acc: 0.2750\n",
      "Epoch 37/800\n",
      "160/160 [==============================] - 0s 146us/step - loss: 0.6419 - acc: 0.6750 - val_loss: 0.6985 - val_acc: 0.2750\n",
      "Epoch 38/800\n",
      "160/160 [==============================] - 0s 189us/step - loss: 0.6393 - acc: 0.6875 - val_loss: 0.6985 - val_acc: 0.2750\n",
      "Epoch 39/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.6369 - acc: 0.6875 - val_loss: 0.6976 - val_acc: 0.2750\n",
      "Epoch 40/800\n",
      "160/160 [==============================] - 0s 107us/step - loss: 0.6346 - acc: 0.7000 - val_loss: 0.6973 - val_acc: 0.2750\n",
      "Epoch 41/800\n",
      "160/160 [==============================] - 0s 163us/step - loss: 0.6324 - acc: 0.6937 - val_loss: 0.6959 - val_acc: 0.2750\n",
      "Epoch 42/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.6301 - acc: 0.7000 - val_loss: 0.6961 - val_acc: 0.2750\n",
      "Epoch 43/800\n",
      "160/160 [==============================] - 0s 114us/step - loss: 0.6281 - acc: 0.7000 - val_loss: 0.6951 - val_acc: 0.2750\n",
      "Epoch 44/800\n",
      "160/160 [==============================] - 0s 118us/step - loss: 0.6259 - acc: 0.7062 - val_loss: 0.6949 - val_acc: 0.2750\n",
      "Epoch 45/800\n",
      "160/160 [==============================] - 0s 126us/step - loss: 0.6235 - acc: 0.7125 - val_loss: 0.6953 - val_acc: 0.2750\n",
      "Epoch 46/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.6214 - acc: 0.7125 - val_loss: 0.6948 - val_acc: 0.2750\n",
      "Epoch 47/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.6191 - acc: 0.7125 - val_loss: 0.6957 - val_acc: 0.2750\n",
      "Epoch 48/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.6168 - acc: 0.7188 - val_loss: 0.6954 - val_acc: 0.2750\n",
      "Epoch 49/800\n",
      "160/160 [==============================] - 0s 120us/step - loss: 0.6145 - acc: 0.7188 - val_loss: 0.6956 - val_acc: 0.3000\n",
      "Epoch 50/800\n",
      "160/160 [==============================] - 0s 105us/step - loss: 0.6125 - acc: 0.7188 - val_loss: 0.6951 - val_acc: 0.3000\n",
      "Epoch 51/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.6102 - acc: 0.7188 - val_loss: 0.6967 - val_acc: 0.3000\n",
      "Epoch 52/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.6078 - acc: 0.7188 - val_loss: 0.6976 - val_acc: 0.3000\n",
      "Epoch 53/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.6057 - acc: 0.7188 - val_loss: 0.6973 - val_acc: 0.3000\n",
      "Epoch 54/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.6036 - acc: 0.7250 - val_loss: 0.6985 - val_acc: 0.3000\n",
      "Epoch 55/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.6014 - acc: 0.7250 - val_loss: 0.6984 - val_acc: 0.3000\n",
      "Epoch 56/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.5992 - acc: 0.7313 - val_loss: 0.6995 - val_acc: 0.3000\n",
      "Epoch 57/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.5971 - acc: 0.7375 - val_loss: 0.7004 - val_acc: 0.3000\n",
      "Epoch 58/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.5948 - acc: 0.7375 - val_loss: 0.7013 - val_acc: 0.3000\n",
      "Epoch 59/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.5926 - acc: 0.7438 - val_loss: 0.7019 - val_acc: 0.3000\n",
      "Epoch 60/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.5904 - acc: 0.7438 - val_loss: 0.7031 - val_acc: 0.3250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/800\n",
      "160/160 [==============================] - 0s 104us/step - loss: 0.5882 - acc: 0.7438 - val_loss: 0.7044 - val_acc: 0.3250\n",
      "Epoch 62/800\n",
      "160/160 [==============================] - 0s 119us/step - loss: 0.5861 - acc: 0.7438 - val_loss: 0.7053 - val_acc: 0.3250\n",
      "Epoch 63/800\n",
      "160/160 [==============================] - 0s 116us/step - loss: 0.5840 - acc: 0.7500 - val_loss: 0.7068 - val_acc: 0.3250\n",
      "Epoch 64/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.5819 - acc: 0.7500 - val_loss: 0.7086 - val_acc: 0.3250\n",
      "Epoch 65/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.5797 - acc: 0.7562 - val_loss: 0.7092 - val_acc: 0.3250\n",
      "Epoch 66/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.5777 - acc: 0.7562 - val_loss: 0.7100 - val_acc: 0.3250\n",
      "Epoch 67/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.5755 - acc: 0.7562 - val_loss: 0.7112 - val_acc: 0.3250\n",
      "Epoch 68/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.5734 - acc: 0.7562 - val_loss: 0.7123 - val_acc: 0.3250\n",
      "Epoch 69/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.5714 - acc: 0.7562 - val_loss: 0.7148 - val_acc: 0.3250\n",
      "Epoch 70/800\n",
      "160/160 [==============================] - 0s 119us/step - loss: 0.5692 - acc: 0.7562 - val_loss: 0.7154 - val_acc: 0.3250\n",
      "Epoch 71/800\n",
      "160/160 [==============================] - 0s 137us/step - loss: 0.5673 - acc: 0.7625 - val_loss: 0.7180 - val_acc: 0.3250\n",
      "Epoch 72/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.5650 - acc: 0.7625 - val_loss: 0.7195 - val_acc: 0.3250\n",
      "Epoch 73/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.5630 - acc: 0.7625 - val_loss: 0.7204 - val_acc: 0.3250\n",
      "Epoch 74/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.5608 - acc: 0.7687 - val_loss: 0.7217 - val_acc: 0.3250\n",
      "Epoch 75/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.5587 - acc: 0.7687 - val_loss: 0.7233 - val_acc: 0.3250\n",
      "Epoch 76/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.5564 - acc: 0.7687 - val_loss: 0.7254 - val_acc: 0.3250\n",
      "Epoch 77/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.5543 - acc: 0.7687 - val_loss: 0.7270 - val_acc: 0.3250\n",
      "Epoch 78/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.5522 - acc: 0.7687 - val_loss: 0.7288 - val_acc: 0.3250\n",
      "Epoch 79/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.5499 - acc: 0.7750 - val_loss: 0.7306 - val_acc: 0.3250\n",
      "Epoch 80/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.5476 - acc: 0.7750 - val_loss: 0.7321 - val_acc: 0.3250\n",
      "Epoch 81/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.5454 - acc: 0.7750 - val_loss: 0.7327 - val_acc: 0.3250\n",
      "Epoch 82/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.5432 - acc: 0.7750 - val_loss: 0.7346 - val_acc: 0.3250\n",
      "Epoch 83/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.5409 - acc: 0.7812 - val_loss: 0.7371 - val_acc: 0.3250\n",
      "Epoch 84/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.5387 - acc: 0.7812 - val_loss: 0.7389 - val_acc: 0.3250\n",
      "Epoch 85/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.5363 - acc: 0.7812 - val_loss: 0.7397 - val_acc: 0.3250\n",
      "Epoch 86/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.5341 - acc: 0.7812 - val_loss: 0.7417 - val_acc: 0.3250\n",
      "Epoch 87/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.5319 - acc: 0.7812 - val_loss: 0.7443 - val_acc: 0.3250\n",
      "Epoch 88/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.5293 - acc: 0.7812 - val_loss: 0.7456 - val_acc: 0.3250\n",
      "Epoch 89/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.5270 - acc: 0.7812 - val_loss: 0.7466 - val_acc: 0.3250\n",
      "Epoch 90/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.5245 - acc: 0.7812 - val_loss: 0.7478 - val_acc: 0.3250\n",
      "Epoch 91/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.5223 - acc: 0.7812 - val_loss: 0.7502 - val_acc: 0.3250\n",
      "Epoch 92/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.5199 - acc: 0.7812 - val_loss: 0.7521 - val_acc: 0.3250\n",
      "Epoch 93/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.5173 - acc: 0.7812 - val_loss: 0.7530 - val_acc: 0.3250\n",
      "Epoch 94/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.5148 - acc: 0.7812 - val_loss: 0.7531 - val_acc: 0.3250\n",
      "Epoch 95/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.5124 - acc: 0.7812 - val_loss: 0.7543 - val_acc: 0.3250\n",
      "Epoch 96/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.5099 - acc: 0.7812 - val_loss: 0.7568 - val_acc: 0.3250\n",
      "Epoch 97/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.5073 - acc: 0.7812 - val_loss: 0.7566 - val_acc: 0.3250\n",
      "Epoch 98/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.5048 - acc: 0.7812 - val_loss: 0.7572 - val_acc: 0.3250\n",
      "Epoch 99/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.5023 - acc: 0.7812 - val_loss: 0.7602 - val_acc: 0.3250\n",
      "Epoch 100/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.4997 - acc: 0.7812 - val_loss: 0.7606 - val_acc: 0.3250\n",
      "Epoch 101/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.4972 - acc: 0.7812 - val_loss: 0.7625 - val_acc: 0.3250\n",
      "Epoch 102/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.4946 - acc: 0.7812 - val_loss: 0.7639 - val_acc: 0.3250\n",
      "Epoch 103/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.4920 - acc: 0.7812 - val_loss: 0.7644 - val_acc: 0.3250\n",
      "Epoch 104/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.4896 - acc: 0.7812 - val_loss: 0.7671 - val_acc: 0.3250\n",
      "Epoch 105/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.4868 - acc: 0.7812 - val_loss: 0.7676 - val_acc: 0.3250\n",
      "Epoch 106/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.4842 - acc: 0.7812 - val_loss: 0.7682 - val_acc: 0.3250\n",
      "Epoch 107/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.4817 - acc: 0.7812 - val_loss: 0.7688 - val_acc: 0.3250\n",
      "Epoch 108/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.4790 - acc: 0.7812 - val_loss: 0.7696 - val_acc: 0.3250\n",
      "Epoch 109/800\n",
      "160/160 [==============================] - 0s 99us/step - loss: 0.4763 - acc: 0.7812 - val_loss: 0.7696 - val_acc: 0.3250\n",
      "Epoch 110/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.4735 - acc: 0.7812 - val_loss: 0.7700 - val_acc: 0.3250\n",
      "Epoch 111/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.4708 - acc: 0.7812 - val_loss: 0.7701 - val_acc: 0.3250\n",
      "Epoch 112/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.4681 - acc: 0.7812 - val_loss: 0.7700 - val_acc: 0.3250\n",
      "Epoch 113/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.4653 - acc: 0.7812 - val_loss: 0.7707 - val_acc: 0.3250\n",
      "Epoch 114/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.4625 - acc: 0.7812 - val_loss: 0.7723 - val_acc: 0.3250\n",
      "Epoch 115/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.4593 - acc: 0.7812 - val_loss: 0.7696 - val_acc: 0.3500\n",
      "Epoch 116/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.4562 - acc: 0.7812 - val_loss: 0.7680 - val_acc: 0.3500\n",
      "Epoch 117/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.4526 - acc: 0.7812 - val_loss: 0.7635 - val_acc: 0.3500\n",
      "Epoch 118/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.4487 - acc: 0.7812 - val_loss: 0.7594 - val_acc: 0.3500\n",
      "Epoch 119/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.4448 - acc: 0.7875 - val_loss: 0.7538 - val_acc: 0.3500\n",
      "Epoch 120/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.4404 - acc: 0.7938 - val_loss: 0.7494 - val_acc: 0.3500\n",
      "Epoch 121/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 90us/step - loss: 0.4358 - acc: 0.8000 - val_loss: 0.7435 - val_acc: 0.3750\n",
      "Epoch 122/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.4309 - acc: 0.8063 - val_loss: 0.7365 - val_acc: 0.4000\n",
      "Epoch 123/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.4258 - acc: 0.8063 - val_loss: 0.7295 - val_acc: 0.4250\n",
      "Epoch 124/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.4203 - acc: 0.8312 - val_loss: 0.7220 - val_acc: 0.4500\n",
      "Epoch 125/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.4153 - acc: 0.8312 - val_loss: 0.7135 - val_acc: 0.4500\n",
      "Epoch 126/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.4103 - acc: 0.8375 - val_loss: 0.7078 - val_acc: 0.4750\n",
      "Epoch 127/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.4048 - acc: 0.8438 - val_loss: 0.7018 - val_acc: 0.5000\n",
      "Epoch 128/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.3997 - acc: 0.8438 - val_loss: 0.6965 - val_acc: 0.5250\n",
      "Epoch 129/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.3948 - acc: 0.8438 - val_loss: 0.6916 - val_acc: 0.5250\n",
      "Epoch 130/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.3896 - acc: 0.8500 - val_loss: 0.6878 - val_acc: 0.5750\n",
      "Epoch 131/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.3848 - acc: 0.8500 - val_loss: 0.6833 - val_acc: 0.6000\n",
      "Epoch 132/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.3799 - acc: 0.8563 - val_loss: 0.6777 - val_acc: 0.6000\n",
      "Epoch 133/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.3752 - acc: 0.8563 - val_loss: 0.6729 - val_acc: 0.6000\n",
      "Epoch 134/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.3702 - acc: 0.8625 - val_loss: 0.6708 - val_acc: 0.6000\n",
      "Epoch 135/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.3653 - acc: 0.8688 - val_loss: 0.6687 - val_acc: 0.6000\n",
      "Epoch 136/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.3606 - acc: 0.8812 - val_loss: 0.6653 - val_acc: 0.6000\n",
      "Epoch 137/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.3560 - acc: 0.8812 - val_loss: 0.6621 - val_acc: 0.6250\n",
      "Epoch 138/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.3515 - acc: 0.8812 - val_loss: 0.6594 - val_acc: 0.6250\n",
      "Epoch 139/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.3469 - acc: 0.8812 - val_loss: 0.6540 - val_acc: 0.6250\n",
      "Epoch 140/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.3423 - acc: 0.8812 - val_loss: 0.6499 - val_acc: 0.6500\n",
      "Epoch 141/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.3374 - acc: 0.8937 - val_loss: 0.6495 - val_acc: 0.6500\n",
      "Epoch 142/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.3328 - acc: 0.8937 - val_loss: 0.6463 - val_acc: 0.6500\n",
      "Epoch 143/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.3285 - acc: 0.9000 - val_loss: 0.6415 - val_acc: 0.6500\n",
      "Epoch 144/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.3238 - acc: 0.9000 - val_loss: 0.6383 - val_acc: 0.6500\n",
      "Epoch 145/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.3196 - acc: 0.9000 - val_loss: 0.6355 - val_acc: 0.6500\n",
      "Epoch 146/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.3150 - acc: 0.9062 - val_loss: 0.6309 - val_acc: 0.6500\n",
      "Epoch 147/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.3106 - acc: 0.9062 - val_loss: 0.6281 - val_acc: 0.6500\n",
      "Epoch 148/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.3065 - acc: 0.9125 - val_loss: 0.6233 - val_acc: 0.6500\n",
      "Epoch 149/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.3022 - acc: 0.9125 - val_loss: 0.6193 - val_acc: 0.6500\n",
      "Epoch 150/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.2981 - acc: 0.9125 - val_loss: 0.6156 - val_acc: 0.6500\n",
      "Epoch 151/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.2940 - acc: 0.9188 - val_loss: 0.6119 - val_acc: 0.6750\n",
      "Epoch 152/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.2902 - acc: 0.9188 - val_loss: 0.6073 - val_acc: 0.6750\n",
      "Epoch 153/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.2861 - acc: 0.9188 - val_loss: 0.6052 - val_acc: 0.6750\n",
      "Epoch 154/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.2822 - acc: 0.9250 - val_loss: 0.6018 - val_acc: 0.6750\n",
      "Epoch 155/800\n",
      "160/160 [==============================] - 0s 107us/step - loss: 0.2784 - acc: 0.9313 - val_loss: 0.5972 - val_acc: 0.6750\n",
      "Epoch 156/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.2745 - acc: 0.9313 - val_loss: 0.5949 - val_acc: 0.6750\n",
      "Epoch 157/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.2711 - acc: 0.9313 - val_loss: 0.5926 - val_acc: 0.6750\n",
      "Epoch 158/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.2674 - acc: 0.9313 - val_loss: 0.5896 - val_acc: 0.6750\n",
      "Epoch 159/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.2637 - acc: 0.9313 - val_loss: 0.5843 - val_acc: 0.7000\n",
      "Epoch 160/800\n",
      "160/160 [==============================] - 0s 113us/step - loss: 0.2605 - acc: 0.9250 - val_loss: 0.5798 - val_acc: 0.7000\n",
      "Epoch 161/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.2570 - acc: 0.9313 - val_loss: 0.5740 - val_acc: 0.7000\n",
      "Epoch 162/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.2537 - acc: 0.9375 - val_loss: 0.5691 - val_acc: 0.7000\n",
      "Epoch 163/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.2505 - acc: 0.9437 - val_loss: 0.5647 - val_acc: 0.7000\n",
      "Epoch 164/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.2473 - acc: 0.9375 - val_loss: 0.5633 - val_acc: 0.7000\n",
      "Epoch 165/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.2443 - acc: 0.9375 - val_loss: 0.5605 - val_acc: 0.7000\n",
      "Epoch 166/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.2411 - acc: 0.9375 - val_loss: 0.5587 - val_acc: 0.7000\n",
      "Epoch 167/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.2381 - acc: 0.9375 - val_loss: 0.5562 - val_acc: 0.7000\n",
      "Epoch 168/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.2352 - acc: 0.9375 - val_loss: 0.5535 - val_acc: 0.7000\n",
      "Epoch 169/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.2323 - acc: 0.9313 - val_loss: 0.5487 - val_acc: 0.7000\n",
      "Epoch 170/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.2295 - acc: 0.9313 - val_loss: 0.5460 - val_acc: 0.7000\n",
      "Epoch 171/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.2267 - acc: 0.9313 - val_loss: 0.5412 - val_acc: 0.7000\n",
      "Epoch 172/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.2241 - acc: 0.9313 - val_loss: 0.5379 - val_acc: 0.7000\n",
      "Epoch 173/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.2214 - acc: 0.9313 - val_loss: 0.5349 - val_acc: 0.7000\n",
      "Epoch 174/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.2189 - acc: 0.9375 - val_loss: 0.5324 - val_acc: 0.7000\n",
      "Epoch 175/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.2162 - acc: 0.9375 - val_loss: 0.5293 - val_acc: 0.7000\n",
      "Epoch 176/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.2138 - acc: 0.9375 - val_loss: 0.5264 - val_acc: 0.7000\n",
      "Epoch 177/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.2112 - acc: 0.9375 - val_loss: 0.5242 - val_acc: 0.7000\n",
      "Epoch 178/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.2087 - acc: 0.9375 - val_loss: 0.5202 - val_acc: 0.7000\n",
      "Epoch 179/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.2063 - acc: 0.9375 - val_loss: 0.5164 - val_acc: 0.7000\n",
      "Epoch 180/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.2039 - acc: 0.9375 - val_loss: 0.5142 - val_acc: 0.7000\n",
      "Epoch 181/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 69us/step - loss: 0.2015 - acc: 0.9375 - val_loss: 0.5112 - val_acc: 0.7000\n",
      "Epoch 182/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.1994 - acc: 0.9375 - val_loss: 0.5090 - val_acc: 0.7000\n",
      "Epoch 183/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.1973 - acc: 0.9375 - val_loss: 0.5050 - val_acc: 0.7000\n",
      "Epoch 184/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.1951 - acc: 0.9375 - val_loss: 0.5023 - val_acc: 0.7000\n",
      "Epoch 185/800\n",
      "160/160 [==============================] - 0s 108us/step - loss: 0.1931 - acc: 0.9375 - val_loss: 0.4994 - val_acc: 0.7000\n",
      "Epoch 186/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.1911 - acc: 0.9375 - val_loss: 0.4977 - val_acc: 0.7000\n",
      "Epoch 187/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.1892 - acc: 0.9375 - val_loss: 0.4954 - val_acc: 0.7000\n",
      "Epoch 188/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.1873 - acc: 0.9375 - val_loss: 0.4927 - val_acc: 0.7000\n",
      "Epoch 189/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.1854 - acc: 0.9375 - val_loss: 0.4895 - val_acc: 0.7000\n",
      "Epoch 190/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.1836 - acc: 0.9375 - val_loss: 0.4865 - val_acc: 0.7500\n",
      "Epoch 191/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1818 - acc: 0.9375 - val_loss: 0.4839 - val_acc: 0.7500\n",
      "Epoch 192/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.1802 - acc: 0.9437 - val_loss: 0.4809 - val_acc: 0.7500\n",
      "Epoch 193/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.1785 - acc: 0.9437 - val_loss: 0.4808 - val_acc: 0.7500\n",
      "Epoch 194/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.1768 - acc: 0.9437 - val_loss: 0.4779 - val_acc: 0.7500\n",
      "Epoch 195/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.1754 - acc: 0.9437 - val_loss: 0.4741 - val_acc: 0.7500\n",
      "Epoch 196/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.1736 - acc: 0.9437 - val_loss: 0.4729 - val_acc: 0.7500\n",
      "Epoch 197/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.1722 - acc: 0.9437 - val_loss: 0.4702 - val_acc: 0.7750\n",
      "Epoch 198/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.1705 - acc: 0.9437 - val_loss: 0.4679 - val_acc: 0.7750\n",
      "Epoch 199/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1691 - acc: 0.9437 - val_loss: 0.4655 - val_acc: 0.7750\n",
      "Epoch 200/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.1676 - acc: 0.9437 - val_loss: 0.4646 - val_acc: 0.7750\n",
      "Epoch 201/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.1662 - acc: 0.9500 - val_loss: 0.4629 - val_acc: 0.7750\n",
      "Epoch 202/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.1647 - acc: 0.9500 - val_loss: 0.4597 - val_acc: 0.7750\n",
      "Epoch 203/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.1633 - acc: 0.9500 - val_loss: 0.4593 - val_acc: 0.7750\n",
      "Epoch 204/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.1618 - acc: 0.9500 - val_loss: 0.4557 - val_acc: 0.7750\n",
      "Epoch 205/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1604 - acc: 0.9500 - val_loss: 0.4531 - val_acc: 0.7750\n",
      "Epoch 206/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.1590 - acc: 0.9500 - val_loss: 0.4494 - val_acc: 0.7750\n",
      "Epoch 207/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.1576 - acc: 0.9500 - val_loss: 0.4463 - val_acc: 0.7750\n",
      "Epoch 208/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.1563 - acc: 0.9500 - val_loss: 0.4438 - val_acc: 0.7750\n",
      "Epoch 209/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.1550 - acc: 0.9500 - val_loss: 0.4431 - val_acc: 0.7750\n",
      "Epoch 210/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.1536 - acc: 0.9500 - val_loss: 0.4411 - val_acc: 0.7750\n",
      "Epoch 211/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1524 - acc: 0.9500 - val_loss: 0.4382 - val_acc: 0.7750\n",
      "Epoch 212/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.1512 - acc: 0.9500 - val_loss: 0.4365 - val_acc: 0.7750\n",
      "Epoch 213/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.1499 - acc: 0.9500 - val_loss: 0.4354 - val_acc: 0.7750\n",
      "Epoch 214/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.1486 - acc: 0.9500 - val_loss: 0.4338 - val_acc: 0.7750\n",
      "Epoch 215/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.1474 - acc: 0.9500 - val_loss: 0.4311 - val_acc: 0.7750\n",
      "Epoch 216/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.1463 - acc: 0.9500 - val_loss: 0.4287 - val_acc: 0.7750\n",
      "Epoch 217/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.1451 - acc: 0.9500 - val_loss: 0.4264 - val_acc: 0.7750\n",
      "Epoch 218/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.1440 - acc: 0.9500 - val_loss: 0.4253 - val_acc: 0.7750\n",
      "Epoch 219/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.1429 - acc: 0.9562 - val_loss: 0.4246 - val_acc: 0.7750\n",
      "Epoch 220/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.1419 - acc: 0.9562 - val_loss: 0.4219 - val_acc: 0.7750\n",
      "Epoch 221/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.1406 - acc: 0.9562 - val_loss: 0.4204 - val_acc: 0.7750\n",
      "Epoch 222/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.1396 - acc: 0.9562 - val_loss: 0.4184 - val_acc: 0.7750\n",
      "Epoch 223/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.1386 - acc: 0.9562 - val_loss: 0.4164 - val_acc: 0.7750\n",
      "Epoch 224/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.1376 - acc: 0.9562 - val_loss: 0.4147 - val_acc: 0.7750\n",
      "Epoch 225/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.1364 - acc: 0.9562 - val_loss: 0.4135 - val_acc: 0.7750\n",
      "Epoch 226/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1355 - acc: 0.9562 - val_loss: 0.4117 - val_acc: 0.7750\n",
      "Epoch 227/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.1345 - acc: 0.9562 - val_loss: 0.4102 - val_acc: 0.7750\n",
      "Epoch 228/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.1336 - acc: 0.9562 - val_loss: 0.4085 - val_acc: 0.7750\n",
      "Epoch 229/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.1328 - acc: 0.9562 - val_loss: 0.4056 - val_acc: 0.7750\n",
      "Epoch 230/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.1319 - acc: 0.9562 - val_loss: 0.4033 - val_acc: 0.7750\n",
      "Epoch 231/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.1309 - acc: 0.9562 - val_loss: 0.4013 - val_acc: 0.7750\n",
      "Epoch 232/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.1300 - acc: 0.9562 - val_loss: 0.4008 - val_acc: 0.7750\n",
      "Epoch 233/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.1291 - acc: 0.9562 - val_loss: 0.3989 - val_acc: 0.7750\n",
      "Epoch 234/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.1283 - acc: 0.9562 - val_loss: 0.3977 - val_acc: 0.7750\n",
      "Epoch 235/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1275 - acc: 0.9562 - val_loss: 0.3967 - val_acc: 0.7750\n",
      "Epoch 236/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.1268 - acc: 0.9562 - val_loss: 0.3935 - val_acc: 0.7750\n",
      "Epoch 237/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.1260 - acc: 0.9562 - val_loss: 0.3917 - val_acc: 0.7750\n",
      "Epoch 238/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.1252 - acc: 0.9562 - val_loss: 0.3914 - val_acc: 0.7750\n",
      "Epoch 239/800\n",
      "160/160 [==============================] - 0s 99us/step - loss: 0.1243 - acc: 0.9562 - val_loss: 0.3891 - val_acc: 0.7750\n",
      "Epoch 240/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.1236 - acc: 0.9562 - val_loss: 0.3862 - val_acc: 0.7750\n",
      "Epoch 241/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 97us/step - loss: 0.1228 - acc: 0.9562 - val_loss: 0.3850 - val_acc: 0.7750\n",
      "Epoch 242/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.1221 - acc: 0.9562 - val_loss: 0.3848 - val_acc: 0.7750\n",
      "Epoch 243/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.1214 - acc: 0.9562 - val_loss: 0.3851 - val_acc: 0.7750\n",
      "Epoch 244/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.1207 - acc: 0.9562 - val_loss: 0.3838 - val_acc: 0.7750\n",
      "Epoch 245/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.1201 - acc: 0.9562 - val_loss: 0.3811 - val_acc: 0.7750\n",
      "Epoch 246/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.1192 - acc: 0.9625 - val_loss: 0.3801 - val_acc: 0.7750\n",
      "Epoch 247/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.1185 - acc: 0.9688 - val_loss: 0.3776 - val_acc: 0.7750\n",
      "Epoch 248/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.1177 - acc: 0.9688 - val_loss: 0.3758 - val_acc: 0.7750\n",
      "Epoch 249/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.1171 - acc: 0.9688 - val_loss: 0.3731 - val_acc: 0.7750\n",
      "Epoch 250/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.1162 - acc: 0.9688 - val_loss: 0.3715 - val_acc: 0.7750\n",
      "Epoch 251/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.1155 - acc: 0.9688 - val_loss: 0.3698 - val_acc: 0.7750\n",
      "Epoch 252/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.1147 - acc: 0.9688 - val_loss: 0.3684 - val_acc: 0.7750\n",
      "Epoch 253/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1141 - acc: 0.9688 - val_loss: 0.3677 - val_acc: 0.7750\n",
      "Epoch 254/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.1133 - acc: 0.9688 - val_loss: 0.3664 - val_acc: 0.7750\n",
      "Epoch 255/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.1126 - acc: 0.9688 - val_loss: 0.3631 - val_acc: 0.7750\n",
      "Epoch 256/800\n",
      "160/160 [==============================] - 0s 105us/step - loss: 0.1118 - acc: 0.9688 - val_loss: 0.3612 - val_acc: 0.7750\n",
      "Epoch 257/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.1113 - acc: 0.9688 - val_loss: 0.3589 - val_acc: 0.7750\n",
      "Epoch 258/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.1104 - acc: 0.9688 - val_loss: 0.3566 - val_acc: 0.7750\n",
      "Epoch 259/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.1097 - acc: 0.9688 - val_loss: 0.3553 - val_acc: 0.7750\n",
      "Epoch 260/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1090 - acc: 0.9688 - val_loss: 0.3538 - val_acc: 0.7750\n",
      "Epoch 261/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.1084 - acc: 0.9688 - val_loss: 0.3528 - val_acc: 0.7750\n",
      "Epoch 262/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.1078 - acc: 0.9688 - val_loss: 0.3511 - val_acc: 0.7750\n",
      "Epoch 263/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.1071 - acc: 0.9688 - val_loss: 0.3487 - val_acc: 0.7750\n",
      "Epoch 264/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.1067 - acc: 0.9688 - val_loss: 0.3468 - val_acc: 0.7750\n",
      "Epoch 265/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.1060 - acc: 0.9688 - val_loss: 0.3452 - val_acc: 0.7750\n",
      "Epoch 266/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.1055 - acc: 0.9688 - val_loss: 0.3462 - val_acc: 0.7750\n",
      "Epoch 267/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.1048 - acc: 0.9688 - val_loss: 0.3439 - val_acc: 0.7750\n",
      "Epoch 268/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1044 - acc: 0.9688 - val_loss: 0.3441 - val_acc: 0.7750\n",
      "Epoch 269/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.1037 - acc: 0.9688 - val_loss: 0.3423 - val_acc: 0.7750\n",
      "Epoch 270/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.1031 - acc: 0.9688 - val_loss: 0.3410 - val_acc: 0.7750\n",
      "Epoch 271/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.1026 - acc: 0.9688 - val_loss: 0.3395 - val_acc: 0.7750\n",
      "Epoch 272/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.1021 - acc: 0.9688 - val_loss: 0.3380 - val_acc: 0.7750\n",
      "Epoch 273/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.1015 - acc: 0.9688 - val_loss: 0.3361 - val_acc: 0.8000\n",
      "Epoch 274/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.1010 - acc: 0.9688 - val_loss: 0.3339 - val_acc: 0.8000\n",
      "Epoch 275/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.1004 - acc: 0.9688 - val_loss: 0.3329 - val_acc: 0.8000\n",
      "Epoch 276/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0999 - acc: 0.9688 - val_loss: 0.3321 - val_acc: 0.8000\n",
      "Epoch 277/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0994 - acc: 0.9688 - val_loss: 0.3317 - val_acc: 0.8000\n",
      "Epoch 278/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0989 - acc: 0.9688 - val_loss: 0.3293 - val_acc: 0.8000\n",
      "Epoch 279/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0983 - acc: 0.9688 - val_loss: 0.3279 - val_acc: 0.8000\n",
      "Epoch 280/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0978 - acc: 0.9688 - val_loss: 0.3268 - val_acc: 0.8000\n",
      "Epoch 281/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0973 - acc: 0.9688 - val_loss: 0.3253 - val_acc: 0.8000\n",
      "Epoch 282/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0968 - acc: 0.9688 - val_loss: 0.3249 - val_acc: 0.8000\n",
      "Epoch 283/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0964 - acc: 0.9688 - val_loss: 0.3242 - val_acc: 0.8000\n",
      "Epoch 284/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0959 - acc: 0.9688 - val_loss: 0.3224 - val_acc: 0.8000\n",
      "Epoch 285/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0954 - acc: 0.9688 - val_loss: 0.3221 - val_acc: 0.8000\n",
      "Epoch 286/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0949 - acc: 0.9688 - val_loss: 0.3200 - val_acc: 0.8000\n",
      "Epoch 287/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0945 - acc: 0.9688 - val_loss: 0.3184 - val_acc: 0.8000\n",
      "Epoch 288/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0940 - acc: 0.9688 - val_loss: 0.3171 - val_acc: 0.8000\n",
      "Epoch 289/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0936 - acc: 0.9688 - val_loss: 0.3156 - val_acc: 0.8000\n",
      "Epoch 290/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0931 - acc: 0.9688 - val_loss: 0.3146 - val_acc: 0.8000\n",
      "Epoch 291/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0928 - acc: 0.9688 - val_loss: 0.3123 - val_acc: 0.8000\n",
      "Epoch 292/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0923 - acc: 0.9688 - val_loss: 0.3114 - val_acc: 0.8000\n",
      "Epoch 293/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0918 - acc: 0.9688 - val_loss: 0.3099 - val_acc: 0.8000\n",
      "Epoch 294/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0915 - acc: 0.9688 - val_loss: 0.3100 - val_acc: 0.8000\n",
      "Epoch 295/800\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.0910 - acc: 0.9688 - val_loss: 0.3083 - val_acc: 0.8000\n",
      "Epoch 296/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0905 - acc: 0.9688 - val_loss: 0.3071 - val_acc: 0.8000\n",
      "Epoch 297/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0901 - acc: 0.9688 - val_loss: 0.3057 - val_acc: 0.8000\n",
      "Epoch 298/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0897 - acc: 0.9688 - val_loss: 0.3056 - val_acc: 0.8000\n",
      "Epoch 299/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0892 - acc: 0.9688 - val_loss: 0.3047 - val_acc: 0.8000\n",
      "Epoch 300/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0888 - acc: 0.9688 - val_loss: 0.3034 - val_acc: 0.8000\n",
      "Epoch 301/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 87us/step - loss: 0.0884 - acc: 0.9688 - val_loss: 0.3019 - val_acc: 0.8000\n",
      "Epoch 302/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0880 - acc: 0.9688 - val_loss: 0.3010 - val_acc: 0.8000\n",
      "Epoch 303/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0875 - acc: 0.9688 - val_loss: 0.2988 - val_acc: 0.8000\n",
      "Epoch 304/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0870 - acc: 0.9688 - val_loss: 0.2969 - val_acc: 0.8000\n",
      "Epoch 305/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0864 - acc: 0.9688 - val_loss: 0.2953 - val_acc: 0.8250\n",
      "Epoch 306/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0860 - acc: 0.9688 - val_loss: 0.2924 - val_acc: 0.8250\n",
      "Epoch 307/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0854 - acc: 0.9688 - val_loss: 0.2910 - val_acc: 0.8250\n",
      "Epoch 308/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0849 - acc: 0.9688 - val_loss: 0.2890 - val_acc: 0.8250\n",
      "Epoch 309/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0843 - acc: 0.9688 - val_loss: 0.2866 - val_acc: 0.8500\n",
      "Epoch 310/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0838 - acc: 0.9688 - val_loss: 0.2854 - val_acc: 0.8500\n",
      "Epoch 311/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0834 - acc: 0.9688 - val_loss: 0.2832 - val_acc: 0.8500\n",
      "Epoch 312/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0828 - acc: 0.9688 - val_loss: 0.2812 - val_acc: 0.8500\n",
      "Epoch 313/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0819 - acc: 0.9688 - val_loss: 0.2783 - val_acc: 0.8500\n",
      "Epoch 314/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0811 - acc: 0.9688 - val_loss: 0.2764 - val_acc: 0.8500\n",
      "Epoch 315/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0806 - acc: 0.9688 - val_loss: 0.2719 - val_acc: 0.8500\n",
      "Epoch 316/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0795 - acc: 0.9688 - val_loss: 0.2682 - val_acc: 0.8500\n",
      "Epoch 317/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0787 - acc: 0.9688 - val_loss: 0.2658 - val_acc: 0.8500\n",
      "Epoch 318/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0779 - acc: 0.9688 - val_loss: 0.2633 - val_acc: 0.8500\n",
      "Epoch 319/800\n",
      "160/160 [==============================] - 0s 106us/step - loss: 0.0771 - acc: 0.9688 - val_loss: 0.2612 - val_acc: 0.8500\n",
      "Epoch 320/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0765 - acc: 0.9688 - val_loss: 0.2583 - val_acc: 0.8500\n",
      "Epoch 321/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0756 - acc: 0.9688 - val_loss: 0.2562 - val_acc: 0.8500\n",
      "Epoch 322/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0752 - acc: 0.9688 - val_loss: 0.2536 - val_acc: 0.8500\n",
      "Epoch 323/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0744 - acc: 0.9688 - val_loss: 0.2504 - val_acc: 0.8500\n",
      "Epoch 324/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0736 - acc: 0.9688 - val_loss: 0.2482 - val_acc: 0.8500\n",
      "Epoch 325/800\n",
      "160/160 [==============================] - 0s 108us/step - loss: 0.0729 - acc: 0.9688 - val_loss: 0.2469 - val_acc: 0.8500\n",
      "Epoch 326/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0724 - acc: 0.9688 - val_loss: 0.2450 - val_acc: 0.8500\n",
      "Epoch 327/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0717 - acc: 0.9688 - val_loss: 0.2430 - val_acc: 0.8500\n",
      "Epoch 328/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0712 - acc: 0.9688 - val_loss: 0.2413 - val_acc: 0.8500\n",
      "Epoch 329/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0704 - acc: 0.9688 - val_loss: 0.2391 - val_acc: 0.8500\n",
      "Epoch 330/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0698 - acc: 0.9688 - val_loss: 0.2370 - val_acc: 0.8500\n",
      "Epoch 331/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0694 - acc: 0.9688 - val_loss: 0.2347 - val_acc: 0.8500\n",
      "Epoch 332/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0688 - acc: 0.9688 - val_loss: 0.2325 - val_acc: 0.8500\n",
      "Epoch 333/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0681 - acc: 0.9750 - val_loss: 0.2312 - val_acc: 0.8500\n",
      "Epoch 334/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0676 - acc: 0.9750 - val_loss: 0.2294 - val_acc: 0.8500\n",
      "Epoch 335/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0671 - acc: 0.9750 - val_loss: 0.2282 - val_acc: 0.8500\n",
      "Epoch 336/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0665 - acc: 0.9750 - val_loss: 0.2271 - val_acc: 0.8750\n",
      "Epoch 337/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0661 - acc: 0.9750 - val_loss: 0.2270 - val_acc: 0.8750\n",
      "Epoch 338/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0656 - acc: 0.9750 - val_loss: 0.2246 - val_acc: 0.8750\n",
      "Epoch 339/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0652 - acc: 0.9750 - val_loss: 0.2220 - val_acc: 0.8750\n",
      "Epoch 340/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0644 - acc: 0.9750 - val_loss: 0.2215 - val_acc: 0.8750\n",
      "Epoch 341/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0640 - acc: 0.9750 - val_loss: 0.2203 - val_acc: 0.8750\n",
      "Epoch 342/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0635 - acc: 0.9750 - val_loss: 0.2192 - val_acc: 0.8750\n",
      "Epoch 343/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0631 - acc: 0.9750 - val_loss: 0.2184 - val_acc: 0.8750\n",
      "Epoch 344/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0626 - acc: 0.9750 - val_loss: 0.2176 - val_acc: 0.8750\n",
      "Epoch 345/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.0622 - acc: 0.9750 - val_loss: 0.2166 - val_acc: 0.8750\n",
      "Epoch 346/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0617 - acc: 0.9750 - val_loss: 0.2147 - val_acc: 0.8750\n",
      "Epoch 347/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0613 - acc: 0.9750 - val_loss: 0.2128 - val_acc: 0.8750\n",
      "Epoch 348/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0608 - acc: 0.9750 - val_loss: 0.2119 - val_acc: 0.8750\n",
      "Epoch 349/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0604 - acc: 0.9750 - val_loss: 0.2109 - val_acc: 0.8750\n",
      "Epoch 350/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0600 - acc: 0.9750 - val_loss: 0.2096 - val_acc: 0.8750\n",
      "Epoch 351/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0596 - acc: 0.9750 - val_loss: 0.2083 - val_acc: 0.8750\n",
      "Epoch 352/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0592 - acc: 0.9750 - val_loss: 0.2073 - val_acc: 0.8750\n",
      "Epoch 353/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0588 - acc: 0.9813 - val_loss: 0.2063 - val_acc: 0.8750\n",
      "Epoch 354/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0585 - acc: 0.9813 - val_loss: 0.2046 - val_acc: 0.8750\n",
      "Epoch 355/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0581 - acc: 0.9813 - val_loss: 0.2045 - val_acc: 0.8750\n",
      "Epoch 356/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0577 - acc: 0.9813 - val_loss: 0.2046 - val_acc: 0.8750\n",
      "Epoch 357/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0573 - acc: 0.9813 - val_loss: 0.2039 - val_acc: 0.8750\n",
      "Epoch 358/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0569 - acc: 0.9813 - val_loss: 0.2030 - val_acc: 0.9000\n",
      "Epoch 359/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0566 - acc: 0.9813 - val_loss: 0.2025 - val_acc: 0.8750\n",
      "Epoch 360/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0562 - acc: 0.9813 - val_loss: 0.2013 - val_acc: 0.9000\n",
      "Epoch 361/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 102us/step - loss: 0.0558 - acc: 0.9813 - val_loss: 0.2004 - val_acc: 0.9000\n",
      "Epoch 362/800\n",
      "160/160 [==============================] - 0s 111us/step - loss: 0.0556 - acc: 0.9813 - val_loss: 0.1988 - val_acc: 0.9000\n",
      "Epoch 363/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0553 - acc: 0.9813 - val_loss: 0.1983 - val_acc: 0.9000\n",
      "Epoch 364/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0549 - acc: 0.9813 - val_loss: 0.1982 - val_acc: 0.9000\n",
      "Epoch 365/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0546 - acc: 0.9813 - val_loss: 0.1975 - val_acc: 0.9000\n",
      "Epoch 366/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0543 - acc: 0.9813 - val_loss: 0.1963 - val_acc: 0.9000\n",
      "Epoch 367/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0540 - acc: 0.9813 - val_loss: 0.1959 - val_acc: 0.9000\n",
      "Epoch 368/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0537 - acc: 0.9813 - val_loss: 0.1952 - val_acc: 0.9000\n",
      "Epoch 369/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0535 - acc: 0.9813 - val_loss: 0.1936 - val_acc: 0.9000\n",
      "Epoch 370/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0532 - acc: 0.9875 - val_loss: 0.1933 - val_acc: 0.9000\n",
      "Epoch 371/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0528 - acc: 0.9875 - val_loss: 0.1928 - val_acc: 0.9000\n",
      "Epoch 372/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0526 - acc: 0.9875 - val_loss: 0.1924 - val_acc: 0.9000\n",
      "Epoch 373/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0524 - acc: 0.9875 - val_loss: 0.1923 - val_acc: 0.9000\n",
      "Epoch 374/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0522 - acc: 0.9875 - val_loss: 0.1919 - val_acc: 0.9000\n",
      "Epoch 375/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0518 - acc: 0.9875 - val_loss: 0.1905 - val_acc: 0.9000\n",
      "Epoch 376/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0515 - acc: 0.9875 - val_loss: 0.1901 - val_acc: 0.9000\n",
      "Epoch 377/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0513 - acc: 0.9875 - val_loss: 0.1889 - val_acc: 0.9000\n",
      "Epoch 378/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0510 - acc: 0.9875 - val_loss: 0.1888 - val_acc: 0.9000\n",
      "Epoch 379/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0508 - acc: 0.9875 - val_loss: 0.1876 - val_acc: 0.9000\n",
      "Epoch 380/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0505 - acc: 0.9875 - val_loss: 0.1867 - val_acc: 0.9000\n",
      "Epoch 381/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0503 - acc: 0.9875 - val_loss: 0.1858 - val_acc: 0.9000\n",
      "Epoch 382/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0501 - acc: 0.9875 - val_loss: 0.1860 - val_acc: 0.9000\n",
      "Epoch 383/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0498 - acc: 0.9875 - val_loss: 0.1858 - val_acc: 0.9000\n",
      "Epoch 384/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0495 - acc: 0.9875 - val_loss: 0.1854 - val_acc: 0.9000\n",
      "Epoch 385/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0493 - acc: 0.9875 - val_loss: 0.1851 - val_acc: 0.9000\n",
      "Epoch 386/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0491 - acc: 0.9875 - val_loss: 0.1845 - val_acc: 0.9000\n",
      "Epoch 387/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0488 - acc: 0.9875 - val_loss: 0.1840 - val_acc: 0.9000\n",
      "Epoch 388/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0486 - acc: 0.9875 - val_loss: 0.1831 - val_acc: 0.9000\n",
      "Epoch 389/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0484 - acc: 0.9875 - val_loss: 0.1820 - val_acc: 0.9000\n",
      "Epoch 390/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0482 - acc: 0.9875 - val_loss: 0.1822 - val_acc: 0.9000\n",
      "Epoch 391/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0479 - acc: 0.9875 - val_loss: 0.1810 - val_acc: 0.9000\n",
      "Epoch 392/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0478 - acc: 0.9875 - val_loss: 0.1805 - val_acc: 0.9000\n",
      "Epoch 393/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0475 - acc: 0.9875 - val_loss: 0.1796 - val_acc: 0.9000\n",
      "Epoch 394/800\n",
      "160/160 [==============================] - 0s 132us/step - loss: 0.0473 - acc: 0.9875 - val_loss: 0.1793 - val_acc: 0.9000\n",
      "Epoch 395/800\n",
      "160/160 [==============================] - 0s 129us/step - loss: 0.0469 - acc: 0.9875 - val_loss: 0.1790 - val_acc: 0.9000\n",
      "Epoch 396/800\n",
      "160/160 [==============================] - 0s 118us/step - loss: 0.0468 - acc: 0.9875 - val_loss: 0.1779 - val_acc: 0.9000\n",
      "Epoch 397/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0466 - acc: 0.9875 - val_loss: 0.1767 - val_acc: 0.9000\n",
      "Epoch 398/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0463 - acc: 0.9875 - val_loss: 0.1765 - val_acc: 0.9000\n",
      "Epoch 399/800\n",
      "160/160 [==============================] - 0s 113us/step - loss: 0.0463 - acc: 0.9875 - val_loss: 0.1750 - val_acc: 0.9000\n",
      "Epoch 400/800\n",
      "160/160 [==============================] - 0s 104us/step - loss: 0.0459 - acc: 0.9875 - val_loss: 0.1745 - val_acc: 0.9000\n",
      "Epoch 401/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0458 - acc: 0.9875 - val_loss: 0.1751 - val_acc: 0.9000\n",
      "Epoch 402/800\n",
      "160/160 [==============================] - 0s 104us/step - loss: 0.0455 - acc: 0.9875 - val_loss: 0.1745 - val_acc: 0.9000\n",
      "Epoch 403/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0453 - acc: 0.9875 - val_loss: 0.1746 - val_acc: 0.9000\n",
      "Epoch 404/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0452 - acc: 0.9875 - val_loss: 0.1734 - val_acc: 0.9000\n",
      "Epoch 405/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0449 - acc: 0.9875 - val_loss: 0.1729 - val_acc: 0.9000\n",
      "Epoch 406/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0447 - acc: 0.9875 - val_loss: 0.1729 - val_acc: 0.9000\n",
      "Epoch 407/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0445 - acc: 0.9875 - val_loss: 0.1720 - val_acc: 0.9000\n",
      "Epoch 408/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0443 - acc: 0.9875 - val_loss: 0.1710 - val_acc: 0.9000\n",
      "Epoch 409/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0441 - acc: 0.9938 - val_loss: 0.1700 - val_acc: 0.9000\n",
      "Epoch 410/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0440 - acc: 0.9938 - val_loss: 0.1692 - val_acc: 0.9000\n",
      "Epoch 411/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0437 - acc: 0.9938 - val_loss: 0.1690 - val_acc: 0.9000\n",
      "Epoch 412/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0435 - acc: 0.9938 - val_loss: 0.1684 - val_acc: 0.9000\n",
      "Epoch 413/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0433 - acc: 0.9938 - val_loss: 0.1681 - val_acc: 0.9000\n",
      "Epoch 414/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0431 - acc: 0.9938 - val_loss: 0.1674 - val_acc: 0.9000\n",
      "Epoch 415/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.0429 - acc: 0.9938 - val_loss: 0.1670 - val_acc: 0.9000\n",
      "Epoch 416/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0427 - acc: 0.9938 - val_loss: 0.1670 - val_acc: 0.9000\n",
      "Epoch 417/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0426 - acc: 0.9938 - val_loss: 0.1669 - val_acc: 0.9000\n",
      "Epoch 418/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0424 - acc: 0.9938 - val_loss: 0.1659 - val_acc: 0.9000\n",
      "Epoch 419/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0422 - acc: 0.9938 - val_loss: 0.1649 - val_acc: 0.9000\n",
      "Epoch 420/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0421 - acc: 0.9938 - val_loss: 0.1641 - val_acc: 0.9000\n",
      "Epoch 421/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 95us/step - loss: 0.0419 - acc: 0.9938 - val_loss: 0.1639 - val_acc: 0.9000\n",
      "Epoch 422/800\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.0417 - acc: 0.9938 - val_loss: 0.1639 - val_acc: 0.9000\n",
      "Epoch 423/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0415 - acc: 0.9938 - val_loss: 0.1637 - val_acc: 0.9000\n",
      "Epoch 424/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0413 - acc: 0.9938 - val_loss: 0.1634 - val_acc: 0.9000\n",
      "Epoch 425/800\n",
      "160/160 [==============================] - 0s 99us/step - loss: 0.0411 - acc: 0.9938 - val_loss: 0.1625 - val_acc: 0.9000\n",
      "Epoch 426/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0410 - acc: 0.9938 - val_loss: 0.1618 - val_acc: 0.9000\n",
      "Epoch 427/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0408 - acc: 0.9938 - val_loss: 0.1608 - val_acc: 0.9000\n",
      "Epoch 428/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0406 - acc: 0.9938 - val_loss: 0.1605 - val_acc: 0.9000\n",
      "Epoch 429/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0404 - acc: 0.9938 - val_loss: 0.1598 - val_acc: 0.9000\n",
      "Epoch 430/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0403 - acc: 0.9938 - val_loss: 0.1591 - val_acc: 0.9000\n",
      "Epoch 431/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0401 - acc: 0.9938 - val_loss: 0.1588 - val_acc: 0.9000\n",
      "Epoch 432/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0400 - acc: 0.9938 - val_loss: 0.1581 - val_acc: 0.9000\n",
      "Epoch 433/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0398 - acc: 0.9938 - val_loss: 0.1581 - val_acc: 0.9000\n",
      "Epoch 434/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0396 - acc: 0.9938 - val_loss: 0.1580 - val_acc: 0.9000\n",
      "Epoch 435/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0396 - acc: 0.9938 - val_loss: 0.1580 - val_acc: 0.9000\n",
      "Epoch 436/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0393 - acc: 0.9938 - val_loss: 0.1571 - val_acc: 0.9000\n",
      "Epoch 437/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0391 - acc: 0.9938 - val_loss: 0.1563 - val_acc: 0.9000\n",
      "Epoch 438/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0391 - acc: 0.9938 - val_loss: 0.1552 - val_acc: 0.9000\n",
      "Epoch 439/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0388 - acc: 0.9938 - val_loss: 0.1549 - val_acc: 0.9000\n",
      "Epoch 440/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0387 - acc: 0.9938 - val_loss: 0.1545 - val_acc: 0.9000\n",
      "Epoch 441/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0385 - acc: 0.9938 - val_loss: 0.1540 - val_acc: 0.9000\n",
      "Epoch 442/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0383 - acc: 0.9938 - val_loss: 0.1539 - val_acc: 0.9000\n",
      "Epoch 443/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0382 - acc: 0.9938 - val_loss: 0.1535 - val_acc: 0.9000\n",
      "Epoch 444/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0381 - acc: 0.9938 - val_loss: 0.1526 - val_acc: 0.9000\n",
      "Epoch 445/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0379 - acc: 0.9938 - val_loss: 0.1523 - val_acc: 0.9000\n",
      "Epoch 446/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0377 - acc: 0.9938 - val_loss: 0.1515 - val_acc: 0.9000\n",
      "Epoch 447/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0376 - acc: 0.9938 - val_loss: 0.1509 - val_acc: 0.9000\n",
      "Epoch 448/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0375 - acc: 0.9938 - val_loss: 0.1511 - val_acc: 0.9000\n",
      "Epoch 449/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0374 - acc: 0.9938 - val_loss: 0.1512 - val_acc: 0.9000\n",
      "Epoch 450/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0371 - acc: 0.9938 - val_loss: 0.1506 - val_acc: 0.9000\n",
      "Epoch 451/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0370 - acc: 0.9938 - val_loss: 0.1498 - val_acc: 0.9000\n",
      "Epoch 452/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0369 - acc: 0.9938 - val_loss: 0.1488 - val_acc: 0.9000\n",
      "Epoch 453/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0367 - acc: 0.9938 - val_loss: 0.1484 - val_acc: 0.9000\n",
      "Epoch 454/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0365 - acc: 0.9938 - val_loss: 0.1479 - val_acc: 0.9000\n",
      "Epoch 455/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0364 - acc: 0.9938 - val_loss: 0.1477 - val_acc: 0.9000\n",
      "Epoch 456/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0364 - acc: 0.9938 - val_loss: 0.1481 - val_acc: 0.9000\n",
      "Epoch 457/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0361 - acc: 0.9938 - val_loss: 0.1469 - val_acc: 0.9000\n",
      "Epoch 458/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0359 - acc: 0.9938 - val_loss: 0.1463 - val_acc: 0.9000\n",
      "Epoch 459/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0359 - acc: 0.9938 - val_loss: 0.1460 - val_acc: 0.9000\n",
      "Epoch 460/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0357 - acc: 0.9938 - val_loss: 0.1454 - val_acc: 0.9000\n",
      "Epoch 461/800\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.0107 - acc: 1.000 - 0s 78us/step - loss: 0.0355 - acc: 0.9938 - val_loss: 0.1449 - val_acc: 0.9000\n",
      "Epoch 462/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0354 - acc: 0.9938 - val_loss: 0.1446 - val_acc: 0.9000\n",
      "Epoch 463/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0353 - acc: 0.9938 - val_loss: 0.1443 - val_acc: 0.9000\n",
      "Epoch 464/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0351 - acc: 0.9938 - val_loss: 0.1443 - val_acc: 0.9000\n",
      "Epoch 465/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0350 - acc: 0.9938 - val_loss: 0.1440 - val_acc: 0.9000\n",
      "Epoch 466/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0349 - acc: 0.9938 - val_loss: 0.1428 - val_acc: 0.9000\n",
      "Epoch 467/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0348 - acc: 0.9938 - val_loss: 0.1425 - val_acc: 0.9000\n",
      "Epoch 468/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0346 - acc: 0.9938 - val_loss: 0.1419 - val_acc: 0.9000\n",
      "Epoch 469/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0345 - acc: 0.9938 - val_loss: 0.1414 - val_acc: 0.9000\n",
      "Epoch 470/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0344 - acc: 0.9938 - val_loss: 0.1403 - val_acc: 0.9000\n",
      "Epoch 471/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0342 - acc: 0.9938 - val_loss: 0.1402 - val_acc: 0.9000\n",
      "Epoch 472/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0340 - acc: 0.9938 - val_loss: 0.1400 - val_acc: 0.9000\n",
      "Epoch 473/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0340 - acc: 0.9938 - val_loss: 0.1390 - val_acc: 0.9000\n",
      "Epoch 474/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0339 - acc: 0.9938 - val_loss: 0.1379 - val_acc: 0.9000\n",
      "Epoch 475/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0337 - acc: 0.9938 - val_loss: 0.1378 - val_acc: 0.9000\n",
      "Epoch 476/800\n",
      "160/160 [==============================] - 0s 99us/step - loss: 0.0336 - acc: 0.9938 - val_loss: 0.1382 - val_acc: 0.9000\n",
      "Epoch 477/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0334 - acc: 0.9938 - val_loss: 0.1379 - val_acc: 0.9000\n",
      "Epoch 478/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0333 - acc: 0.9938 - val_loss: 0.1376 - val_acc: 0.9000\n",
      "Epoch 479/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0332 - acc: 0.9938 - val_loss: 0.1371 - val_acc: 0.9000\n",
      "Epoch 480/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0331 - acc: 0.9938 - val_loss: 0.1359 - val_acc: 0.9000\n",
      "Epoch 481/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 79us/step - loss: 0.0329 - acc: 0.9938 - val_loss: 0.1352 - val_acc: 0.9000\n",
      "Epoch 482/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0327 - acc: 0.9938 - val_loss: 0.1347 - val_acc: 0.9000\n",
      "Epoch 483/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0327 - acc: 0.9938 - val_loss: 0.1346 - val_acc: 0.9000\n",
      "Epoch 484/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0325 - acc: 0.9938 - val_loss: 0.1338 - val_acc: 0.9000\n",
      "Epoch 485/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0326 - acc: 0.9938 - val_loss: 0.1338 - val_acc: 0.9000\n",
      "Epoch 486/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0323 - acc: 0.9938 - val_loss: 0.1332 - val_acc: 0.9000\n",
      "Epoch 487/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0322 - acc: 0.9938 - val_loss: 0.1329 - val_acc: 0.9000\n",
      "Epoch 488/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0321 - acc: 0.9938 - val_loss: 0.1320 - val_acc: 0.9000\n",
      "Epoch 489/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0319 - acc: 0.9938 - val_loss: 0.1317 - val_acc: 0.9000\n",
      "Epoch 490/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0319 - acc: 0.9938 - val_loss: 0.1317 - val_acc: 0.9000\n",
      "Epoch 491/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0318 - acc: 0.9938 - val_loss: 0.1310 - val_acc: 0.9000\n",
      "Epoch 492/800\n",
      "160/160 [==============================] - 0s 108us/step - loss: 0.0316 - acc: 0.9938 - val_loss: 0.1302 - val_acc: 0.9000\n",
      "Epoch 493/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0315 - acc: 0.9938 - val_loss: 0.1297 - val_acc: 0.9000\n",
      "Epoch 494/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0314 - acc: 0.9938 - val_loss: 0.1292 - val_acc: 0.9000\n",
      "Epoch 495/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0313 - acc: 0.9938 - val_loss: 0.1288 - val_acc: 0.9000\n",
      "Epoch 496/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0312 - acc: 0.9938 - val_loss: 0.1286 - val_acc: 0.9000\n",
      "Epoch 497/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0310 - acc: 0.9938 - val_loss: 0.1281 - val_acc: 0.9000\n",
      "Epoch 498/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0309 - acc: 0.9938 - val_loss: 0.1275 - val_acc: 0.9000\n",
      "Epoch 499/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0308 - acc: 0.9938 - val_loss: 0.1270 - val_acc: 0.9000\n",
      "Epoch 500/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0307 - acc: 0.9938 - val_loss: 0.1268 - val_acc: 0.9000\n",
      "Epoch 501/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0306 - acc: 0.9938 - val_loss: 0.1267 - val_acc: 0.9000\n",
      "Epoch 502/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0304 - acc: 0.9938 - val_loss: 0.1261 - val_acc: 0.9000\n",
      "Epoch 503/800\n",
      "160/160 [==============================] - 0s 108us/step - loss: 0.0304 - acc: 0.9938 - val_loss: 0.1260 - val_acc: 0.9000\n",
      "Epoch 504/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0303 - acc: 0.9938 - val_loss: 0.1249 - val_acc: 0.9000\n",
      "Epoch 505/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0301 - acc: 0.9938 - val_loss: 0.1247 - val_acc: 0.9000\n",
      "Epoch 506/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0300 - acc: 0.9938 - val_loss: 0.1243 - val_acc: 0.9000\n",
      "Epoch 507/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0300 - acc: 0.9938 - val_loss: 0.1235 - val_acc: 0.9000\n",
      "Epoch 508/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0299 - acc: 0.9938 - val_loss: 0.1236 - val_acc: 0.9000\n",
      "Epoch 509/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0297 - acc: 0.9938 - val_loss: 0.1230 - val_acc: 0.9000\n",
      "Epoch 510/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0297 - acc: 0.9938 - val_loss: 0.1224 - val_acc: 0.9000\n",
      "Epoch 511/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0295 - acc: 0.9938 - val_loss: 0.1222 - val_acc: 0.9000\n",
      "Epoch 512/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0294 - acc: 0.9938 - val_loss: 0.1218 - val_acc: 0.9000\n",
      "Epoch 513/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0293 - acc: 0.9938 - val_loss: 0.1212 - val_acc: 0.9000\n",
      "Epoch 514/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0292 - acc: 0.9938 - val_loss: 0.1210 - val_acc: 0.9000\n",
      "Epoch 515/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0291 - acc: 0.9938 - val_loss: 0.1208 - val_acc: 0.9000\n",
      "Epoch 516/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0290 - acc: 0.9938 - val_loss: 0.1206 - val_acc: 0.9000\n",
      "Epoch 517/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0289 - acc: 0.9938 - val_loss: 0.1201 - val_acc: 0.9000\n",
      "Epoch 518/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0288 - acc: 0.9938 - val_loss: 0.1198 - val_acc: 0.9000\n",
      "Epoch 519/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0288 - acc: 0.9938 - val_loss: 0.1189 - val_acc: 0.9000\n",
      "Epoch 520/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0287 - acc: 0.9938 - val_loss: 0.1192 - val_acc: 0.9000\n",
      "Epoch 521/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0285 - acc: 0.9938 - val_loss: 0.1185 - val_acc: 0.9000\n",
      "Epoch 522/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0285 - acc: 0.9938 - val_loss: 0.1175 - val_acc: 0.9000\n",
      "Epoch 523/800\n",
      "160/160 [==============================] - 0s 111us/step - loss: 0.0283 - acc: 0.9938 - val_loss: 0.1167 - val_acc: 0.9000\n",
      "Epoch 524/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0282 - acc: 0.9938 - val_loss: 0.1169 - val_acc: 0.9000\n",
      "Epoch 525/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0281 - acc: 0.9938 - val_loss: 0.1164 - val_acc: 0.9000\n",
      "Epoch 526/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0280 - acc: 0.9938 - val_loss: 0.1163 - val_acc: 0.9000\n",
      "Epoch 527/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0279 - acc: 0.9938 - val_loss: 0.1155 - val_acc: 0.9000\n",
      "Epoch 528/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0278 - acc: 0.9938 - val_loss: 0.1152 - val_acc: 0.9000\n",
      "Epoch 529/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0277 - acc: 0.9938 - val_loss: 0.1146 - val_acc: 0.9000\n",
      "Epoch 530/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0276 - acc: 0.9938 - val_loss: 0.1143 - val_acc: 0.9000\n",
      "Epoch 531/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.0276 - acc: 0.9938 - val_loss: 0.1141 - val_acc: 0.9000\n",
      "Epoch 532/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0275 - acc: 0.9938 - val_loss: 0.1140 - val_acc: 0.9000\n",
      "Epoch 533/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0274 - acc: 0.9938 - val_loss: 0.1137 - val_acc: 0.9000\n",
      "Epoch 534/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0273 - acc: 0.9938 - val_loss: 0.1132 - val_acc: 0.9000\n",
      "Epoch 535/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0272 - acc: 0.9938 - val_loss: 0.1128 - val_acc: 0.9000\n",
      "Epoch 536/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0271 - acc: 0.9938 - val_loss: 0.1126 - val_acc: 0.9000\n",
      "Epoch 537/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0270 - acc: 0.9938 - val_loss: 0.1123 - val_acc: 0.9000\n",
      "Epoch 538/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0269 - acc: 0.9938 - val_loss: 0.1121 - val_acc: 0.9000\n",
      "Epoch 539/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0269 - acc: 0.9938 - val_loss: 0.1112 - val_acc: 0.9250\n",
      "Epoch 540/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0267 - acc: 0.9938 - val_loss: 0.1108 - val_acc: 0.9250\n",
      "Epoch 541/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 65us/step - loss: 0.0266 - acc: 0.9938 - val_loss: 0.1105 - val_acc: 0.9250\n",
      "Epoch 542/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0265 - acc: 0.9938 - val_loss: 0.1101 - val_acc: 0.9250\n",
      "Epoch 543/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0264 - acc: 0.9938 - val_loss: 0.1099 - val_acc: 0.9250\n",
      "Epoch 544/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0264 - acc: 0.9938 - val_loss: 0.1099 - val_acc: 0.9250\n",
      "Epoch 545/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0263 - acc: 0.9938 - val_loss: 0.1097 - val_acc: 0.9250\n",
      "Epoch 546/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0262 - acc: 0.9938 - val_loss: 0.1090 - val_acc: 0.9250\n",
      "Epoch 547/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0260 - acc: 0.9938 - val_loss: 0.1086 - val_acc: 0.9250\n",
      "Epoch 548/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0260 - acc: 0.9938 - val_loss: 0.1078 - val_acc: 0.9250\n",
      "Epoch 549/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0259 - acc: 0.9938 - val_loss: 0.1074 - val_acc: 0.9250\n",
      "Epoch 550/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0259 - acc: 0.9938 - val_loss: 0.1075 - val_acc: 0.9250\n",
      "Epoch 551/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0257 - acc: 0.9938 - val_loss: 0.1071 - val_acc: 0.9250\n",
      "Epoch 552/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0256 - acc: 0.9938 - val_loss: 0.1067 - val_acc: 0.9250\n",
      "Epoch 553/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0256 - acc: 0.9938 - val_loss: 0.1067 - val_acc: 0.9250\n",
      "Epoch 554/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0255 - acc: 0.9938 - val_loss: 0.1062 - val_acc: 0.9250\n",
      "Epoch 555/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0254 - acc: 0.9938 - val_loss: 0.1056 - val_acc: 0.9250\n",
      "Epoch 556/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0254 - acc: 0.9938 - val_loss: 0.1056 - val_acc: 0.9250\n",
      "Epoch 557/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0252 - acc: 0.9938 - val_loss: 0.1053 - val_acc: 0.9250\n",
      "Epoch 558/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0252 - acc: 0.9938 - val_loss: 0.1046 - val_acc: 0.9250\n",
      "Epoch 559/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0250 - acc: 0.9938 - val_loss: 0.1044 - val_acc: 0.9250\n",
      "Epoch 560/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0250 - acc: 0.9938 - val_loss: 0.1040 - val_acc: 0.9250\n",
      "Epoch 561/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0249 - acc: 0.9938 - val_loss: 0.1040 - val_acc: 0.9250\n",
      "Epoch 562/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0248 - acc: 0.9938 - val_loss: 0.1036 - val_acc: 0.9250\n",
      "Epoch 563/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0247 - acc: 0.9938 - val_loss: 0.1030 - val_acc: 0.9250\n",
      "Epoch 564/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0247 - acc: 0.9938 - val_loss: 0.1027 - val_acc: 0.9250\n",
      "Epoch 565/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0245 - acc: 0.9938 - val_loss: 0.1022 - val_acc: 0.9250\n",
      "Epoch 566/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0245 - acc: 0.9938 - val_loss: 0.1017 - val_acc: 0.9250\n",
      "Epoch 567/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.0244 - acc: 0.9938 - val_loss: 0.1013 - val_acc: 0.9250\n",
      "Epoch 568/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0243 - acc: 0.9938 - val_loss: 0.1009 - val_acc: 0.9250\n",
      "Epoch 569/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0242 - acc: 0.9938 - val_loss: 0.1006 - val_acc: 0.9250\n",
      "Epoch 570/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0242 - acc: 0.9938 - val_loss: 0.1009 - val_acc: 0.9250\n",
      "Epoch 571/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0240 - acc: 0.9938 - val_loss: 0.1008 - val_acc: 0.9250\n",
      "Epoch 572/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0240 - acc: 0.9938 - val_loss: 0.1003 - val_acc: 0.9250\n",
      "Epoch 573/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0239 - acc: 0.9938 - val_loss: 0.0998 - val_acc: 0.9250\n",
      "Epoch 574/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0238 - acc: 0.9938 - val_loss: 0.0998 - val_acc: 0.9250\n",
      "Epoch 575/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0238 - acc: 0.9938 - val_loss: 0.0992 - val_acc: 0.9250\n",
      "Epoch 576/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0237 - acc: 0.9938 - val_loss: 0.0991 - val_acc: 0.9250\n",
      "Epoch 577/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0236 - acc: 0.9938 - val_loss: 0.0987 - val_acc: 0.9250\n",
      "Epoch 578/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0235 - acc: 0.9938 - val_loss: 0.0983 - val_acc: 0.9250\n",
      "Epoch 579/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0234 - acc: 0.9938 - val_loss: 0.0978 - val_acc: 0.9250\n",
      "Epoch 580/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0233 - acc: 0.9938 - val_loss: 0.0976 - val_acc: 0.9250\n",
      "Epoch 581/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0233 - acc: 0.9938 - val_loss: 0.0975 - val_acc: 0.9250\n",
      "Epoch 582/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0232 - acc: 0.9938 - val_loss: 0.0972 - val_acc: 0.9250\n",
      "Epoch 583/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0231 - acc: 0.9938 - val_loss: 0.0967 - val_acc: 0.9250\n",
      "Epoch 584/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0231 - acc: 0.9938 - val_loss: 0.0966 - val_acc: 0.9250\n",
      "Epoch 585/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0231 - acc: 0.9938 - val_loss: 0.0960 - val_acc: 0.9250\n",
      "Epoch 586/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0230 - acc: 0.9938 - val_loss: 0.0959 - val_acc: 0.9250\n",
      "Epoch 587/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0228 - acc: 0.9938 - val_loss: 0.0958 - val_acc: 0.9250\n",
      "Epoch 588/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0228 - acc: 0.9938 - val_loss: 0.0953 - val_acc: 0.9250\n",
      "Epoch 589/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0227 - acc: 0.9938 - val_loss: 0.0949 - val_acc: 0.9250\n",
      "Epoch 590/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0226 - acc: 0.9938 - val_loss: 0.0948 - val_acc: 0.9250\n",
      "Epoch 591/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0226 - acc: 0.9938 - val_loss: 0.0948 - val_acc: 0.9250\n",
      "Epoch 592/800\n",
      "160/160 [==============================] - 0s 105us/step - loss: 0.0225 - acc: 0.9938 - val_loss: 0.0941 - val_acc: 0.9250\n",
      "Epoch 593/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0224 - acc: 0.9938 - val_loss: 0.0940 - val_acc: 0.9250\n",
      "Epoch 594/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0224 - acc: 0.9938 - val_loss: 0.0934 - val_acc: 0.9250\n",
      "Epoch 595/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0222 - acc: 0.9938 - val_loss: 0.0931 - val_acc: 0.9250\n",
      "Epoch 596/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0222 - acc: 0.9938 - val_loss: 0.0928 - val_acc: 0.9250\n",
      "Epoch 597/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0221 - acc: 0.9938 - val_loss: 0.0925 - val_acc: 0.9250\n",
      "Epoch 598/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0220 - acc: 0.9938 - val_loss: 0.0923 - val_acc: 0.9250\n",
      "Epoch 599/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0220 - acc: 0.9938 - val_loss: 0.0917 - val_acc: 0.9250\n",
      "Epoch 600/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0219 - acc: 0.9938 - val_loss: 0.0918 - val_acc: 0.9250\n",
      "Epoch 601/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 80us/step - loss: 0.0218 - acc: 0.9938 - val_loss: 0.0913 - val_acc: 0.9250\n",
      "Epoch 602/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0217 - acc: 0.9938 - val_loss: 0.0912 - val_acc: 0.9250\n",
      "Epoch 603/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0216 - acc: 0.9938 - val_loss: 0.0910 - val_acc: 0.9250\n",
      "Epoch 604/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0217 - acc: 0.9938 - val_loss: 0.0902 - val_acc: 0.9250\n",
      "Epoch 605/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0216 - acc: 0.9938 - val_loss: 0.0900 - val_acc: 0.9250\n",
      "Epoch 606/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0214 - acc: 0.9938 - val_loss: 0.0898 - val_acc: 0.9250\n",
      "Epoch 607/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0214 - acc: 0.9938 - val_loss: 0.0899 - val_acc: 0.9250\n",
      "Epoch 608/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0213 - acc: 0.9938 - val_loss: 0.0895 - val_acc: 0.9250\n",
      "Epoch 609/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0213 - acc: 0.9938 - val_loss: 0.0891 - val_acc: 0.9250\n",
      "Epoch 610/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0211 - acc: 0.9938 - val_loss: 0.0891 - val_acc: 0.9250\n",
      "Epoch 611/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0211 - acc: 0.9938 - val_loss: 0.0886 - val_acc: 0.9250\n",
      "Epoch 612/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0210 - acc: 0.9938 - val_loss: 0.0885 - val_acc: 0.9250\n",
      "Epoch 613/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0210 - acc: 0.9938 - val_loss: 0.0882 - val_acc: 1.0000\n",
      "Epoch 614/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0210 - acc: 1.0000 - val_loss: 0.0881 - val_acc: 1.0000\n",
      "Epoch 615/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0209 - acc: 1.0000 - val_loss: 0.0876 - val_acc: 1.0000\n",
      "Epoch 616/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0209 - acc: 1.0000 - val_loss: 0.0869 - val_acc: 1.0000\n",
      "Epoch 617/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0207 - acc: 1.0000 - val_loss: 0.0869 - val_acc: 1.0000\n",
      "Epoch 618/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0206 - acc: 1.0000 - val_loss: 0.0869 - val_acc: 1.0000\n",
      "Epoch 619/800\n",
      "160/160 [==============================] - 0s 107us/step - loss: 0.0206 - acc: 1.0000 - val_loss: 0.0866 - val_acc: 1.0000\n",
      "Epoch 620/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.0862 - val_acc: 1.0000\n",
      "Epoch 621/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0206 - acc: 1.0000 - val_loss: 0.0856 - val_acc: 1.0000\n",
      "Epoch 622/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0204 - acc: 1.0000 - val_loss: 0.0853 - val_acc: 1.0000\n",
      "Epoch 623/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0204 - acc: 1.0000 - val_loss: 0.0855 - val_acc: 1.0000\n",
      "Epoch 624/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0202 - acc: 1.0000 - val_loss: 0.0852 - val_acc: 1.0000\n",
      "Epoch 625/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0202 - acc: 1.0000 - val_loss: 0.0848 - val_acc: 1.0000\n",
      "Epoch 626/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.0846 - val_acc: 1.0000\n",
      "Epoch 627/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.0844 - val_acc: 1.0000\n",
      "Epoch 628/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0200 - acc: 1.0000 - val_loss: 0.0842 - val_acc: 1.0000\n",
      "Epoch 629/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0200 - acc: 1.0000 - val_loss: 0.0838 - val_acc: 1.0000\n",
      "Epoch 630/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0199 - acc: 1.0000 - val_loss: 0.0834 - val_acc: 1.0000\n",
      "Epoch 631/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 1.0000\n",
      "Epoch 632/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 1.0000\n",
      "Epoch 633/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 0.0830 - val_acc: 1.0000\n",
      "Epoch 634/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0196 - acc: 1.0000 - val_loss: 0.0828 - val_acc: 1.0000\n",
      "Epoch 635/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0196 - acc: 1.0000 - val_loss: 0.0827 - val_acc: 1.0000\n",
      "Epoch 636/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.0823 - val_acc: 1.0000\n",
      "Epoch 637/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0194 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 1.0000\n",
      "Epoch 638/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0194 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 1.0000\n",
      "Epoch 639/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.0814 - val_acc: 1.0000\n",
      "Epoch 640/800\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.0190 - acc: 1.000 - 0s 77us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.0810 - val_acc: 1.0000\n",
      "Epoch 641/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0192 - acc: 1.0000 - val_loss: 0.0807 - val_acc: 1.0000\n",
      "Epoch 642/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0191 - acc: 1.0000 - val_loss: 0.0805 - val_acc: 1.0000\n",
      "Epoch 643/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0191 - acc: 1.0000 - val_loss: 0.0804 - val_acc: 1.0000\n",
      "Epoch 644/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.0801 - val_acc: 1.0000\n",
      "Epoch 645/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.0797 - val_acc: 1.0000\n",
      "Epoch 646/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.0796 - val_acc: 1.0000\n",
      "Epoch 647/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.0794 - val_acc: 1.0000\n",
      "Epoch 648/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.0792 - val_acc: 1.0000\n",
      "Epoch 649/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.0785 - val_acc: 1.0000\n",
      "Epoch 650/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.0785 - val_acc: 1.0000\n",
      "Epoch 651/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 1.0000\n",
      "Epoch 652/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.0778 - val_acc: 1.0000\n",
      "Epoch 653/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.0779 - val_acc: 1.0000\n",
      "Epoch 654/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.0776 - val_acc: 1.0000\n",
      "Epoch 655/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.0772 - val_acc: 1.0000\n",
      "Epoch 656/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.0768 - val_acc: 1.0000\n",
      "Epoch 657/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.0768 - val_acc: 1.0000\n",
      "Epoch 658/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.0766 - val_acc: 1.0000\n",
      "Epoch 659/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.0761 - val_acc: 1.0000\n",
      "Epoch 660/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.0760 - val_acc: 1.0000\n",
      "Epoch 661/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 77us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.0760 - val_acc: 1.0000\n",
      "Epoch 662/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.0760 - val_acc: 1.0000\n",
      "Epoch 663/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.0756 - val_acc: 1.0000\n",
      "Epoch 664/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.0752 - val_acc: 1.0000\n",
      "Epoch 665/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.0749 - val_acc: 1.0000\n",
      "Epoch 666/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.0749 - val_acc: 1.0000\n",
      "Epoch 667/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.0747 - val_acc: 1.0000\n",
      "Epoch 668/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.0745 - val_acc: 1.0000\n",
      "Epoch 669/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.0742 - val_acc: 1.0000\n",
      "Epoch 670/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.0739 - val_acc: 1.0000\n",
      "Epoch 671/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.0735 - val_acc: 1.0000\n",
      "Epoch 672/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.0733 - val_acc: 1.0000\n",
      "Epoch 673/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.0734 - val_acc: 1.0000\n",
      "Epoch 674/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.0731 - val_acc: 1.0000\n",
      "Epoch 675/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.0731 - val_acc: 1.0000\n",
      "Epoch 676/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.0731 - val_acc: 1.0000\n",
      "Epoch 677/800\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0727 - val_acc: 1.0000\n",
      "Epoch 678/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0170 - acc: 1.0000 - val_loss: 0.0725 - val_acc: 1.0000\n",
      "Epoch 679/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0723 - val_acc: 1.0000\n",
      "Epoch 680/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 0.0721 - val_acc: 1.0000\n",
      "Epoch 681/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 0.0717 - val_acc: 1.0000\n",
      "Epoch 682/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.0715 - val_acc: 1.0000\n",
      "Epoch 683/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.0713 - val_acc: 1.0000\n",
      "Epoch 684/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0167 - acc: 1.0000 - val_loss: 0.0711 - val_acc: 1.0000\n",
      "Epoch 685/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0167 - acc: 1.0000 - val_loss: 0.0713 - val_acc: 1.0000\n",
      "Epoch 686/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.0712 - val_acc: 1.0000\n",
      "Epoch 687/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.0708 - val_acc: 1.0000\n",
      "Epoch 688/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.0706 - val_acc: 1.0000\n",
      "Epoch 689/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.0704 - val_acc: 1.0000\n",
      "Epoch 690/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.0700 - val_acc: 1.0000\n",
      "Epoch 691/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0164 - acc: 1.0000 - val_loss: 0.0698 - val_acc: 1.0000\n",
      "Epoch 692/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0164 - acc: 1.0000 - val_loss: 0.0699 - val_acc: 1.0000\n",
      "Epoch 693/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.0692 - val_acc: 1.0000\n",
      "Epoch 694/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.0688 - val_acc: 1.0000\n",
      "Epoch 695/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.0688 - val_acc: 1.0000\n",
      "Epoch 696/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.0687 - val_acc: 1.0000\n",
      "Epoch 697/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.0687 - val_acc: 1.0000\n",
      "Epoch 698/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.0687 - val_acc: 1.0000\n",
      "Epoch 699/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.0685 - val_acc: 1.0000\n",
      "Epoch 700/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.0683 - val_acc: 1.0000\n",
      "Epoch 701/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.0680 - val_acc: 1.0000\n",
      "Epoch 702/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.0675 - val_acc: 1.0000\n",
      "Epoch 703/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.0676 - val_acc: 1.0000\n",
      "Epoch 704/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.0676 - val_acc: 1.0000\n",
      "Epoch 705/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.0671 - val_acc: 1.0000\n",
      "Epoch 706/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.0671 - val_acc: 1.0000\n",
      "Epoch 707/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.0671 - val_acc: 1.0000\n",
      "Epoch 708/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0155 - acc: 1.0000 - val_loss: 0.0667 - val_acc: 1.0000\n",
      "Epoch 709/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.0669 - val_acc: 1.0000\n",
      "Epoch 710/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0155 - acc: 1.0000 - val_loss: 0.0665 - val_acc: 1.0000\n",
      "Epoch 711/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0154 - acc: 1.0000 - val_loss: 0.0659 - val_acc: 1.0000\n",
      "Epoch 712/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0154 - acc: 1.0000 - val_loss: 0.0653 - val_acc: 1.0000\n",
      "Epoch 713/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 0.0654 - val_acc: 1.0000\n",
      "Epoch 714/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.0652 - val_acc: 1.0000\n",
      "Epoch 715/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.0651 - val_acc: 1.0000\n",
      "Epoch 716/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.0649 - val_acc: 1.0000\n",
      "Epoch 717/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.0649 - val_acc: 1.0000\n",
      "Epoch 718/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 0.0647 - val_acc: 1.0000\n",
      "Epoch 719/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 0.0645 - val_acc: 1.0000\n",
      "Epoch 720/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0149 - acc: 1.0000 - val_loss: 0.0644 - val_acc: 1.0000\n",
      "Epoch 721/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 67us/step - loss: 0.0149 - acc: 1.0000 - val_loss: 0.0643 - val_acc: 1.0000\n",
      "Epoch 722/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.0642 - val_acc: 1.0000\n",
      "Epoch 723/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.0638 - val_acc: 1.0000\n",
      "Epoch 724/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.0636 - val_acc: 1.0000\n",
      "Epoch 725/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.0634 - val_acc: 1.0000\n",
      "Epoch 726/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.0633 - val_acc: 1.0000\n",
      "Epoch 727/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0146 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 1.0000\n",
      "Epoch 728/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0146 - acc: 1.0000 - val_loss: 0.0631 - val_acc: 1.0000\n",
      "Epoch 729/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 1.0000\n",
      "Epoch 730/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 0.0627 - val_acc: 1.0000\n",
      "Epoch 731/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.0625 - val_acc: 1.0000\n",
      "Epoch 732/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.0625 - val_acc: 1.0000\n",
      "Epoch 733/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.0621 - val_acc: 1.0000\n",
      "Epoch 734/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.0620 - val_acc: 1.0000\n",
      "Epoch 735/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.0616 - val_acc: 1.0000\n",
      "Epoch 736/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.0613 - val_acc: 1.0000\n",
      "Epoch 737/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.0614 - val_acc: 1.0000\n",
      "Epoch 738/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0141 - acc: 1.0000 - val_loss: 0.0612 - val_acc: 1.0000\n",
      "Epoch 739/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0141 - acc: 1.0000 - val_loss: 0.0611 - val_acc: 1.0000\n",
      "Epoch 740/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 0.0608 - val_acc: 1.0000\n",
      "Epoch 741/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 0.0607 - val_acc: 1.0000\n",
      "Epoch 742/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 0.0606 - val_acc: 1.0000\n",
      "Epoch 743/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0139 - acc: 1.0000 - val_loss: 0.0606 - val_acc: 1.0000\n",
      "Epoch 744/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0138 - acc: 1.0000 - val_loss: 0.0605 - val_acc: 1.0000\n",
      "Epoch 745/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0139 - acc: 1.0000 - val_loss: 0.0605 - val_acc: 1.0000\n",
      "Epoch 746/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0138 - acc: 1.0000 - val_loss: 0.0600 - val_acc: 1.0000\n",
      "Epoch 747/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0138 - acc: 1.0000 - val_loss: 0.0596 - val_acc: 1.0000\n",
      "Epoch 748/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0137 - acc: 1.0000 - val_loss: 0.0594 - val_acc: 1.0000\n",
      "Epoch 749/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 0.0591 - val_acc: 1.0000\n",
      "Epoch 750/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 0.0589 - val_acc: 1.0000\n",
      "Epoch 751/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 0.0585 - val_acc: 1.0000\n",
      "Epoch 752/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 0.0586 - val_acc: 1.0000\n",
      "Epoch 753/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 0.0586 - val_acc: 1.0000\n",
      "Epoch 754/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 0.0588 - val_acc: 1.0000\n",
      "Epoch 755/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 0.0585 - val_acc: 1.0000\n",
      "Epoch 756/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.0584 - val_acc: 1.0000\n",
      "Epoch 757/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.0580 - val_acc: 1.0000\n",
      "Epoch 758/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.0578 - val_acc: 1.0000\n",
      "Epoch 759/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.0576 - val_acc: 1.0000\n",
      "Epoch 760/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.0575 - val_acc: 1.0000\n",
      "Epoch 761/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 1.0000\n",
      "Epoch 762/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.0573 - val_acc: 1.0000\n",
      "Epoch 763/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.0573 - val_acc: 1.0000\n",
      "Epoch 764/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.0571 - val_acc: 1.0000\n",
      "Epoch 765/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.0567 - val_acc: 1.0000\n",
      "Epoch 766/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.0562 - val_acc: 1.0000\n",
      "Epoch 767/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.0562 - val_acc: 1.0000\n",
      "Epoch 768/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.0561 - val_acc: 1.0000\n",
      "Epoch 769/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.0558 - val_acc: 1.0000\n",
      "Epoch 770/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.0558 - val_acc: 1.0000\n",
      "Epoch 771/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 0.0559 - val_acc: 1.0000\n",
      "Epoch 772/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.0556 - val_acc: 1.0000\n",
      "Epoch 773/800\n",
      "160/160 [==============================] - 0s 110us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.0557 - val_acc: 1.0000\n",
      "Epoch 774/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.0557 - val_acc: 1.0000\n",
      "Epoch 775/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.0553 - val_acc: 1.0000\n",
      "Epoch 776/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.0549 - val_acc: 1.0000\n",
      "Epoch 777/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 1.0000\n",
      "Epoch 778/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 1.0000\n",
      "Epoch 779/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.0544 - val_acc: 1.0000\n",
      "Epoch 780/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.0544 - val_acc: 1.0000\n",
      "Epoch 781/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 84us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 1.0000\n",
      "Epoch 782/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.0541 - val_acc: 1.0000\n",
      "Epoch 783/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.0539 - val_acc: 1.0000\n",
      "Epoch 784/800\n",
      "160/160 [==============================] - 0s 104us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 0.0540 - val_acc: 1.0000\n",
      "Epoch 785/800\n",
      "160/160 [==============================] - 0s 128us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.0536 - val_acc: 1.0000\n",
      "Epoch 786/800\n",
      "160/160 [==============================] - 0s 112us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.0537 - val_acc: 1.0000\n",
      "Epoch 787/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.0535 - val_acc: 1.0000\n",
      "Epoch 788/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.0531 - val_acc: 1.0000\n",
      "Epoch 789/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.0530 - val_acc: 1.0000\n",
      "Epoch 790/800\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.0083 - acc: 1.000 - 0s 80us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.0529 - val_acc: 1.0000\n",
      "Epoch 791/800\n",
      "160/160 [==============================] - 0s 105us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.0528 - val_acc: 1.0000\n",
      "Epoch 792/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.0528 - val_acc: 1.0000\n",
      "Epoch 793/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.0525 - val_acc: 1.0000\n",
      "Epoch 794/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.0523 - val_acc: 1.0000\n",
      "Epoch 795/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.0523 - val_acc: 1.0000\n",
      "Epoch 796/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.0522 - val_acc: 1.0000\n",
      "Epoch 797/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.0522 - val_acc: 1.0000\n",
      "Epoch 798/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.0520 - val_acc: 1.0000\n",
      "Epoch 799/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.0518 - val_acc: 1.0000\n",
      "Epoch 800/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.0516 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3ce659910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_relu.compile(optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "nn_relu.fit(X, y, epochs=800, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa3aef8ec50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3caa4dad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_range = np.arange(-5., 5., 0.02)\n",
    "dgrid = len(grid_range)\n",
    "Xpred = np.array([(a, b) for a in grid_range for b in grid_range])\n",
    "prob_pred = nn_relu.predict(Xpred)\n",
    "ypred = np.round(prob_pred)\n",
    "ypred = ypred.reshape((dgrid, dgrid))\n",
    "ypred = np.flipud(ypred)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.imshow(ypred, extent=[-4, 4, -4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ReLU activation function is discontinuous, and it results in a discontinuous decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we could use a tanh activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_tanh = models.Sequential()\n",
    "nn_tanh.add(layers.Dense(5, input_dim=2, activation=\"tanh\"))\n",
    "nn_tanh.add(layers.Dense(5, activation=\"tanh\"))\n",
    "nn_tanh.add(layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 40 samples\n",
      "Epoch 1/800\n",
      "160/160 [==============================] - 0s 1ms/step - loss: 0.6970 - acc: 0.4875 - val_loss: 0.6967 - val_acc: 0.5750\n",
      "Epoch 2/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.6928 - acc: 0.5062 - val_loss: 0.7024 - val_acc: 0.5750\n",
      "Epoch 3/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.6898 - acc: 0.5187 - val_loss: 0.7087 - val_acc: 0.5250\n",
      "Epoch 4/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.6856 - acc: 0.5312 - val_loss: 0.7145 - val_acc: 0.3750\n",
      "Epoch 5/800\n",
      "160/160 [==============================] - 0s 131us/step - loss: 0.6822 - acc: 0.5625 - val_loss: 0.7205 - val_acc: 0.3500\n",
      "Epoch 6/800\n",
      "160/160 [==============================] - 0s 108us/step - loss: 0.6793 - acc: 0.5687 - val_loss: 0.7269 - val_acc: 0.3000\n",
      "Epoch 7/800\n",
      "160/160 [==============================] - 0s 115us/step - loss: 0.6759 - acc: 0.5750 - val_loss: 0.7329 - val_acc: 0.3750\n",
      "Epoch 8/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.6730 - acc: 0.5750 - val_loss: 0.7390 - val_acc: 0.4000\n",
      "Epoch 9/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.6705 - acc: 0.5938 - val_loss: 0.7454 - val_acc: 0.4500\n",
      "Epoch 10/800\n",
      "160/160 [==============================] - 0s 123us/step - loss: 0.6677 - acc: 0.6063 - val_loss: 0.7514 - val_acc: 0.4500\n",
      "Epoch 11/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.6655 - acc: 0.6312 - val_loss: 0.7577 - val_acc: 0.4500\n",
      "Epoch 12/800\n",
      "160/160 [==============================] - 0s 103us/step - loss: 0.6631 - acc: 0.6375 - val_loss: 0.7640 - val_acc: 0.4250\n",
      "Epoch 13/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.6606 - acc: 0.6500 - val_loss: 0.7697 - val_acc: 0.4250\n",
      "Epoch 14/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.6585 - acc: 0.6625 - val_loss: 0.7756 - val_acc: 0.4250\n",
      "Epoch 15/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.6565 - acc: 0.6688 - val_loss: 0.7819 - val_acc: 0.4250\n",
      "Epoch 16/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.6544 - acc: 0.6625 - val_loss: 0.7881 - val_acc: 0.4250\n",
      "Epoch 17/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.6522 - acc: 0.6750 - val_loss: 0.7941 - val_acc: 0.4250\n",
      "Epoch 18/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.6504 - acc: 0.6813 - val_loss: 0.8004 - val_acc: 0.4250\n",
      "Epoch 19/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.6482 - acc: 0.6875 - val_loss: 0.8061 - val_acc: 0.4250\n",
      "Epoch 20/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.6465 - acc: 0.7125 - val_loss: 0.8124 - val_acc: 0.4250\n",
      "Epoch 21/800\n",
      "160/160 [==============================] - 0s 54us/step - loss: 0.6445 - acc: 0.7313 - val_loss: 0.8184 - val_acc: 0.4250\n",
      "Epoch 22/800\n",
      "160/160 [==============================] - 0s 131us/step - loss: 0.6426 - acc: 0.7375 - val_loss: 0.8241 - val_acc: 0.4000\n",
      "Epoch 23/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.6412 - acc: 0.7375 - val_loss: 0.8307 - val_acc: 0.3750\n",
      "Epoch 24/800\n",
      "160/160 [==============================] - 0s 106us/step - loss: 0.6391 - acc: 0.7438 - val_loss: 0.8361 - val_acc: 0.3750\n",
      "Epoch 25/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.6375 - acc: 0.7438 - val_loss: 0.8423 - val_acc: 0.3750\n",
      "Epoch 26/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.6355 - acc: 0.7500 - val_loss: 0.8475 - val_acc: 0.3500\n",
      "Epoch 27/800\n",
      "160/160 [==============================] - 0s 116us/step - loss: 0.6339 - acc: 0.7625 - val_loss: 0.8533 - val_acc: 0.3500\n",
      "Epoch 28/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.6322 - acc: 0.7625 - val_loss: 0.8586 - val_acc: 0.3500\n",
      "Epoch 29/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.6305 - acc: 0.7687 - val_loss: 0.8637 - val_acc: 0.3500\n",
      "Epoch 30/800\n",
      "160/160 [==============================] - 0s 112us/step - loss: 0.6291 - acc: 0.7812 - val_loss: 0.8697 - val_acc: 0.3500\n",
      "Epoch 31/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.6273 - acc: 0.7812 - val_loss: 0.8746 - val_acc: 0.3500\n",
      "Epoch 32/800\n",
      "160/160 [==============================] - 0s 110us/step - loss: 0.6257 - acc: 0.7687 - val_loss: 0.8796 - val_acc: 0.3500\n",
      "Epoch 33/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.6242 - acc: 0.7625 - val_loss: 0.8851 - val_acc: 0.3500\n",
      "Epoch 34/800\n",
      "160/160 [==============================] - 0s 99us/step - loss: 0.6225 - acc: 0.7562 - val_loss: 0.8898 - val_acc: 0.3500\n",
      "Epoch 35/800\n",
      "160/160 [==============================] - 0s 116us/step - loss: 0.6210 - acc: 0.7625 - val_loss: 0.8951 - val_acc: 0.3500\n",
      "Epoch 36/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.6194 - acc: 0.7687 - val_loss: 0.9002 - val_acc: 0.3500\n",
      "Epoch 37/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.6178 - acc: 0.7687 - val_loss: 0.9051 - val_acc: 0.3500\n",
      "Epoch 38/800\n",
      "160/160 [==============================] - 0s 109us/step - loss: 0.6165 - acc: 0.7687 - val_loss: 0.9106 - val_acc: 0.3500\n",
      "Epoch 39/800\n",
      "160/160 [==============================] - 0s 110us/step - loss: 0.6147 - acc: 0.7750 - val_loss: 0.9151 - val_acc: 0.3500\n",
      "Epoch 40/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.6133 - acc: 0.7812 - val_loss: 0.9194 - val_acc: 0.3500\n",
      "Epoch 41/800\n",
      "160/160 [==============================] - 0s 131us/step - loss: 0.6117 - acc: 0.7812 - val_loss: 0.9237 - val_acc: 0.3500\n",
      "Epoch 42/800\n",
      "160/160 [==============================] - 0s 148us/step - loss: 0.6103 - acc: 0.7812 - val_loss: 0.9283 - val_acc: 0.3250\n",
      "Epoch 43/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.6089 - acc: 0.7750 - val_loss: 0.9328 - val_acc: 0.3500\n",
      "Epoch 44/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.6072 - acc: 0.7750 - val_loss: 0.9364 - val_acc: 0.3250\n",
      "Epoch 45/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.6056 - acc: 0.7750 - val_loss: 0.9397 - val_acc: 0.3250\n",
      "Epoch 46/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.6043 - acc: 0.7750 - val_loss: 0.9433 - val_acc: 0.3250\n",
      "Epoch 47/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.6027 - acc: 0.7750 - val_loss: 0.9468 - val_acc: 0.3250\n",
      "Epoch 48/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.6014 - acc: 0.7750 - val_loss: 0.9508 - val_acc: 0.3000\n",
      "Epoch 49/800\n",
      "160/160 [==============================] - 0s 105us/step - loss: 0.5997 - acc: 0.7812 - val_loss: 0.9544 - val_acc: 0.3250\n",
      "Epoch 50/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.5982 - acc: 0.7812 - val_loss: 0.9574 - val_acc: 0.3250\n",
      "Epoch 51/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.5966 - acc: 0.7750 - val_loss: 0.9605 - val_acc: 0.3250\n",
      "Epoch 52/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.5950 - acc: 0.7750 - val_loss: 0.9627 - val_acc: 0.3250\n",
      "Epoch 53/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.5934 - acc: 0.7812 - val_loss: 0.9647 - val_acc: 0.3250\n",
      "Epoch 54/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.5922 - acc: 0.7812 - val_loss: 0.9683 - val_acc: 0.3250\n",
      "Epoch 55/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.5902 - acc: 0.7750 - val_loss: 0.9698 - val_acc: 0.3250\n",
      "Epoch 56/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.5886 - acc: 0.7687 - val_loss: 0.9718 - val_acc: 0.3250\n",
      "Epoch 57/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.5870 - acc: 0.7750 - val_loss: 0.9740 - val_acc: 0.3250\n",
      "Epoch 58/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.5853 - acc: 0.7875 - val_loss: 0.9755 - val_acc: 0.3250\n",
      "Epoch 59/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.5838 - acc: 0.7875 - val_loss: 0.9786 - val_acc: 0.3250\n",
      "Epoch 60/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.5819 - acc: 0.7875 - val_loss: 0.9800 - val_acc: 0.3250\n",
      "Epoch 61/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 86us/step - loss: 0.5803 - acc: 0.7875 - val_loss: 0.9812 - val_acc: 0.3250\n",
      "Epoch 62/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.5785 - acc: 0.7875 - val_loss: 0.9827 - val_acc: 0.3250\n",
      "Epoch 63/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.5766 - acc: 0.7812 - val_loss: 0.9836 - val_acc: 0.3250\n",
      "Epoch 64/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.5750 - acc: 0.7812 - val_loss: 0.9852 - val_acc: 0.3250\n",
      "Epoch 65/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.5731 - acc: 0.7812 - val_loss: 0.9855 - val_acc: 0.3250\n",
      "Epoch 66/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.5713 - acc: 0.7812 - val_loss: 0.9865 - val_acc: 0.3250\n",
      "Epoch 67/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.5693 - acc: 0.7812 - val_loss: 0.9864 - val_acc: 0.3250\n",
      "Epoch 68/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.5674 - acc: 0.7812 - val_loss: 0.9871 - val_acc: 0.3250\n",
      "Epoch 69/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.5655 - acc: 0.7812 - val_loss: 0.9875 - val_acc: 0.3250\n",
      "Epoch 70/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.5637 - acc: 0.7812 - val_loss: 0.9888 - val_acc: 0.3250\n",
      "Epoch 71/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.5615 - acc: 0.7812 - val_loss: 0.9871 - val_acc: 0.3250\n",
      "Epoch 72/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.5595 - acc: 0.7750 - val_loss: 0.9873 - val_acc: 0.3250\n",
      "Epoch 73/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.5576 - acc: 0.7750 - val_loss: 0.9879 - val_acc: 0.3250\n",
      "Epoch 74/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.5555 - acc: 0.7750 - val_loss: 0.9876 - val_acc: 0.3250\n",
      "Epoch 75/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.5534 - acc: 0.7750 - val_loss: 0.9867 - val_acc: 0.3250\n",
      "Epoch 76/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.5512 - acc: 0.7750 - val_loss: 0.9859 - val_acc: 0.3250\n",
      "Epoch 77/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.5494 - acc: 0.7750 - val_loss: 0.9859 - val_acc: 0.3500\n",
      "Epoch 78/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.5470 - acc: 0.7750 - val_loss: 0.9839 - val_acc: 0.3500\n",
      "Epoch 79/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.5447 - acc: 0.7750 - val_loss: 0.9833 - val_acc: 0.3500\n",
      "Epoch 80/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.5425 - acc: 0.7750 - val_loss: 0.9827 - val_acc: 0.3500\n",
      "Epoch 81/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.5402 - acc: 0.7750 - val_loss: 0.9815 - val_acc: 0.3500\n",
      "Epoch 82/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.5381 - acc: 0.7750 - val_loss: 0.9808 - val_acc: 0.3500\n",
      "Epoch 83/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.5358 - acc: 0.7750 - val_loss: 0.9805 - val_acc: 0.3500\n",
      "Epoch 84/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.5334 - acc: 0.7750 - val_loss: 0.9787 - val_acc: 0.3500\n",
      "Epoch 85/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.5310 - acc: 0.7750 - val_loss: 0.9770 - val_acc: 0.3500\n",
      "Epoch 86/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.5286 - acc: 0.7750 - val_loss: 0.9765 - val_acc: 0.3500\n",
      "Epoch 87/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.5263 - acc: 0.7750 - val_loss: 0.9746 - val_acc: 0.3500\n",
      "Epoch 88/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.5237 - acc: 0.7750 - val_loss: 0.9732 - val_acc: 0.3500\n",
      "Epoch 89/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.5214 - acc: 0.7750 - val_loss: 0.9704 - val_acc: 0.3500\n",
      "Epoch 90/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.5190 - acc: 0.7750 - val_loss: 0.9695 - val_acc: 0.3500\n",
      "Epoch 91/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.5164 - acc: 0.7750 - val_loss: 0.9680 - val_acc: 0.3500\n",
      "Epoch 92/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.5139 - acc: 0.7812 - val_loss: 0.9664 - val_acc: 0.3500\n",
      "Epoch 93/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.5115 - acc: 0.7875 - val_loss: 0.9645 - val_acc: 0.3500\n",
      "Epoch 94/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.5089 - acc: 0.7875 - val_loss: 0.9628 - val_acc: 0.3500\n",
      "Epoch 95/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.5063 - acc: 0.8000 - val_loss: 0.9603 - val_acc: 0.3500\n",
      "Epoch 96/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.5037 - acc: 0.8000 - val_loss: 0.9590 - val_acc: 0.3500\n",
      "Epoch 97/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.5011 - acc: 0.8000 - val_loss: 0.9578 - val_acc: 0.3500\n",
      "Epoch 98/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.4985 - acc: 0.8063 - val_loss: 0.9558 - val_acc: 0.3500\n",
      "Epoch 99/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.4959 - acc: 0.8063 - val_loss: 0.9544 - val_acc: 0.3500\n",
      "Epoch 100/800\n",
      "160/160 [==============================] - 0s 106us/step - loss: 0.4933 - acc: 0.8063 - val_loss: 0.9513 - val_acc: 0.3500\n",
      "Epoch 101/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.4905 - acc: 0.8063 - val_loss: 0.9493 - val_acc: 0.3500\n",
      "Epoch 102/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.4879 - acc: 0.8063 - val_loss: 0.9472 - val_acc: 0.3500\n",
      "Epoch 103/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.4852 - acc: 0.8063 - val_loss: 0.9450 - val_acc: 0.3500\n",
      "Epoch 104/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.4825 - acc: 0.8063 - val_loss: 0.9426 - val_acc: 0.3500\n",
      "Epoch 105/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.4798 - acc: 0.8063 - val_loss: 0.9394 - val_acc: 0.3500\n",
      "Epoch 106/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.4771 - acc: 0.8063 - val_loss: 0.9368 - val_acc: 0.3500\n",
      "Epoch 107/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.4742 - acc: 0.8063 - val_loss: 0.9348 - val_acc: 0.3500\n",
      "Epoch 108/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.4716 - acc: 0.8125 - val_loss: 0.9328 - val_acc: 0.3750\n",
      "Epoch 109/800\n",
      "160/160 [==============================] - 0s 139us/step - loss: 0.4688 - acc: 0.8125 - val_loss: 0.9311 - val_acc: 0.3750\n",
      "Epoch 110/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.4662 - acc: 0.8125 - val_loss: 0.9287 - val_acc: 0.3750\n",
      "Epoch 111/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.4632 - acc: 0.8125 - val_loss: 0.9270 - val_acc: 0.3750\n",
      "Epoch 112/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.4604 - acc: 0.8125 - val_loss: 0.9249 - val_acc: 0.3750\n",
      "Epoch 113/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.4576 - acc: 0.8125 - val_loss: 0.9218 - val_acc: 0.3750\n",
      "Epoch 114/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.4548 - acc: 0.8125 - val_loss: 0.9192 - val_acc: 0.3750\n",
      "Epoch 115/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.4520 - acc: 0.8125 - val_loss: 0.9155 - val_acc: 0.3750\n",
      "Epoch 116/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.4492 - acc: 0.8125 - val_loss: 0.9126 - val_acc: 0.3750\n",
      "Epoch 117/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.4463 - acc: 0.8125 - val_loss: 0.9101 - val_acc: 0.3750\n",
      "Epoch 118/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.4435 - acc: 0.8125 - val_loss: 0.9074 - val_acc: 0.3750\n",
      "Epoch 119/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.4405 - acc: 0.8187 - val_loss: 0.9050 - val_acc: 0.3750\n",
      "Epoch 120/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.4375 - acc: 0.8187 - val_loss: 0.9011 - val_acc: 0.3750\n",
      "Epoch 121/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 73us/step - loss: 0.4348 - acc: 0.8187 - val_loss: 0.8988 - val_acc: 0.3750\n",
      "Epoch 122/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.4318 - acc: 0.8187 - val_loss: 0.8945 - val_acc: 0.3750\n",
      "Epoch 123/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.4288 - acc: 0.8187 - val_loss: 0.8915 - val_acc: 0.3750\n",
      "Epoch 124/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.4259 - acc: 0.8187 - val_loss: 0.8883 - val_acc: 0.3750\n",
      "Epoch 125/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.4231 - acc: 0.8187 - val_loss: 0.8850 - val_acc: 0.3750\n",
      "Epoch 126/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.4201 - acc: 0.8250 - val_loss: 0.8818 - val_acc: 0.4000\n",
      "Epoch 127/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.4172 - acc: 0.8250 - val_loss: 0.8787 - val_acc: 0.4000\n",
      "Epoch 128/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.4143 - acc: 0.8250 - val_loss: 0.8753 - val_acc: 0.4250\n",
      "Epoch 129/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.4115 - acc: 0.8312 - val_loss: 0.8710 - val_acc: 0.4250\n",
      "Epoch 130/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.4084 - acc: 0.8312 - val_loss: 0.8684 - val_acc: 0.4250\n",
      "Epoch 131/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.4054 - acc: 0.8312 - val_loss: 0.8656 - val_acc: 0.4500\n",
      "Epoch 132/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.4026 - acc: 0.8312 - val_loss: 0.8612 - val_acc: 0.4500\n",
      "Epoch 133/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.3995 - acc: 0.8312 - val_loss: 0.8584 - val_acc: 0.4500\n",
      "Epoch 134/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.3968 - acc: 0.8438 - val_loss: 0.8551 - val_acc: 0.4750\n",
      "Epoch 135/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.3937 - acc: 0.8500 - val_loss: 0.8511 - val_acc: 0.5250\n",
      "Epoch 136/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.3908 - acc: 0.8500 - val_loss: 0.8483 - val_acc: 0.5250\n",
      "Epoch 137/800\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.4068 - acc: 0.812 - 0s 77us/step - loss: 0.3880 - acc: 0.8563 - val_loss: 0.8467 - val_acc: 0.5250\n",
      "Epoch 138/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.3850 - acc: 0.8625 - val_loss: 0.8425 - val_acc: 0.5250\n",
      "Epoch 139/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.3821 - acc: 0.8625 - val_loss: 0.8385 - val_acc: 0.5250\n",
      "Epoch 140/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.3792 - acc: 0.8688 - val_loss: 0.8337 - val_acc: 0.5250\n",
      "Epoch 141/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.3763 - acc: 0.8688 - val_loss: 0.8308 - val_acc: 0.5500\n",
      "Epoch 142/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.3733 - acc: 0.8688 - val_loss: 0.8269 - val_acc: 0.5500\n",
      "Epoch 143/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.3706 - acc: 0.8688 - val_loss: 0.8225 - val_acc: 0.5500\n",
      "Epoch 144/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.3675 - acc: 0.8688 - val_loss: 0.8184 - val_acc: 0.5500\n",
      "Epoch 145/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.3647 - acc: 0.8750 - val_loss: 0.8137 - val_acc: 0.5500\n",
      "Epoch 146/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.3619 - acc: 0.8812 - val_loss: 0.8105 - val_acc: 0.5500\n",
      "Epoch 147/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.3591 - acc: 0.8812 - val_loss: 0.8066 - val_acc: 0.5500\n",
      "Epoch 148/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.3561 - acc: 0.8812 - val_loss: 0.8012 - val_acc: 0.5500\n",
      "Epoch 149/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.3534 - acc: 0.8812 - val_loss: 0.7968 - val_acc: 0.5500\n",
      "Epoch 150/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.3504 - acc: 0.8812 - val_loss: 0.7916 - val_acc: 0.5500\n",
      "Epoch 151/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.3477 - acc: 0.8812 - val_loss: 0.7860 - val_acc: 0.5500\n",
      "Epoch 152/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.3448 - acc: 0.8812 - val_loss: 0.7825 - val_acc: 0.5750\n",
      "Epoch 153/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.3420 - acc: 0.8812 - val_loss: 0.7784 - val_acc: 0.6250\n",
      "Epoch 154/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.3391 - acc: 0.8812 - val_loss: 0.7756 - val_acc: 0.6250\n",
      "Epoch 155/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.3366 - acc: 0.8812 - val_loss: 0.7718 - val_acc: 0.6250\n",
      "Epoch 156/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.3337 - acc: 0.8812 - val_loss: 0.7672 - val_acc: 0.6250\n",
      "Epoch 157/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.3309 - acc: 0.8812 - val_loss: 0.7641 - val_acc: 0.6250\n",
      "Epoch 158/800\n",
      "160/160 [==============================] - 0s 119us/step - loss: 0.3283 - acc: 0.8875 - val_loss: 0.7595 - val_acc: 0.6250\n",
      "Epoch 159/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.3257 - acc: 0.8875 - val_loss: 0.7560 - val_acc: 0.6250\n",
      "Epoch 160/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.3231 - acc: 0.8875 - val_loss: 0.7497 - val_acc: 0.6250\n",
      "Epoch 161/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.3203 - acc: 0.8937 - val_loss: 0.7453 - val_acc: 0.6500\n",
      "Epoch 162/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.3178 - acc: 0.8937 - val_loss: 0.7415 - val_acc: 0.6500\n",
      "Epoch 163/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.3150 - acc: 0.8937 - val_loss: 0.7376 - val_acc: 0.6500\n",
      "Epoch 164/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.3123 - acc: 0.8937 - val_loss: 0.7339 - val_acc: 0.6500\n",
      "Epoch 165/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.3100 - acc: 0.8937 - val_loss: 0.7275 - val_acc: 0.6500\n",
      "Epoch 166/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.3073 - acc: 0.8937 - val_loss: 0.7235 - val_acc: 0.6500\n",
      "Epoch 167/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.3048 - acc: 0.8937 - val_loss: 0.7192 - val_acc: 0.6500\n",
      "Epoch 168/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.3024 - acc: 0.8937 - val_loss: 0.7162 - val_acc: 0.6500\n",
      "Epoch 169/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.2997 - acc: 0.8937 - val_loss: 0.7114 - val_acc: 0.6500\n",
      "Epoch 170/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.2973 - acc: 0.8937 - val_loss: 0.7081 - val_acc: 0.6500\n",
      "Epoch 171/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.2950 - acc: 0.8937 - val_loss: 0.7044 - val_acc: 0.6500\n",
      "Epoch 172/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.2924 - acc: 0.9062 - val_loss: 0.7005 - val_acc: 0.6500\n",
      "Epoch 173/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.2902 - acc: 0.9062 - val_loss: 0.6945 - val_acc: 0.6500\n",
      "Epoch 174/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.2877 - acc: 0.9062 - val_loss: 0.6913 - val_acc: 0.6500\n",
      "Epoch 175/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.2856 - acc: 0.9062 - val_loss: 0.6850 - val_acc: 0.6750\n",
      "Epoch 176/800\n",
      "160/160 [==============================] - 0s 106us/step - loss: 0.2831 - acc: 0.9062 - val_loss: 0.6805 - val_acc: 0.6750\n",
      "Epoch 177/800\n",
      "160/160 [==============================] - 0s 124us/step - loss: 0.2807 - acc: 0.9062 - val_loss: 0.6784 - val_acc: 0.6750\n",
      "Epoch 178/800\n",
      "160/160 [==============================] - 0s 137us/step - loss: 0.2786 - acc: 0.9125 - val_loss: 0.6751 - val_acc: 0.6750\n",
      "Epoch 179/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.2763 - acc: 0.9188 - val_loss: 0.6713 - val_acc: 0.6750\n",
      "Epoch 180/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.2743 - acc: 0.9188 - val_loss: 0.6661 - val_acc: 0.6750\n",
      "Epoch 181/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 148us/step - loss: 0.2719 - acc: 0.9188 - val_loss: 0.6637 - val_acc: 0.6750\n",
      "Epoch 182/800\n",
      "160/160 [==============================] - 0s 127us/step - loss: 0.2699 - acc: 0.9188 - val_loss: 0.6598 - val_acc: 0.7000\n",
      "Epoch 183/800\n",
      "160/160 [==============================] - 0s 110us/step - loss: 0.2678 - acc: 0.9188 - val_loss: 0.6556 - val_acc: 0.6750\n",
      "Epoch 184/800\n",
      "160/160 [==============================] - 0s 107us/step - loss: 0.2656 - acc: 0.9250 - val_loss: 0.6522 - val_acc: 0.7000\n",
      "Epoch 185/800\n",
      "160/160 [==============================] - 0s 103us/step - loss: 0.2637 - acc: 0.9250 - val_loss: 0.6482 - val_acc: 0.7000\n",
      "Epoch 186/800\n",
      "160/160 [==============================] - 0s 123us/step - loss: 0.2615 - acc: 0.9313 - val_loss: 0.6440 - val_acc: 0.7000\n",
      "Epoch 187/800\n",
      "160/160 [==============================] - 0s 127us/step - loss: 0.2595 - acc: 0.9313 - val_loss: 0.6404 - val_acc: 0.7000\n",
      "Epoch 188/800\n",
      "160/160 [==============================] - 0s 111us/step - loss: 0.2574 - acc: 0.9313 - val_loss: 0.6377 - val_acc: 0.7000\n",
      "Epoch 189/800\n",
      "160/160 [==============================] - 0s 120us/step - loss: 0.2554 - acc: 0.9313 - val_loss: 0.6345 - val_acc: 0.7000\n",
      "Epoch 190/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.2535 - acc: 0.9313 - val_loss: 0.6294 - val_acc: 0.7000\n",
      "Epoch 191/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.2516 - acc: 0.9313 - val_loss: 0.6275 - val_acc: 0.7000\n",
      "Epoch 192/800\n",
      "160/160 [==============================] - 0s 110us/step - loss: 0.2496 - acc: 0.9313 - val_loss: 0.6230 - val_acc: 0.7000\n",
      "Epoch 193/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.2479 - acc: 0.9313 - val_loss: 0.6192 - val_acc: 0.7000\n",
      "Epoch 194/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.2458 - acc: 0.9313 - val_loss: 0.6161 - val_acc: 0.7000\n",
      "Epoch 195/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.2442 - acc: 0.9313 - val_loss: 0.6129 - val_acc: 0.7000\n",
      "Epoch 196/800\n",
      "160/160 [==============================] - 0s 135us/step - loss: 0.2422 - acc: 0.9313 - val_loss: 0.6104 - val_acc: 0.7000\n",
      "Epoch 197/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.2405 - acc: 0.9313 - val_loss: 0.6064 - val_acc: 0.7000\n",
      "Epoch 198/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.2387 - acc: 0.9313 - val_loss: 0.6024 - val_acc: 0.7000\n",
      "Epoch 199/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.2369 - acc: 0.9375 - val_loss: 0.5992 - val_acc: 0.7000\n",
      "Epoch 200/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.2351 - acc: 0.9375 - val_loss: 0.5963 - val_acc: 0.7000\n",
      "Epoch 201/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.2336 - acc: 0.9375 - val_loss: 0.5938 - val_acc: 0.7000\n",
      "Epoch 202/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.2319 - acc: 0.9375 - val_loss: 0.5903 - val_acc: 0.7000\n",
      "Epoch 203/800\n",
      "160/160 [==============================] - 0s 114us/step - loss: 0.2302 - acc: 0.9375 - val_loss: 0.5889 - val_acc: 0.7250\n",
      "Epoch 204/800\n",
      "160/160 [==============================] - 0s 104us/step - loss: 0.2286 - acc: 0.9375 - val_loss: 0.5845 - val_acc: 0.7500\n",
      "Epoch 205/800\n",
      "160/160 [==============================] - 0s 136us/step - loss: 0.2268 - acc: 0.9375 - val_loss: 0.5811 - val_acc: 0.7750\n",
      "Epoch 206/800\n",
      "160/160 [==============================] - 0s 131us/step - loss: 0.2252 - acc: 0.9375 - val_loss: 0.5789 - val_acc: 0.7750\n",
      "Epoch 207/800\n",
      "160/160 [==============================] - 0s 112us/step - loss: 0.2238 - acc: 0.9375 - val_loss: 0.5734 - val_acc: 0.7750\n",
      "Epoch 208/800\n",
      "160/160 [==============================] - 0s 114us/step - loss: 0.2222 - acc: 0.9375 - val_loss: 0.5699 - val_acc: 0.7750\n",
      "Epoch 209/800\n",
      "160/160 [==============================] - 0s 116us/step - loss: 0.2206 - acc: 0.9375 - val_loss: 0.5691 - val_acc: 0.7750\n",
      "Epoch 210/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.2191 - acc: 0.9375 - val_loss: 0.5657 - val_acc: 0.7750\n",
      "Epoch 211/800\n",
      "160/160 [==============================] - 0s 111us/step - loss: 0.2175 - acc: 0.9437 - val_loss: 0.5628 - val_acc: 0.7750\n",
      "Epoch 212/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.2160 - acc: 0.9437 - val_loss: 0.5599 - val_acc: 0.7750\n",
      "Epoch 213/800\n",
      "160/160 [==============================] - 0s 118us/step - loss: 0.2145 - acc: 0.9500 - val_loss: 0.5575 - val_acc: 0.7750\n",
      "Epoch 214/800\n",
      "160/160 [==============================] - 0s 122us/step - loss: 0.2131 - acc: 0.9500 - val_loss: 0.5547 - val_acc: 0.7750\n",
      "Epoch 215/800\n",
      "160/160 [==============================] - 0s 152us/step - loss: 0.2117 - acc: 0.9562 - val_loss: 0.5518 - val_acc: 0.7750\n",
      "Epoch 216/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.2104 - acc: 0.9562 - val_loss: 0.5497 - val_acc: 0.7750\n",
      "Epoch 217/800\n",
      "160/160 [==============================] - 0s 99us/step - loss: 0.2089 - acc: 0.9562 - val_loss: 0.5466 - val_acc: 0.7750\n",
      "Epoch 218/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.2075 - acc: 0.9500 - val_loss: 0.5428 - val_acc: 0.7750\n",
      "Epoch 219/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.2061 - acc: 0.9500 - val_loss: 0.5406 - val_acc: 0.7750\n",
      "Epoch 220/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.2048 - acc: 0.9500 - val_loss: 0.5371 - val_acc: 0.7750\n",
      "Epoch 221/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.2034 - acc: 0.9500 - val_loss: 0.5335 - val_acc: 0.7750\n",
      "Epoch 222/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.2021 - acc: 0.9500 - val_loss: 0.5309 - val_acc: 0.7750\n",
      "Epoch 223/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.2007 - acc: 0.9500 - val_loss: 0.5287 - val_acc: 0.7750\n",
      "Epoch 224/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.1995 - acc: 0.9500 - val_loss: 0.5272 - val_acc: 0.7750\n",
      "Epoch 225/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.1983 - acc: 0.9500 - val_loss: 0.5239 - val_acc: 0.7750\n",
      "Epoch 226/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.1969 - acc: 0.9500 - val_loss: 0.5209 - val_acc: 0.8000\n",
      "Epoch 227/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.1957 - acc: 0.9500 - val_loss: 0.5178 - val_acc: 0.8000\n",
      "Epoch 228/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.1944 - acc: 0.9562 - val_loss: 0.5151 - val_acc: 0.8000\n",
      "Epoch 229/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.1932 - acc: 0.9562 - val_loss: 0.5122 - val_acc: 0.8000\n",
      "Epoch 230/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.1919 - acc: 0.9562 - val_loss: 0.5110 - val_acc: 0.8000\n",
      "Epoch 231/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.1908 - acc: 0.9562 - val_loss: 0.5091 - val_acc: 0.8000\n",
      "Epoch 232/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.1894 - acc: 0.9562 - val_loss: 0.5064 - val_acc: 0.8000\n",
      "Epoch 233/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1884 - acc: 0.9562 - val_loss: 0.5018 - val_acc: 0.8000\n",
      "Epoch 234/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.1871 - acc: 0.9562 - val_loss: 0.5002 - val_acc: 0.8000\n",
      "Epoch 235/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.1859 - acc: 0.9562 - val_loss: 0.4973 - val_acc: 0.8000\n",
      "Epoch 236/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.1848 - acc: 0.9562 - val_loss: 0.4952 - val_acc: 0.8000\n",
      "Epoch 237/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.1837 - acc: 0.9562 - val_loss: 0.4939 - val_acc: 0.8000\n",
      "Epoch 238/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1827 - acc: 0.9562 - val_loss: 0.4897 - val_acc: 0.8000\n",
      "Epoch 239/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.1813 - acc: 0.9625 - val_loss: 0.4866 - val_acc: 0.8000\n",
      "Epoch 240/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.1802 - acc: 0.9625 - val_loss: 0.4843 - val_acc: 0.8000\n",
      "Epoch 241/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 88us/step - loss: 0.1792 - acc: 0.9625 - val_loss: 0.4813 - val_acc: 0.8000\n",
      "Epoch 242/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.1781 - acc: 0.9688 - val_loss: 0.4792 - val_acc: 0.8000\n",
      "Epoch 243/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.1771 - acc: 0.9688 - val_loss: 0.4784 - val_acc: 0.8000\n",
      "Epoch 244/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.1759 - acc: 0.9688 - val_loss: 0.4756 - val_acc: 0.8000\n",
      "Epoch 245/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.1750 - acc: 0.9688 - val_loss: 0.4730 - val_acc: 0.8000\n",
      "Epoch 246/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.1738 - acc: 0.9688 - val_loss: 0.4705 - val_acc: 0.8000\n",
      "Epoch 247/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.1728 - acc: 0.9688 - val_loss: 0.4682 - val_acc: 0.8000\n",
      "Epoch 248/800\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.2174 - acc: 0.968 - 0s 78us/step - loss: 0.1720 - acc: 0.9688 - val_loss: 0.4643 - val_acc: 0.8000\n",
      "Epoch 249/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.1707 - acc: 0.9688 - val_loss: 0.4625 - val_acc: 0.8000\n",
      "Epoch 250/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.1697 - acc: 0.9688 - val_loss: 0.4587 - val_acc: 0.8000\n",
      "Epoch 251/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.1686 - acc: 0.9688 - val_loss: 0.4574 - val_acc: 0.8000\n",
      "Epoch 252/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.1676 - acc: 0.9688 - val_loss: 0.4549 - val_acc: 0.8000\n",
      "Epoch 253/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.1667 - acc: 0.9688 - val_loss: 0.4519 - val_acc: 0.8000\n",
      "Epoch 254/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.1656 - acc: 0.9688 - val_loss: 0.4500 - val_acc: 0.8000\n",
      "Epoch 255/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.1647 - acc: 0.9688 - val_loss: 0.4495 - val_acc: 0.8000\n",
      "Epoch 256/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.1636 - acc: 0.9688 - val_loss: 0.4465 - val_acc: 0.8000\n",
      "Epoch 257/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.1627 - acc: 0.9688 - val_loss: 0.4454 - val_acc: 0.8000\n",
      "Epoch 258/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.1618 - acc: 0.9688 - val_loss: 0.4416 - val_acc: 0.8000\n",
      "Epoch 259/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.1607 - acc: 0.9688 - val_loss: 0.4387 - val_acc: 0.8000\n",
      "Epoch 260/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.1598 - acc: 0.9688 - val_loss: 0.4361 - val_acc: 0.8000\n",
      "Epoch 261/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.1589 - acc: 0.9688 - val_loss: 0.4339 - val_acc: 0.8000\n",
      "Epoch 262/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1580 - acc: 0.9688 - val_loss: 0.4304 - val_acc: 0.8000\n",
      "Epoch 263/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.1570 - acc: 0.9688 - val_loss: 0.4276 - val_acc: 0.8000\n",
      "Epoch 264/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.1562 - acc: 0.9688 - val_loss: 0.4254 - val_acc: 0.8000\n",
      "Epoch 265/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.1552 - acc: 0.9688 - val_loss: 0.4227 - val_acc: 0.8000\n",
      "Epoch 266/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.1543 - acc: 0.9688 - val_loss: 0.4207 - val_acc: 0.8000\n",
      "Epoch 267/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.1533 - acc: 0.9688 - val_loss: 0.4191 - val_acc: 0.8000\n",
      "Epoch 268/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.1525 - acc: 0.9688 - val_loss: 0.4178 - val_acc: 0.8000\n",
      "Epoch 269/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.1516 - acc: 0.9688 - val_loss: 0.4155 - val_acc: 0.8000\n",
      "Epoch 270/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.1507 - acc: 0.9688 - val_loss: 0.4124 - val_acc: 0.8000\n",
      "Epoch 271/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.1498 - acc: 0.9688 - val_loss: 0.4107 - val_acc: 0.8000\n",
      "Epoch 272/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.1489 - acc: 0.9688 - val_loss: 0.4072 - val_acc: 0.8000\n",
      "Epoch 273/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.1481 - acc: 0.9688 - val_loss: 0.4039 - val_acc: 0.8000\n",
      "Epoch 274/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.1471 - acc: 0.9688 - val_loss: 0.4013 - val_acc: 0.8000\n",
      "Epoch 275/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.1463 - acc: 0.9688 - val_loss: 0.4000 - val_acc: 0.8000\n",
      "Epoch 276/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.1456 - acc: 0.9688 - val_loss: 0.3972 - val_acc: 0.8000\n",
      "Epoch 277/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.1446 - acc: 0.9688 - val_loss: 0.3948 - val_acc: 0.8000\n",
      "Epoch 278/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.1437 - acc: 0.9750 - val_loss: 0.3931 - val_acc: 0.8000\n",
      "Epoch 279/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.1430 - acc: 0.9750 - val_loss: 0.3917 - val_acc: 0.8000\n",
      "Epoch 280/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.1422 - acc: 0.9750 - val_loss: 0.3878 - val_acc: 0.8000\n",
      "Epoch 281/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.1413 - acc: 0.9750 - val_loss: 0.3847 - val_acc: 0.8250\n",
      "Epoch 282/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.1404 - acc: 0.9750 - val_loss: 0.3829 - val_acc: 0.8250\n",
      "Epoch 283/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.1396 - acc: 0.9750 - val_loss: 0.3812 - val_acc: 0.8250\n",
      "Epoch 284/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.1388 - acc: 0.9750 - val_loss: 0.3797 - val_acc: 0.8250\n",
      "Epoch 285/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.1381 - acc: 0.9750 - val_loss: 0.3771 - val_acc: 0.8250\n",
      "Epoch 286/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.1372 - acc: 0.9750 - val_loss: 0.3756 - val_acc: 0.8250\n",
      "Epoch 287/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.1364 - acc: 0.9750 - val_loss: 0.3730 - val_acc: 0.8250\n",
      "Epoch 288/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.1356 - acc: 0.9750 - val_loss: 0.3703 - val_acc: 0.8250\n",
      "Epoch 289/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.1348 - acc: 0.9750 - val_loss: 0.3685 - val_acc: 0.8250\n",
      "Epoch 290/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.1341 - acc: 0.9750 - val_loss: 0.3644 - val_acc: 0.8250\n",
      "Epoch 291/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.1332 - acc: 0.9750 - val_loss: 0.3625 - val_acc: 0.8250\n",
      "Epoch 292/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.1326 - acc: 0.9750 - val_loss: 0.3594 - val_acc: 0.8250\n",
      "Epoch 293/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.1316 - acc: 0.9750 - val_loss: 0.3586 - val_acc: 0.8250\n",
      "Epoch 294/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.1310 - acc: 0.9750 - val_loss: 0.3547 - val_acc: 0.8250\n",
      "Epoch 295/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.1302 - acc: 0.9750 - val_loss: 0.3539 - val_acc: 0.8250\n",
      "Epoch 296/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.1294 - acc: 0.9750 - val_loss: 0.3521 - val_acc: 0.8250\n",
      "Epoch 297/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.1287 - acc: 0.9750 - val_loss: 0.3479 - val_acc: 0.8250\n",
      "Epoch 298/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.1278 - acc: 0.9750 - val_loss: 0.3457 - val_acc: 0.8500\n",
      "Epoch 299/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.1271 - acc: 0.9750 - val_loss: 0.3427 - val_acc: 0.8500\n",
      "Epoch 300/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.1264 - acc: 0.9750 - val_loss: 0.3403 - val_acc: 0.8500\n",
      "Epoch 301/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 70us/step - loss: 0.1257 - acc: 0.9750 - val_loss: 0.3378 - val_acc: 0.8500\n",
      "Epoch 302/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.1248 - acc: 0.9750 - val_loss: 0.3363 - val_acc: 0.8500\n",
      "Epoch 303/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.1242 - acc: 0.9750 - val_loss: 0.3352 - val_acc: 0.8500\n",
      "Epoch 304/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.1234 - acc: 0.9750 - val_loss: 0.3338 - val_acc: 0.8500\n",
      "Epoch 305/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.1227 - acc: 0.9750 - val_loss: 0.3312 - val_acc: 0.8500\n",
      "Epoch 306/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.1219 - acc: 0.9750 - val_loss: 0.3286 - val_acc: 0.8500\n",
      "Epoch 307/800\n",
      "160/160 [==============================] - 0s 123us/step - loss: 0.1212 - acc: 0.9750 - val_loss: 0.3250 - val_acc: 0.8500\n",
      "Epoch 308/800\n",
      "160/160 [==============================] - 0s 162us/step - loss: 0.1205 - acc: 0.9750 - val_loss: 0.3220 - val_acc: 0.8500\n",
      "Epoch 309/800\n",
      "160/160 [==============================] - 0s 109us/step - loss: 0.1198 - acc: 0.9750 - val_loss: 0.3197 - val_acc: 0.8500\n",
      "Epoch 310/800\n",
      "160/160 [==============================] - 0s 122us/step - loss: 0.1191 - acc: 0.9750 - val_loss: 0.3173 - val_acc: 0.8500\n",
      "Epoch 311/800\n",
      "160/160 [==============================] - 0s 111us/step - loss: 0.1183 - acc: 0.9750 - val_loss: 0.3151 - val_acc: 0.8500\n",
      "Epoch 312/800\n",
      "160/160 [==============================] - 0s 118us/step - loss: 0.1177 - acc: 0.9750 - val_loss: 0.3140 - val_acc: 0.8500\n",
      "Epoch 313/800\n",
      "160/160 [==============================] - 0s 131us/step - loss: 0.1170 - acc: 0.9750 - val_loss: 0.3110 - val_acc: 0.8500\n",
      "Epoch 314/800\n",
      "160/160 [==============================] - 0s 107us/step - loss: 0.1163 - acc: 0.9750 - val_loss: 0.3084 - val_acc: 0.8500\n",
      "Epoch 315/800\n",
      "160/160 [==============================] - 0s 116us/step - loss: 0.1155 - acc: 0.9750 - val_loss: 0.3065 - val_acc: 0.8500\n",
      "Epoch 316/800\n",
      "160/160 [==============================] - 0s 111us/step - loss: 0.1149 - acc: 0.9750 - val_loss: 0.3038 - val_acc: 0.8500\n",
      "Epoch 317/800\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.1143 - acc: 0.9750 - val_loss: 0.3020 - val_acc: 0.8500\n",
      "Epoch 318/800\n",
      "160/160 [==============================] - 0s 117us/step - loss: 0.1136 - acc: 0.9750 - val_loss: 0.2992 - val_acc: 0.8500\n",
      "Epoch 319/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.1129 - acc: 0.9750 - val_loss: 0.2980 - val_acc: 0.8500\n",
      "Epoch 320/800\n",
      "160/160 [==============================] - 0s 106us/step - loss: 0.1122 - acc: 0.9750 - val_loss: 0.2957 - val_acc: 0.8500\n",
      "Epoch 321/800\n",
      "160/160 [==============================] - 0s 107us/step - loss: 0.1115 - acc: 0.9750 - val_loss: 0.2932 - val_acc: 0.8500\n",
      "Epoch 322/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.1108 - acc: 0.9750 - val_loss: 0.2903 - val_acc: 0.8500\n",
      "Epoch 323/800\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.1288 - acc: 0.937 - 0s 104us/step - loss: 0.1102 - acc: 0.9750 - val_loss: 0.2879 - val_acc: 0.8500\n",
      "Epoch 324/800\n",
      "160/160 [==============================] - 0s 104us/step - loss: 0.1095 - acc: 0.9750 - val_loss: 0.2872 - val_acc: 0.8500\n",
      "Epoch 325/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.1090 - acc: 0.9750 - val_loss: 0.2851 - val_acc: 0.8500\n",
      "Epoch 326/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.1082 - acc: 0.9750 - val_loss: 0.2831 - val_acc: 0.8500\n",
      "Epoch 327/800\n",
      "160/160 [==============================] - 0s 102us/step - loss: 0.1075 - acc: 0.9750 - val_loss: 0.2802 - val_acc: 0.8500\n",
      "Epoch 328/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.1069 - acc: 0.9750 - val_loss: 0.2787 - val_acc: 0.8500\n",
      "Epoch 329/800\n",
      "160/160 [==============================] - 0s 105us/step - loss: 0.1062 - acc: 0.9750 - val_loss: 0.2765 - val_acc: 0.8500\n",
      "Epoch 330/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.1056 - acc: 0.9750 - val_loss: 0.2733 - val_acc: 0.8500\n",
      "Epoch 331/800\n",
      "160/160 [==============================] - 0s 120us/step - loss: 0.1050 - acc: 0.9750 - val_loss: 0.2706 - val_acc: 0.8500\n",
      "Epoch 332/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.1043 - acc: 0.9750 - val_loss: 0.2695 - val_acc: 0.8500\n",
      "Epoch 333/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.1038 - acc: 0.9750 - val_loss: 0.2663 - val_acc: 0.8500\n",
      "Epoch 334/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.1031 - acc: 0.9750 - val_loss: 0.2639 - val_acc: 0.8500\n",
      "Epoch 335/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.1025 - acc: 0.9750 - val_loss: 0.2631 - val_acc: 0.8500\n",
      "Epoch 336/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.1019 - acc: 0.9750 - val_loss: 0.2607 - val_acc: 0.8500\n",
      "Epoch 337/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.1012 - acc: 0.9750 - val_loss: 0.2582 - val_acc: 0.8500\n",
      "Epoch 338/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.1007 - acc: 0.9750 - val_loss: 0.2560 - val_acc: 0.8500\n",
      "Epoch 339/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.1000 - acc: 0.9813 - val_loss: 0.2549 - val_acc: 0.8750\n",
      "Epoch 340/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0996 - acc: 0.9813 - val_loss: 0.2531 - val_acc: 0.8750\n",
      "Epoch 341/800\n",
      "160/160 [==============================] - 0s 110us/step - loss: 0.0988 - acc: 0.9813 - val_loss: 0.2503 - val_acc: 0.8750\n",
      "Epoch 342/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0984 - acc: 0.9813 - val_loss: 0.2464 - val_acc: 0.8750\n",
      "Epoch 343/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0977 - acc: 0.9813 - val_loss: 0.2454 - val_acc: 0.8750\n",
      "Epoch 344/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0971 - acc: 0.9813 - val_loss: 0.2423 - val_acc: 0.8750\n",
      "Epoch 345/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0966 - acc: 0.9813 - val_loss: 0.2400 - val_acc: 0.8750\n",
      "Epoch 346/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0959 - acc: 0.9813 - val_loss: 0.2392 - val_acc: 0.9000\n",
      "Epoch 347/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0953 - acc: 0.9813 - val_loss: 0.2374 - val_acc: 0.9000\n",
      "Epoch 348/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0948 - acc: 0.9813 - val_loss: 0.2355 - val_acc: 0.9000\n",
      "Epoch 349/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0943 - acc: 0.9813 - val_loss: 0.2331 - val_acc: 0.9000\n",
      "Epoch 350/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0936 - acc: 0.9813 - val_loss: 0.2312 - val_acc: 0.9000\n",
      "Epoch 351/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0931 - acc: 0.9813 - val_loss: 0.2307 - val_acc: 0.9000\n",
      "Epoch 352/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0925 - acc: 0.9813 - val_loss: 0.2289 - val_acc: 0.9000\n",
      "Epoch 353/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0921 - acc: 0.9813 - val_loss: 0.2259 - val_acc: 0.9000\n",
      "Epoch 354/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0915 - acc: 0.9813 - val_loss: 0.2239 - val_acc: 0.9000\n",
      "Epoch 355/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0910 - acc: 0.9813 - val_loss: 0.2220 - val_acc: 0.9000\n",
      "Epoch 356/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.0904 - acc: 0.9813 - val_loss: 0.2216 - val_acc: 0.9000\n",
      "Epoch 357/800\n",
      "160/160 [==============================] - 0s 103us/step - loss: 0.0898 - acc: 0.9813 - val_loss: 0.2198 - val_acc: 0.9000\n",
      "Epoch 358/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0893 - acc: 0.9813 - val_loss: 0.2172 - val_acc: 0.9000\n",
      "Epoch 359/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0888 - acc: 0.9813 - val_loss: 0.2155 - val_acc: 0.9000\n",
      "Epoch 360/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0883 - acc: 0.9813 - val_loss: 0.2131 - val_acc: 0.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 361/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0877 - acc: 0.9813 - val_loss: 0.2112 - val_acc: 0.9000\n",
      "Epoch 362/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0872 - acc: 0.9813 - val_loss: 0.2096 - val_acc: 0.9000\n",
      "Epoch 363/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0867 - acc: 0.9813 - val_loss: 0.2076 - val_acc: 0.9000\n",
      "Epoch 364/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0862 - acc: 0.9813 - val_loss: 0.2051 - val_acc: 0.9000\n",
      "Epoch 365/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0857 - acc: 0.9813 - val_loss: 0.2040 - val_acc: 0.9000\n",
      "Epoch 366/800\n",
      "160/160 [==============================] - 0s 110us/step - loss: 0.0853 - acc: 0.9813 - val_loss: 0.2028 - val_acc: 0.9000\n",
      "Epoch 367/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0847 - acc: 0.9813 - val_loss: 0.2005 - val_acc: 0.9250\n",
      "Epoch 368/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0842 - acc: 0.9813 - val_loss: 0.1995 - val_acc: 0.9250\n",
      "Epoch 369/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0837 - acc: 0.9813 - val_loss: 0.1978 - val_acc: 0.9500\n",
      "Epoch 370/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0832 - acc: 0.9813 - val_loss: 0.1962 - val_acc: 0.9500\n",
      "Epoch 371/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0827 - acc: 0.9813 - val_loss: 0.1940 - val_acc: 0.9500\n",
      "Epoch 372/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0823 - acc: 0.9813 - val_loss: 0.1913 - val_acc: 0.9500\n",
      "Epoch 373/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0819 - acc: 0.9813 - val_loss: 0.1898 - val_acc: 0.9500\n",
      "Epoch 374/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0813 - acc: 0.9813 - val_loss: 0.1887 - val_acc: 0.9500\n",
      "Epoch 375/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0809 - acc: 0.9813 - val_loss: 0.1868 - val_acc: 0.9750\n",
      "Epoch 376/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0804 - acc: 0.9813 - val_loss: 0.1861 - val_acc: 1.0000\n",
      "Epoch 377/800\n",
      "160/160 [==============================] - 0s 109us/step - loss: 0.0800 - acc: 0.9813 - val_loss: 0.1839 - val_acc: 1.0000\n",
      "Epoch 378/800\n",
      "160/160 [==============================] - 0s 148us/step - loss: 0.0795 - acc: 0.9813 - val_loss: 0.1831 - val_acc: 1.0000\n",
      "Epoch 379/800\n",
      "160/160 [==============================] - 0s 115us/step - loss: 0.0791 - acc: 0.9813 - val_loss: 0.1813 - val_acc: 1.0000\n",
      "Epoch 380/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.0786 - acc: 0.9813 - val_loss: 0.1799 - val_acc: 1.0000\n",
      "Epoch 381/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0783 - acc: 0.9875 - val_loss: 0.1784 - val_acc: 1.0000\n",
      "Epoch 382/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0777 - acc: 0.9875 - val_loss: 0.1770 - val_acc: 1.0000\n",
      "Epoch 383/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0773 - acc: 0.9875 - val_loss: 0.1755 - val_acc: 1.0000\n",
      "Epoch 384/800\n",
      "160/160 [==============================] - 0s 147us/step - loss: 0.0768 - acc: 0.9875 - val_loss: 0.1745 - val_acc: 1.0000\n",
      "Epoch 385/800\n",
      "160/160 [==============================] - 0s 104us/step - loss: 0.0764 - acc: 0.9875 - val_loss: 0.1733 - val_acc: 1.0000\n",
      "Epoch 386/800\n",
      "160/160 [==============================] - 0s 109us/step - loss: 0.0761 - acc: 0.9875 - val_loss: 0.1710 - val_acc: 1.0000\n",
      "Epoch 387/800\n",
      "160/160 [==============================] - 0s 124us/step - loss: 0.0757 - acc: 0.9875 - val_loss: 0.1689 - val_acc: 1.0000\n",
      "Epoch 388/800\n",
      "160/160 [==============================] - 0s 122us/step - loss: 0.0752 - acc: 0.9875 - val_loss: 0.1674 - val_acc: 1.0000\n",
      "Epoch 389/800\n",
      "160/160 [==============================] - 0s 103us/step - loss: 0.0748 - acc: 0.9875 - val_loss: 0.1660 - val_acc: 1.0000\n",
      "Epoch 390/800\n",
      "160/160 [==============================] - 0s 121us/step - loss: 0.0745 - acc: 0.9875 - val_loss: 0.1643 - val_acc: 1.0000\n",
      "Epoch 391/800\n",
      "160/160 [==============================] - 0s 104us/step - loss: 0.0740 - acc: 0.9875 - val_loss: 0.1642 - val_acc: 1.0000\n",
      "Epoch 392/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0736 - acc: 0.9875 - val_loss: 0.1625 - val_acc: 1.0000\n",
      "Epoch 393/800\n",
      "160/160 [==============================] - 0s 151us/step - loss: 0.0732 - acc: 0.9875 - val_loss: 0.1620 - val_acc: 1.0000\n",
      "Epoch 394/800\n",
      "160/160 [==============================] - 0s 108us/step - loss: 0.0728 - acc: 0.9875 - val_loss: 0.1604 - val_acc: 1.0000\n",
      "Epoch 395/800\n",
      "160/160 [==============================] - 0s 113us/step - loss: 0.0724 - acc: 0.9875 - val_loss: 0.1595 - val_acc: 1.0000\n",
      "Epoch 396/800\n",
      "160/160 [==============================] - 0s 130us/step - loss: 0.0720 - acc: 0.9875 - val_loss: 0.1585 - val_acc: 1.0000\n",
      "Epoch 397/800\n",
      "160/160 [==============================] - 0s 106us/step - loss: 0.0716 - acc: 0.9875 - val_loss: 0.1576 - val_acc: 1.0000\n",
      "Epoch 398/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.0713 - acc: 0.9875 - val_loss: 0.1562 - val_acc: 1.0000\n",
      "Epoch 399/800\n",
      "160/160 [==============================] - 0s 107us/step - loss: 0.0709 - acc: 0.9938 - val_loss: 0.1559 - val_acc: 1.0000\n",
      "Epoch 400/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0704 - acc: 0.9938 - val_loss: 0.1547 - val_acc: 1.0000\n",
      "Epoch 401/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0701 - acc: 0.9938 - val_loss: 0.1530 - val_acc: 1.0000\n",
      "Epoch 402/800\n",
      "160/160 [==============================] - 0s 108us/step - loss: 0.0697 - acc: 0.9938 - val_loss: 0.1516 - val_acc: 1.0000\n",
      "Epoch 403/800\n",
      "160/160 [==============================] - 0s 175us/step - loss: 0.0694 - acc: 0.9938 - val_loss: 0.1496 - val_acc: 1.0000\n",
      "Epoch 404/800\n",
      "160/160 [==============================] - 0s 112us/step - loss: 0.0690 - acc: 0.9938 - val_loss: 0.1484 - val_acc: 1.0000\n",
      "Epoch 405/800\n",
      "160/160 [==============================] - 0s 114us/step - loss: 0.0686 - acc: 1.0000 - val_loss: 0.1478 - val_acc: 1.0000\n",
      "Epoch 406/800\n",
      "160/160 [==============================] - 0s 133us/step - loss: 0.0682 - acc: 1.0000 - val_loss: 0.1467 - val_acc: 1.0000\n",
      "Epoch 407/800\n",
      "160/160 [==============================] - 0s 108us/step - loss: 0.0679 - acc: 1.0000 - val_loss: 0.1453 - val_acc: 1.0000\n",
      "Epoch 408/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0677 - acc: 1.0000 - val_loss: 0.1437 - val_acc: 1.0000\n",
      "Epoch 409/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0672 - acc: 1.0000 - val_loss: 0.1428 - val_acc: 1.0000\n",
      "Epoch 410/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0669 - acc: 1.0000 - val_loss: 0.1413 - val_acc: 1.0000\n",
      "Epoch 411/800\n",
      "160/160 [==============================] - 0s 99us/step - loss: 0.0666 - acc: 1.0000 - val_loss: 0.1410 - val_acc: 1.0000\n",
      "Epoch 412/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0662 - acc: 1.0000 - val_loss: 0.1397 - val_acc: 1.0000\n",
      "Epoch 413/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0659 - acc: 1.0000 - val_loss: 0.1394 - val_acc: 1.0000\n",
      "Epoch 414/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0655 - acc: 1.0000 - val_loss: 0.1376 - val_acc: 1.0000\n",
      "Epoch 415/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0652 - acc: 1.0000 - val_loss: 0.1370 - val_acc: 1.0000\n",
      "Epoch 416/800\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.0649 - acc: 1.0000 - val_loss: 0.1358 - val_acc: 1.0000\n",
      "Epoch 417/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0646 - acc: 1.0000 - val_loss: 0.1357 - val_acc: 1.0000\n",
      "Epoch 418/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0642 - acc: 1.0000 - val_loss: 0.1344 - val_acc: 1.0000\n",
      "Epoch 419/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0639 - acc: 1.0000 - val_loss: 0.1332 - val_acc: 1.0000\n",
      "Epoch 420/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0636 - acc: 1.0000 - val_loss: 0.1326 - val_acc: 1.0000\n",
      "Epoch 421/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 88us/step - loss: 0.0632 - acc: 1.0000 - val_loss: 0.1311 - val_acc: 1.0000\n",
      "Epoch 422/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.0630 - acc: 1.0000 - val_loss: 0.1299 - val_acc: 1.0000\n",
      "Epoch 423/800\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.0799 - acc: 1.000 - 0s 86us/step - loss: 0.0628 - acc: 1.0000 - val_loss: 0.1282 - val_acc: 1.0000\n",
      "Epoch 424/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.0624 - acc: 1.0000 - val_loss: 0.1283 - val_acc: 1.0000\n",
      "Epoch 425/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0620 - acc: 1.0000 - val_loss: 0.1274 - val_acc: 1.0000\n",
      "Epoch 426/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0617 - acc: 1.0000 - val_loss: 0.1262 - val_acc: 1.0000\n",
      "Epoch 427/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0615 - acc: 1.0000 - val_loss: 0.1251 - val_acc: 1.0000\n",
      "Epoch 428/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0611 - acc: 1.0000 - val_loss: 0.1248 - val_acc: 1.0000\n",
      "Epoch 429/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0608 - acc: 1.0000 - val_loss: 0.1241 - val_acc: 1.0000\n",
      "Epoch 430/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.0605 - acc: 1.0000 - val_loss: 0.1236 - val_acc: 1.0000\n",
      "Epoch 431/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0602 - acc: 1.0000 - val_loss: 0.1225 - val_acc: 1.0000\n",
      "Epoch 432/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0600 - acc: 1.0000 - val_loss: 0.1216 - val_acc: 1.0000\n",
      "Epoch 433/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0597 - acc: 1.0000 - val_loss: 0.1209 - val_acc: 1.0000\n",
      "Epoch 434/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0594 - acc: 1.0000 - val_loss: 0.1201 - val_acc: 1.0000\n",
      "Epoch 435/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0591 - acc: 1.0000 - val_loss: 0.1191 - val_acc: 1.0000\n",
      "Epoch 436/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0588 - acc: 1.0000 - val_loss: 0.1183 - val_acc: 1.0000\n",
      "Epoch 437/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0585 - acc: 1.0000 - val_loss: 0.1172 - val_acc: 1.0000\n",
      "Epoch 438/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0583 - acc: 1.0000 - val_loss: 0.1163 - val_acc: 1.0000\n",
      "Epoch 439/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0580 - acc: 1.0000 - val_loss: 0.1153 - val_acc: 1.0000\n",
      "Epoch 440/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0578 - acc: 1.0000 - val_loss: 0.1141 - val_acc: 1.0000\n",
      "Epoch 441/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0575 - acc: 1.0000 - val_loss: 0.1140 - val_acc: 1.0000\n",
      "Epoch 442/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0572 - acc: 1.0000 - val_loss: 0.1133 - val_acc: 1.0000\n",
      "Epoch 443/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0569 - acc: 1.0000 - val_loss: 0.1131 - val_acc: 1.0000\n",
      "Epoch 444/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0566 - acc: 1.0000 - val_loss: 0.1125 - val_acc: 1.0000\n",
      "Epoch 445/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0564 - acc: 1.0000 - val_loss: 0.1119 - val_acc: 1.0000\n",
      "Epoch 446/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0562 - acc: 1.0000 - val_loss: 0.1111 - val_acc: 1.0000\n",
      "Epoch 447/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0559 - acc: 1.0000 - val_loss: 0.1102 - val_acc: 1.0000\n",
      "Epoch 448/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0556 - acc: 1.0000 - val_loss: 0.1094 - val_acc: 1.0000\n",
      "Epoch 449/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0554 - acc: 1.0000 - val_loss: 0.1090 - val_acc: 1.0000\n",
      "Epoch 450/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.0551 - acc: 1.0000 - val_loss: 0.1084 - val_acc: 1.0000\n",
      "Epoch 451/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0548 - acc: 1.0000 - val_loss: 0.1073 - val_acc: 1.0000\n",
      "Epoch 452/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0546 - acc: 1.0000 - val_loss: 0.1062 - val_acc: 1.0000\n",
      "Epoch 453/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0544 - acc: 1.0000 - val_loss: 0.1051 - val_acc: 1.0000\n",
      "Epoch 454/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0541 - acc: 1.0000 - val_loss: 0.1044 - val_acc: 1.0000\n",
      "Epoch 455/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0539 - acc: 1.0000 - val_loss: 0.1041 - val_acc: 1.0000\n",
      "Epoch 456/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0536 - acc: 1.0000 - val_loss: 0.1031 - val_acc: 1.0000\n",
      "Epoch 457/800\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.0534 - acc: 1.0000 - val_loss: 0.1025 - val_acc: 1.0000\n",
      "Epoch 458/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0531 - acc: 1.0000 - val_loss: 0.1023 - val_acc: 1.0000\n",
      "Epoch 459/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0529 - acc: 1.0000 - val_loss: 0.1017 - val_acc: 1.0000\n",
      "Epoch 460/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0527 - acc: 1.0000 - val_loss: 0.1009 - val_acc: 1.0000\n",
      "Epoch 461/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0524 - acc: 1.0000 - val_loss: 0.1005 - val_acc: 1.0000\n",
      "Epoch 462/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0522 - acc: 1.0000 - val_loss: 0.1002 - val_acc: 1.0000\n",
      "Epoch 463/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0520 - acc: 1.0000 - val_loss: 0.0992 - val_acc: 1.0000\n",
      "Epoch 464/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0518 - acc: 1.0000 - val_loss: 0.0992 - val_acc: 1.0000\n",
      "Epoch 465/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0515 - acc: 1.0000 - val_loss: 0.0986 - val_acc: 1.0000\n",
      "Epoch 466/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0513 - acc: 1.0000 - val_loss: 0.0978 - val_acc: 1.0000\n",
      "Epoch 467/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0511 - acc: 1.0000 - val_loss: 0.0970 - val_acc: 1.0000\n",
      "Epoch 468/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0509 - acc: 1.0000 - val_loss: 0.0969 - val_acc: 1.0000\n",
      "Epoch 469/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0506 - acc: 1.0000 - val_loss: 0.0957 - val_acc: 1.0000\n",
      "Epoch 470/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0504 - acc: 1.0000 - val_loss: 0.0947 - val_acc: 1.0000\n",
      "Epoch 471/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0502 - acc: 1.0000 - val_loss: 0.0940 - val_acc: 1.0000\n",
      "Epoch 472/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0500 - acc: 1.0000 - val_loss: 0.0941 - val_acc: 1.0000\n",
      "Epoch 473/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0498 - acc: 1.0000 - val_loss: 0.0938 - val_acc: 1.0000\n",
      "Epoch 474/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0495 - acc: 1.0000 - val_loss: 0.0929 - val_acc: 1.0000\n",
      "Epoch 475/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0493 - acc: 1.0000 - val_loss: 0.0923 - val_acc: 1.0000\n",
      "Epoch 476/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0492 - acc: 1.0000 - val_loss: 0.0912 - val_acc: 1.0000\n",
      "Epoch 477/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0490 - acc: 1.0000 - val_loss: 0.0911 - val_acc: 1.0000\n",
      "Epoch 478/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0487 - acc: 1.0000 - val_loss: 0.0907 - val_acc: 1.0000\n",
      "Epoch 479/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0485 - acc: 1.0000 - val_loss: 0.0901 - val_acc: 1.0000\n",
      "Epoch 480/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0483 - acc: 1.0000 - val_loss: 0.0895 - val_acc: 1.0000\n",
      "Epoch 481/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 103us/step - loss: 0.0481 - acc: 1.0000 - val_loss: 0.0886 - val_acc: 1.0000\n",
      "Epoch 482/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0479 - acc: 1.0000 - val_loss: 0.0878 - val_acc: 1.0000\n",
      "Epoch 483/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0477 - acc: 1.0000 - val_loss: 0.0878 - val_acc: 1.0000\n",
      "Epoch 484/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0475 - acc: 1.0000 - val_loss: 0.0871 - val_acc: 1.0000\n",
      "Epoch 485/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0473 - acc: 1.0000 - val_loss: 0.0867 - val_acc: 1.0000\n",
      "Epoch 486/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0471 - acc: 1.0000 - val_loss: 0.0864 - val_acc: 1.0000\n",
      "Epoch 487/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0469 - acc: 1.0000 - val_loss: 0.0861 - val_acc: 1.0000\n",
      "Epoch 488/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0467 - acc: 1.0000 - val_loss: 0.0859 - val_acc: 1.0000\n",
      "Epoch 489/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.0465 - acc: 1.0000 - val_loss: 0.0851 - val_acc: 1.0000\n",
      "Epoch 490/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0464 - acc: 1.0000 - val_loss: 0.0853 - val_acc: 1.0000\n",
      "Epoch 491/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0461 - acc: 1.0000 - val_loss: 0.0847 - val_acc: 1.0000\n",
      "Epoch 492/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0459 - acc: 1.0000 - val_loss: 0.0839 - val_acc: 1.0000\n",
      "Epoch 493/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0458 - acc: 1.0000 - val_loss: 0.0827 - val_acc: 1.0000\n",
      "Epoch 494/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0455 - acc: 1.0000 - val_loss: 0.0823 - val_acc: 1.0000\n",
      "Epoch 495/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0453 - acc: 1.0000 - val_loss: 0.0819 - val_acc: 1.0000\n",
      "Epoch 496/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0452 - acc: 1.0000 - val_loss: 0.0814 - val_acc: 1.0000\n",
      "Epoch 497/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0450 - acc: 1.0000 - val_loss: 0.0815 - val_acc: 1.0000\n",
      "Epoch 498/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0448 - acc: 1.0000 - val_loss: 0.0814 - val_acc: 1.0000\n",
      "Epoch 499/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0447 - acc: 1.0000 - val_loss: 0.0804 - val_acc: 1.0000\n",
      "Epoch 500/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0445 - acc: 1.0000 - val_loss: 0.0797 - val_acc: 1.0000\n",
      "Epoch 501/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0443 - acc: 1.0000 - val_loss: 0.0794 - val_acc: 1.0000\n",
      "Epoch 502/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0441 - acc: 1.0000 - val_loss: 0.0790 - val_acc: 1.0000\n",
      "Epoch 503/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.0440 - acc: 1.0000 - val_loss: 0.0788 - val_acc: 1.0000\n",
      "Epoch 504/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0437 - acc: 1.0000 - val_loss: 0.0778 - val_acc: 1.0000\n",
      "Epoch 505/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0436 - acc: 1.0000 - val_loss: 0.0772 - val_acc: 1.0000\n",
      "Epoch 506/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0433 - acc: 1.0000 - val_loss: 0.0769 - val_acc: 1.0000\n",
      "Epoch 507/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0432 - acc: 1.0000 - val_loss: 0.0767 - val_acc: 1.0000\n",
      "Epoch 508/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0431 - acc: 1.0000 - val_loss: 0.0760 - val_acc: 1.0000\n",
      "Epoch 509/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0428 - acc: 1.0000 - val_loss: 0.0760 - val_acc: 1.0000\n",
      "Epoch 510/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0427 - acc: 1.0000 - val_loss: 0.0756 - val_acc: 1.0000\n",
      "Epoch 511/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0425 - acc: 1.0000 - val_loss: 0.0751 - val_acc: 1.0000\n",
      "Epoch 512/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0423 - acc: 1.0000 - val_loss: 0.0750 - val_acc: 1.0000\n",
      "Epoch 513/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0421 - acc: 1.0000 - val_loss: 0.0743 - val_acc: 1.0000\n",
      "Epoch 514/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0420 - acc: 1.0000 - val_loss: 0.0740 - val_acc: 1.0000\n",
      "Epoch 515/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0418 - acc: 1.0000 - val_loss: 0.0734 - val_acc: 1.0000\n",
      "Epoch 516/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0416 - acc: 1.0000 - val_loss: 0.0731 - val_acc: 1.0000\n",
      "Epoch 517/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0415 - acc: 1.0000 - val_loss: 0.0726 - val_acc: 1.0000\n",
      "Epoch 518/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0413 - acc: 1.0000 - val_loss: 0.0725 - val_acc: 1.0000\n",
      "Epoch 519/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0412 - acc: 1.0000 - val_loss: 0.0720 - val_acc: 1.0000\n",
      "Epoch 520/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0410 - acc: 1.0000 - val_loss: 0.0717 - val_acc: 1.0000\n",
      "Epoch 521/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0409 - acc: 1.0000 - val_loss: 0.0710 - val_acc: 1.0000\n",
      "Epoch 522/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0407 - acc: 1.0000 - val_loss: 0.0703 - val_acc: 1.0000\n",
      "Epoch 523/800\n",
      "160/160 [==============================] - 0s 99us/step - loss: 0.0405 - acc: 1.0000 - val_loss: 0.0702 - val_acc: 1.0000\n",
      "Epoch 524/800\n",
      "160/160 [==============================] - 0s 86us/step - loss: 0.0404 - acc: 1.0000 - val_loss: 0.0701 - val_acc: 1.0000\n",
      "Epoch 525/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0402 - acc: 1.0000 - val_loss: 0.0696 - val_acc: 1.0000\n",
      "Epoch 526/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0400 - acc: 1.0000 - val_loss: 0.0691 - val_acc: 1.0000\n",
      "Epoch 527/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0399 - acc: 1.0000 - val_loss: 0.0691 - val_acc: 1.0000\n",
      "Epoch 528/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0397 - acc: 1.0000 - val_loss: 0.0683 - val_acc: 1.0000\n",
      "Epoch 529/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0396 - acc: 1.0000 - val_loss: 0.0678 - val_acc: 1.0000\n",
      "Epoch 530/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0394 - acc: 1.0000 - val_loss: 0.0675 - val_acc: 1.0000\n",
      "Epoch 531/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0393 - acc: 1.0000 - val_loss: 0.0674 - val_acc: 1.0000\n",
      "Epoch 532/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0391 - acc: 1.0000 - val_loss: 0.0672 - val_acc: 1.0000\n",
      "Epoch 533/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0390 - acc: 1.0000 - val_loss: 0.0672 - val_acc: 1.0000\n",
      "Epoch 534/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0388 - acc: 1.0000 - val_loss: 0.0665 - val_acc: 1.0000\n",
      "Epoch 535/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0386 - acc: 1.0000 - val_loss: 0.0660 - val_acc: 1.0000\n",
      "Epoch 536/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0385 - acc: 1.0000 - val_loss: 0.0656 - val_acc: 1.0000\n",
      "Epoch 537/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0383 - acc: 1.0000 - val_loss: 0.0649 - val_acc: 1.0000\n",
      "Epoch 538/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0383 - acc: 1.0000 - val_loss: 0.0642 - val_acc: 1.0000\n",
      "Epoch 539/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0380 - acc: 1.0000 - val_loss: 0.0639 - val_acc: 1.0000\n",
      "Epoch 540/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0378 - acc: 1.0000 - val_loss: 0.0637 - val_acc: 1.0000\n",
      "Epoch 541/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 92us/step - loss: 0.0378 - acc: 1.0000 - val_loss: 0.0639 - val_acc: 1.0000\n",
      "Epoch 542/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0376 - acc: 1.0000 - val_loss: 0.0636 - val_acc: 1.0000\n",
      "Epoch 543/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0374 - acc: 1.0000 - val_loss: 0.0634 - val_acc: 1.0000\n",
      "Epoch 544/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0373 - acc: 1.0000 - val_loss: 0.0626 - val_acc: 1.0000\n",
      "Epoch 545/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0371 - acc: 1.0000 - val_loss: 0.0626 - val_acc: 1.0000\n",
      "Epoch 546/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.0370 - acc: 1.0000 - val_loss: 0.0621 - val_acc: 1.0000\n",
      "Epoch 547/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0369 - acc: 1.0000 - val_loss: 0.0617 - val_acc: 1.0000\n",
      "Epoch 548/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0367 - acc: 1.0000 - val_loss: 0.0618 - val_acc: 1.0000\n",
      "Epoch 549/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0366 - acc: 1.0000 - val_loss: 0.0615 - val_acc: 1.0000\n",
      "Epoch 550/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0364 - acc: 1.0000 - val_loss: 0.0612 - val_acc: 1.0000\n",
      "Epoch 551/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0363 - acc: 1.0000 - val_loss: 0.0609 - val_acc: 1.0000\n",
      "Epoch 552/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0362 - acc: 1.0000 - val_loss: 0.0602 - val_acc: 1.0000\n",
      "Epoch 553/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0360 - acc: 1.0000 - val_loss: 0.0600 - val_acc: 1.0000\n",
      "Epoch 554/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0359 - acc: 1.0000 - val_loss: 0.0596 - val_acc: 1.0000\n",
      "Epoch 555/800\n",
      "160/160 [==============================] - 0s 103us/step - loss: 0.0357 - acc: 1.0000 - val_loss: 0.0590 - val_acc: 1.0000\n",
      "Epoch 556/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0356 - acc: 1.0000 - val_loss: 0.0588 - val_acc: 1.0000\n",
      "Epoch 557/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.0354 - acc: 1.0000 - val_loss: 0.0587 - val_acc: 1.0000\n",
      "Epoch 558/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.0353 - acc: 1.0000 - val_loss: 0.0588 - val_acc: 1.0000\n",
      "Epoch 559/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0352 - acc: 1.0000 - val_loss: 0.0587 - val_acc: 1.0000\n",
      "Epoch 560/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0351 - acc: 1.0000 - val_loss: 0.0580 - val_acc: 1.0000\n",
      "Epoch 561/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0349 - acc: 1.0000 - val_loss: 0.0575 - val_acc: 1.0000\n",
      "Epoch 562/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0348 - acc: 1.0000 - val_loss: 0.0574 - val_acc: 1.0000\n",
      "Epoch 563/800\n",
      "160/160 [==============================] - 0s 106us/step - loss: 0.0347 - acc: 1.0000 - val_loss: 0.0571 - val_acc: 1.0000\n",
      "Epoch 564/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0345 - acc: 1.0000 - val_loss: 0.0568 - val_acc: 1.0000\n",
      "Epoch 565/800\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.0344 - acc: 1.0000 - val_loss: 0.0565 - val_acc: 1.0000\n",
      "Epoch 566/800\n",
      "160/160 [==============================] - 0s 96us/step - loss: 0.0342 - acc: 1.0000 - val_loss: 0.0563 - val_acc: 1.0000\n",
      "Epoch 567/800\n",
      "160/160 [==============================] - 0s 112us/step - loss: 0.0341 - acc: 1.0000 - val_loss: 0.0562 - val_acc: 1.0000\n",
      "Epoch 568/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0340 - acc: 1.0000 - val_loss: 0.0555 - val_acc: 1.0000\n",
      "Epoch 569/800\n",
      "160/160 [==============================] - 0s 104us/step - loss: 0.0338 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 1.0000\n",
      "Epoch 570/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0337 - acc: 1.0000 - val_loss: 0.0546 - val_acc: 1.0000\n",
      "Epoch 571/800\n",
      "160/160 [==============================] - 0s 100us/step - loss: 0.0336 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 1.0000\n",
      "Epoch 572/800\n",
      "160/160 [==============================] - 0s 136us/step - loss: 0.0334 - acc: 1.0000 - val_loss: 0.0542 - val_acc: 1.0000\n",
      "Epoch 573/800\n",
      "160/160 [==============================] - 0s 85us/step - loss: 0.0333 - acc: 1.0000 - val_loss: 0.0539 - val_acc: 1.0000\n",
      "Epoch 574/800\n",
      "160/160 [==============================] - 0s 108us/step - loss: 0.0332 - acc: 1.0000 - val_loss: 0.0534 - val_acc: 1.0000\n",
      "Epoch 575/800\n",
      "160/160 [==============================] - 0s 81us/step - loss: 0.0331 - acc: 1.0000 - val_loss: 0.0537 - val_acc: 1.0000\n",
      "Epoch 576/800\n",
      "160/160 [==============================] - 0s 92us/step - loss: 0.0330 - acc: 1.0000 - val_loss: 0.0531 - val_acc: 1.0000\n",
      "Epoch 577/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0328 - acc: 1.0000 - val_loss: 0.0531 - val_acc: 1.0000\n",
      "Epoch 578/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0327 - acc: 1.0000 - val_loss: 0.0527 - val_acc: 1.0000\n",
      "Epoch 579/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0325 - acc: 1.0000 - val_loss: 0.0525 - val_acc: 1.0000\n",
      "Epoch 580/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0324 - acc: 1.0000 - val_loss: 0.0524 - val_acc: 1.0000\n",
      "Epoch 581/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0323 - acc: 1.0000 - val_loss: 0.0519 - val_acc: 1.0000\n",
      "Epoch 582/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0322 - acc: 1.0000 - val_loss: 0.0517 - val_acc: 1.0000\n",
      "Epoch 583/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0321 - acc: 1.0000 - val_loss: 0.0513 - val_acc: 1.0000\n",
      "Epoch 584/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0319 - acc: 1.0000 - val_loss: 0.0513 - val_acc: 1.0000\n",
      "Epoch 585/800\n",
      "160/160 [==============================] - 0s 90us/step - loss: 0.0318 - acc: 1.0000 - val_loss: 0.0509 - val_acc: 1.0000\n",
      "Epoch 586/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0317 - acc: 1.0000 - val_loss: 0.0507 - val_acc: 1.0000\n",
      "Epoch 587/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0316 - acc: 1.0000 - val_loss: 0.0505 - val_acc: 1.0000\n",
      "Epoch 588/800\n",
      "160/160 [==============================] - 0s 132us/step - loss: 0.0315 - acc: 1.0000 - val_loss: 0.0500 - val_acc: 1.0000\n",
      "Epoch 589/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0314 - acc: 1.0000 - val_loss: 0.0500 - val_acc: 1.0000\n",
      "Epoch 590/800\n",
      "160/160 [==============================] - 0s 159us/step - loss: 0.0312 - acc: 1.0000 - val_loss: 0.0495 - val_acc: 1.0000\n",
      "Epoch 591/800\n",
      "160/160 [==============================] - 0s 97us/step - loss: 0.0312 - acc: 1.0000 - val_loss: 0.0499 - val_acc: 1.0000\n",
      "Epoch 592/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0310 - acc: 1.0000 - val_loss: 0.0493 - val_acc: 1.0000\n",
      "Epoch 593/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0309 - acc: 1.0000 - val_loss: 0.0490 - val_acc: 1.0000\n",
      "Epoch 594/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0308 - acc: 1.0000 - val_loss: 0.0484 - val_acc: 1.0000\n",
      "Epoch 595/800\n",
      "160/160 [==============================] - 0s 95us/step - loss: 0.0307 - acc: 1.0000 - val_loss: 0.0481 - val_acc: 1.0000\n",
      "Epoch 596/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0305 - acc: 1.0000 - val_loss: 0.0482 - val_acc: 1.0000\n",
      "Epoch 597/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0304 - acc: 1.0000 - val_loss: 0.0481 - val_acc: 1.0000\n",
      "Epoch 598/800\n",
      "160/160 [==============================] - 0s 91us/step - loss: 0.0303 - acc: 1.0000 - val_loss: 0.0476 - val_acc: 1.0000\n",
      "Epoch 599/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0302 - acc: 1.0000 - val_loss: 0.0472 - val_acc: 1.0000\n",
      "Epoch 600/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0301 - acc: 1.0000 - val_loss: 0.0472 - val_acc: 1.0000\n",
      "Epoch 601/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 72us/step - loss: 0.0300 - acc: 1.0000 - val_loss: 0.0474 - val_acc: 1.0000\n",
      "Epoch 602/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0299 - acc: 1.0000 - val_loss: 0.0472 - val_acc: 1.0000\n",
      "Epoch 603/800\n",
      "160/160 [==============================] - 0s 82us/step - loss: 0.0297 - acc: 1.0000 - val_loss: 0.0469 - val_acc: 1.0000\n",
      "Epoch 604/800\n",
      "160/160 [==============================] - 0s 98us/step - loss: 0.0297 - acc: 1.0000 - val_loss: 0.0462 - val_acc: 1.0000\n",
      "Epoch 605/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0296 - acc: 1.0000 - val_loss: 0.0462 - val_acc: 1.0000\n",
      "Epoch 606/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0294 - acc: 1.0000 - val_loss: 0.0456 - val_acc: 1.0000\n",
      "Epoch 607/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0293 - acc: 1.0000 - val_loss: 0.0456 - val_acc: 1.0000\n",
      "Epoch 608/800\n",
      "160/160 [==============================] - 0s 80us/step - loss: 0.0292 - acc: 1.0000 - val_loss: 0.0456 - val_acc: 1.0000\n",
      "Epoch 609/800\n",
      "160/160 [==============================] - 0s 88us/step - loss: 0.0291 - acc: 1.0000 - val_loss: 0.0452 - val_acc: 1.0000\n",
      "Epoch 610/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.0290 - acc: 1.0000 - val_loss: 0.0451 - val_acc: 1.0000\n",
      "Epoch 611/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0289 - acc: 1.0000 - val_loss: 0.0446 - val_acc: 1.0000\n",
      "Epoch 612/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0287 - acc: 1.0000 - val_loss: 0.0446 - val_acc: 1.0000\n",
      "Epoch 613/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0287 - acc: 1.0000 - val_loss: 0.0445 - val_acc: 1.0000\n",
      "Epoch 614/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0285 - acc: 1.0000 - val_loss: 0.0444 - val_acc: 1.0000\n",
      "Epoch 615/800\n",
      "160/160 [==============================] - 0s 94us/step - loss: 0.0285 - acc: 1.0000 - val_loss: 0.0440 - val_acc: 1.0000\n",
      "Epoch 616/800\n",
      "160/160 [==============================] - 0s 135us/step - loss: 0.0283 - acc: 1.0000 - val_loss: 0.0437 - val_acc: 1.0000\n",
      "Epoch 617/800\n",
      "160/160 [==============================] - 0s 158us/step - loss: 0.0282 - acc: 1.0000 - val_loss: 0.0434 - val_acc: 1.0000\n",
      "Epoch 618/800\n",
      "160/160 [==============================] - 0s 160us/step - loss: 0.0281 - acc: 1.0000 - val_loss: 0.0432 - val_acc: 1.0000\n",
      "Epoch 619/800\n",
      "160/160 [==============================] - 0s 104us/step - loss: 0.0280 - acc: 1.0000 - val_loss: 0.0432 - val_acc: 1.0000\n",
      "Epoch 620/800\n",
      "160/160 [==============================] - 0s 83us/step - loss: 0.0279 - acc: 1.0000 - val_loss: 0.0429 - val_acc: 1.0000\n",
      "Epoch 621/800\n",
      "160/160 [==============================] - 0s 108us/step - loss: 0.0278 - acc: 1.0000 - val_loss: 0.0425 - val_acc: 1.0000\n",
      "Epoch 622/800\n",
      "160/160 [==============================] - 0s 101us/step - loss: 0.0277 - acc: 1.0000 - val_loss: 0.0426 - val_acc: 1.0000\n",
      "Epoch 623/800\n",
      "160/160 [==============================] - 0s 103us/step - loss: 0.0276 - acc: 1.0000 - val_loss: 0.0420 - val_acc: 1.0000\n",
      "Epoch 624/800\n",
      "160/160 [==============================] - 0s 111us/step - loss: 0.0276 - acc: 1.0000 - val_loss: 0.0420 - val_acc: 1.0000\n",
      "Epoch 625/800\n",
      "160/160 [==============================] - 0s 110us/step - loss: 0.0274 - acc: 1.0000 - val_loss: 0.0417 - val_acc: 1.0000\n",
      "Epoch 626/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0274 - acc: 1.0000 - val_loss: 0.0415 - val_acc: 1.0000\n",
      "Epoch 627/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0272 - acc: 1.0000 - val_loss: 0.0416 - val_acc: 1.0000\n",
      "Epoch 628/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0271 - acc: 1.0000 - val_loss: 0.0413 - val_acc: 1.0000\n",
      "Epoch 629/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0270 - acc: 1.0000 - val_loss: 0.0413 - val_acc: 1.0000\n",
      "Epoch 630/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0269 - acc: 1.0000 - val_loss: 0.0410 - val_acc: 1.0000\n",
      "Epoch 631/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0268 - acc: 1.0000 - val_loss: 0.0405 - val_acc: 1.0000\n",
      "Epoch 632/800\n",
      "160/160 [==============================] - 0s 93us/step - loss: 0.0267 - acc: 1.0000 - val_loss: 0.0404 - val_acc: 1.0000\n",
      "Epoch 633/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0267 - acc: 1.0000 - val_loss: 0.0408 - val_acc: 1.0000\n",
      "Epoch 634/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0265 - acc: 1.0000 - val_loss: 0.0405 - val_acc: 1.0000\n",
      "Epoch 635/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0264 - acc: 1.0000 - val_loss: 0.0400 - val_acc: 1.0000\n",
      "Epoch 636/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0264 - acc: 1.0000 - val_loss: 0.0395 - val_acc: 1.0000\n",
      "Epoch 637/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0263 - acc: 1.0000 - val_loss: 0.0397 - val_acc: 1.0000\n",
      "Epoch 638/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0261 - acc: 1.0000 - val_loss: 0.0394 - val_acc: 1.0000\n",
      "Epoch 639/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0261 - acc: 1.0000 - val_loss: 0.0390 - val_acc: 1.0000\n",
      "Epoch 640/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0260 - acc: 1.0000 - val_loss: 0.0388 - val_acc: 1.0000\n",
      "Epoch 641/800\n",
      "160/160 [==============================] - 0s 87us/step - loss: 0.0259 - acc: 1.0000 - val_loss: 0.0387 - val_acc: 1.0000\n",
      "Epoch 642/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0258 - acc: 1.0000 - val_loss: 0.0384 - val_acc: 1.0000\n",
      "Epoch 643/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0257 - acc: 1.0000 - val_loss: 0.0382 - val_acc: 1.0000\n",
      "Epoch 644/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0256 - acc: 1.0000 - val_loss: 0.0384 - val_acc: 1.0000\n",
      "Epoch 645/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0255 - acc: 1.0000 - val_loss: 0.0384 - val_acc: 1.0000\n",
      "Epoch 646/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0254 - acc: 1.0000 - val_loss: 0.0382 - val_acc: 1.0000\n",
      "Epoch 647/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0254 - acc: 1.0000 - val_loss: 0.0382 - val_acc: 1.0000\n",
      "Epoch 648/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0252 - acc: 1.0000 - val_loss: 0.0377 - val_acc: 1.0000\n",
      "Epoch 649/800\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.0273 - acc: 1.000 - 0s 63us/step - loss: 0.0252 - acc: 1.0000 - val_loss: 0.0376 - val_acc: 1.0000\n",
      "Epoch 650/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0251 - acc: 1.0000 - val_loss: 0.0374 - val_acc: 1.0000\n",
      "Epoch 651/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0250 - acc: 1.0000 - val_loss: 0.0372 - val_acc: 1.0000\n",
      "Epoch 652/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0249 - acc: 1.0000 - val_loss: 0.0372 - val_acc: 1.0000\n",
      "Epoch 653/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0248 - acc: 1.0000 - val_loss: 0.0369 - val_acc: 1.0000\n",
      "Epoch 654/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0247 - acc: 1.0000 - val_loss: 0.0368 - val_acc: 1.0000\n",
      "Epoch 655/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0246 - acc: 1.0000 - val_loss: 0.0365 - val_acc: 1.0000\n",
      "Epoch 656/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0245 - acc: 1.0000 - val_loss: 0.0364 - val_acc: 1.0000\n",
      "Epoch 657/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0245 - acc: 1.0000 - val_loss: 0.0362 - val_acc: 1.0000\n",
      "Epoch 658/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0244 - acc: 1.0000 - val_loss: 0.0358 - val_acc: 1.0000\n",
      "Epoch 659/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0243 - acc: 1.0000 - val_loss: 0.0356 - val_acc: 1.0000\n",
      "Epoch 660/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0242 - acc: 1.0000 - val_loss: 0.0356 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 661/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0242 - acc: 1.0000 - val_loss: 0.0358 - val_acc: 1.0000\n",
      "Epoch 662/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0241 - acc: 1.0000 - val_loss: 0.0352 - val_acc: 1.0000\n",
      "Epoch 663/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0240 - acc: 1.0000 - val_loss: 0.0351 - val_acc: 1.0000\n",
      "Epoch 664/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0239 - acc: 1.0000 - val_loss: 0.0348 - val_acc: 1.0000\n",
      "Epoch 665/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0238 - acc: 1.0000 - val_loss: 0.0348 - val_acc: 1.0000\n",
      "Epoch 666/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0237 - acc: 1.0000 - val_loss: 0.0345 - val_acc: 1.0000\n",
      "Epoch 667/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0236 - acc: 1.0000 - val_loss: 0.0343 - val_acc: 1.0000\n",
      "Epoch 668/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0235 - acc: 1.0000 - val_loss: 0.0344 - val_acc: 1.0000\n",
      "Epoch 669/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0235 - acc: 1.0000 - val_loss: 0.0344 - val_acc: 1.0000\n",
      "Epoch 670/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0234 - acc: 1.0000 - val_loss: 0.0343 - val_acc: 1.0000\n",
      "Epoch 671/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0233 - acc: 1.0000 - val_loss: 0.0342 - val_acc: 1.0000\n",
      "Epoch 672/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0233 - acc: 1.0000 - val_loss: 0.0343 - val_acc: 1.0000\n",
      "Epoch 673/800\n",
      "160/160 [==============================] - 0s 72us/step - loss: 0.0231 - acc: 1.0000 - val_loss: 0.0341 - val_acc: 1.0000\n",
      "Epoch 674/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0231 - acc: 1.0000 - val_loss: 0.0336 - val_acc: 1.0000\n",
      "Epoch 675/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0230 - acc: 1.0000 - val_loss: 0.0333 - val_acc: 1.0000\n",
      "Epoch 676/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0229 - acc: 1.0000 - val_loss: 0.0329 - val_acc: 1.0000\n",
      "Epoch 677/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0228 - acc: 1.0000 - val_loss: 0.0331 - val_acc: 1.0000\n",
      "Epoch 678/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0228 - acc: 1.0000 - val_loss: 0.0326 - val_acc: 1.0000\n",
      "Epoch 679/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0227 - acc: 1.0000 - val_loss: 0.0327 - val_acc: 1.0000\n",
      "Epoch 680/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0226 - acc: 1.0000 - val_loss: 0.0328 - val_acc: 1.0000\n",
      "Epoch 681/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0226 - acc: 1.0000 - val_loss: 0.0327 - val_acc: 1.0000\n",
      "Epoch 682/800\n",
      "160/160 [==============================] - 0s 89us/step - loss: 0.0225 - acc: 1.0000 - val_loss: 0.0325 - val_acc: 1.0000\n",
      "Epoch 683/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0224 - acc: 1.0000 - val_loss: 0.0325 - val_acc: 1.0000\n",
      "Epoch 684/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0223 - acc: 1.0000 - val_loss: 0.0325 - val_acc: 1.0000\n",
      "Epoch 685/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0222 - acc: 1.0000 - val_loss: 0.0321 - val_acc: 1.0000\n",
      "Epoch 686/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0222 - acc: 1.0000 - val_loss: 0.0319 - val_acc: 1.0000\n",
      "Epoch 687/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0222 - acc: 1.0000 - val_loss: 0.0315 - val_acc: 1.0000\n",
      "Epoch 688/800\n",
      "160/160 [==============================] - 0s 79us/step - loss: 0.0221 - acc: 1.0000 - val_loss: 0.0318 - val_acc: 1.0000\n",
      "Epoch 689/800\n",
      "160/160 [==============================] - 0s 78us/step - loss: 0.0219 - acc: 1.0000 - val_loss: 0.0316 - val_acc: 1.0000\n",
      "Epoch 690/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0219 - acc: 1.0000 - val_loss: 0.0313 - val_acc: 1.0000\n",
      "Epoch 691/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0218 - acc: 1.0000 - val_loss: 0.0310 - val_acc: 1.0000\n",
      "Epoch 692/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0217 - acc: 1.0000 - val_loss: 0.0310 - val_acc: 1.0000\n",
      "Epoch 693/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0217 - acc: 1.0000 - val_loss: 0.0308 - val_acc: 1.0000\n",
      "Epoch 694/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0216 - acc: 1.0000 - val_loss: 0.0307 - val_acc: 1.0000\n",
      "Epoch 695/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0215 - acc: 1.0000 - val_loss: 0.0308 - val_acc: 1.0000\n",
      "Epoch 696/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0215 - acc: 1.0000 - val_loss: 0.0306 - val_acc: 1.0000\n",
      "Epoch 697/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0214 - acc: 1.0000 - val_loss: 0.0304 - val_acc: 1.0000\n",
      "Epoch 698/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0213 - acc: 1.0000 - val_loss: 0.0305 - val_acc: 1.0000\n",
      "Epoch 699/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0213 - acc: 1.0000 - val_loss: 0.0305 - val_acc: 1.0000\n",
      "Epoch 700/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0213 - acc: 1.0000 - val_loss: 0.0301 - val_acc: 1.0000\n",
      "Epoch 701/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0211 - acc: 1.0000 - val_loss: 0.0299 - val_acc: 1.0000\n",
      "Epoch 702/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0210 - acc: 1.0000 - val_loss: 0.0300 - val_acc: 1.0000\n",
      "Epoch 703/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0210 - acc: 1.0000 - val_loss: 0.0297 - val_acc: 1.0000\n",
      "Epoch 704/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0209 - acc: 1.0000 - val_loss: 0.0292 - val_acc: 1.0000\n",
      "Epoch 705/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0209 - acc: 1.0000 - val_loss: 0.0294 - val_acc: 1.0000\n",
      "Epoch 706/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0208 - acc: 1.0000 - val_loss: 0.0294 - val_acc: 1.0000\n",
      "Epoch 707/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0207 - acc: 1.0000 - val_loss: 0.0293 - val_acc: 1.0000\n",
      "Epoch 708/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0206 - acc: 1.0000 - val_loss: 0.0292 - val_acc: 1.0000\n",
      "Epoch 709/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0206 - acc: 1.0000 - val_loss: 0.0291 - val_acc: 1.0000\n",
      "Epoch 710/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.0291 - val_acc: 1.0000\n",
      "Epoch 711/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0204 - acc: 1.0000 - val_loss: 0.0288 - val_acc: 1.0000\n",
      "Epoch 712/800\n",
      "160/160 [==============================] - 0s 76us/step - loss: 0.0204 - acc: 1.0000 - val_loss: 0.0285 - val_acc: 1.0000\n",
      "Epoch 713/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0203 - acc: 1.0000 - val_loss: 0.0286 - val_acc: 1.0000\n",
      "Epoch 714/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0203 - acc: 1.0000 - val_loss: 0.0284 - val_acc: 1.0000\n",
      "Epoch 715/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0202 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 1.0000\n",
      "Epoch 716/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 1.0000\n",
      "Epoch 717/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.0281 - val_acc: 1.0000\n",
      "Epoch 718/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0200 - acc: 1.0000 - val_loss: 0.0279 - val_acc: 1.0000\n",
      "Epoch 719/800\n",
      "160/160 [==============================] - 0s 73us/step - loss: 0.0200 - acc: 1.0000 - val_loss: 0.0277 - val_acc: 1.0000\n",
      "Epoch 720/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0199 - acc: 1.0000 - val_loss: 0.0277 - val_acc: 1.0000\n",
      "Epoch 721/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 60us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.0275 - val_acc: 1.0000\n",
      "Epoch 722/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.0276 - val_acc: 1.0000\n",
      "Epoch 723/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 0.0275 - val_acc: 1.0000\n",
      "Epoch 724/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 0.0271 - val_acc: 1.0000\n",
      "Epoch 725/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0196 - acc: 1.0000 - val_loss: 0.0273 - val_acc: 1.0000\n",
      "Epoch 726/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.0274 - val_acc: 1.0000\n",
      "Epoch 727/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0194 - acc: 1.0000 - val_loss: 0.0271 - val_acc: 1.0000\n",
      "Epoch 728/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0194 - acc: 1.0000 - val_loss: 0.0268 - val_acc: 1.0000\n",
      "Epoch 729/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.0267 - val_acc: 1.0000\n",
      "Epoch 730/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.0265 - val_acc: 1.0000\n",
      "Epoch 731/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0192 - acc: 1.0000 - val_loss: 0.0265 - val_acc: 1.0000\n",
      "Epoch 732/800\n",
      "160/160 [==============================] - 0s 54us/step - loss: 0.0192 - acc: 1.0000 - val_loss: 0.0266 - val_acc: 1.0000\n",
      "Epoch 733/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0191 - acc: 1.0000 - val_loss: 0.0265 - val_acc: 1.0000\n",
      "Epoch 734/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 0.0264 - val_acc: 1.0000\n",
      "Epoch 735/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.0262 - val_acc: 1.0000\n",
      "Epoch 736/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.0263 - val_acc: 1.0000\n",
      "Epoch 737/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0189 - acc: 1.0000 - val_loss: 0.0261 - val_acc: 1.0000\n",
      "Epoch 738/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.0261 - val_acc: 1.0000\n",
      "Epoch 739/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.0259 - val_acc: 1.0000\n",
      "Epoch 740/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.0257 - val_acc: 1.0000\n",
      "Epoch 741/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0186 - acc: 1.0000 - val_loss: 0.0256 - val_acc: 1.0000\n",
      "Epoch 742/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.0254 - val_acc: 1.0000\n",
      "Epoch 743/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.0254 - val_acc: 1.0000\n",
      "Epoch 744/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.0252 - val_acc: 1.0000\n",
      "Epoch 745/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 0.0248 - val_acc: 1.0000\n",
      "Epoch 746/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.0249 - val_acc: 1.0000\n",
      "Epoch 747/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.0248 - val_acc: 1.0000\n",
      "Epoch 748/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0182 - acc: 1.0000 - val_loss: 0.0246 - val_acc: 1.0000\n",
      "Epoch 749/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.0246 - val_acc: 1.0000\n",
      "Epoch 750/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.0244 - val_acc: 1.0000\n",
      "Epoch 751/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.0243 - val_acc: 1.0000\n",
      "Epoch 752/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0180 - acc: 1.0000 - val_loss: 0.0244 - val_acc: 1.0000\n",
      "Epoch 753/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.0245 - val_acc: 1.0000\n",
      "Epoch 754/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.0245 - val_acc: 1.0000\n",
      "Epoch 755/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.0245 - val_acc: 1.0000\n",
      "Epoch 756/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.0243 - val_acc: 1.0000\n",
      "Epoch 757/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.0243 - val_acc: 1.0000\n",
      "Epoch 758/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 0.0241 - val_acc: 1.0000\n",
      "Epoch 759/800\n",
      "160/160 [==============================] - 0s 69us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.0240 - val_acc: 1.0000\n",
      "Epoch 760/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.0239 - val_acc: 1.0000\n",
      "Epoch 761/800\n",
      "160/160 [==============================] - 0s 74us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.0239 - val_acc: 1.0000\n",
      "Epoch 762/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.0237 - val_acc: 1.0000\n",
      "Epoch 763/800\n",
      "160/160 [==============================] - 0s 77us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.0236 - val_acc: 1.0000\n",
      "Epoch 764/800\n",
      "160/160 [==============================] - 0s 57us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.0234 - val_acc: 1.0000\n",
      "Epoch 765/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.0231 - val_acc: 1.0000\n",
      "Epoch 766/800\n",
      "160/160 [==============================] - 0s 68us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.0230 - val_acc: 1.0000\n",
      "Epoch 767/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 0.0230 - val_acc: 1.0000\n",
      "Epoch 768/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0231 - val_acc: 1.0000\n",
      "Epoch 769/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0230 - val_acc: 1.0000\n",
      "Epoch 770/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0228 - val_acc: 1.0000\n",
      "Epoch 771/800\n",
      "160/160 [==============================] - 0s 75us/step - loss: 0.0170 - acc: 1.0000 - val_loss: 0.0227 - val_acc: 1.0000\n",
      "Epoch 772/800\n",
      "160/160 [==============================] - 0s 66us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 0.0227 - val_acc: 1.0000\n",
      "Epoch 773/800\n",
      "160/160 [==============================] - 0s 58us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 0.0226 - val_acc: 1.0000\n",
      "Epoch 774/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 0.0227 - val_acc: 1.0000\n",
      "Epoch 775/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.0225 - val_acc: 1.0000\n",
      "Epoch 776/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0167 - acc: 1.0000 - val_loss: 0.0223 - val_acc: 1.0000\n",
      "Epoch 777/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0167 - acc: 1.0000 - val_loss: 0.0222 - val_acc: 1.0000\n",
      "Epoch 778/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.0222 - val_acc: 1.0000\n",
      "Epoch 779/800\n",
      "160/160 [==============================] - 0s 125us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.0219 - val_acc: 1.0000\n",
      "Epoch 780/800\n",
      "160/160 [==============================] - 0s 59us/step - loss: 0.0166 - acc: 1.0000 - val_loss: 0.0220 - val_acc: 1.0000\n",
      "Epoch 781/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s 68us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.0219 - val_acc: 1.0000\n",
      "Epoch 782/800\n",
      "160/160 [==============================] - 0s 70us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.0218 - val_acc: 1.0000\n",
      "Epoch 783/800\n",
      "160/160 [==============================] - 0s 61us/step - loss: 0.0164 - acc: 1.0000 - val_loss: 0.0218 - val_acc: 1.0000\n",
      "Epoch 784/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0164 - acc: 1.0000 - val_loss: 0.0219 - val_acc: 1.0000\n",
      "Epoch 785/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0163 - acc: 1.0000 - val_loss: 0.0218 - val_acc: 1.0000\n",
      "Epoch 786/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.0217 - val_acc: 1.0000\n",
      "Epoch 787/800\n",
      "160/160 [==============================] - 0s 67us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.0214 - val_acc: 1.0000\n",
      "Epoch 788/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.0214 - val_acc: 1.0000\n",
      "Epoch 789/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 0.0215 - val_acc: 1.0000\n",
      "Epoch 790/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.0213 - val_acc: 1.0000\n",
      "Epoch 791/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.0213 - val_acc: 1.0000\n",
      "Epoch 792/800\n",
      "160/160 [==============================] - 0s 64us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.0210 - val_acc: 1.0000\n",
      "Epoch 793/800\n",
      "160/160 [==============================] - 0s 65us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.0208 - val_acc: 1.0000\n",
      "Epoch 794/800\n",
      "160/160 [==============================] - 0s 60us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 0.0207 - val_acc: 1.0000\n",
      "Epoch 795/800\n",
      "160/160 [==============================] - 0s 56us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.0208 - val_acc: 1.0000\n",
      "Epoch 796/800\n",
      "160/160 [==============================] - 0s 55us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.0206 - val_acc: 1.0000\n",
      "Epoch 797/800\n",
      "160/160 [==============================] - 0s 71us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.0207 - val_acc: 1.0000\n",
      "Epoch 798/800\n",
      "160/160 [==============================] - 0s 84us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.0209 - val_acc: 1.0000\n",
      "Epoch 799/800\n",
      "160/160 [==============================] - 0s 62us/step - loss: 0.0157 - acc: 1.0000 - val_loss: 0.0208 - val_acc: 1.0000\n",
      "Epoch 800/800\n",
      "160/160 [==============================] - 0s 63us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.0208 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa3b7f57050>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_tanh.compile(optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "nn_tanh.fit(X, y, epochs=800, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa3acfae790>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa3b78fb510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_range = np.arange(-5., 5., 0.02)\n",
    "dgrid = len(grid_range)\n",
    "Xpred = np.array([(a, b) for a in grid_range for b in grid_range])\n",
    "prob_pred = nn_tanh.predict(Xpred)\n",
    "ypred = np.round(prob_pred)\n",
    "ypred = ypred.reshape((dgrid, dgrid))\n",
    "ypred = np.flipud(ypred)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.imshow(ypred, extent=[-4, 4, -4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tanh activation function is continuous and it results in a smooth decision boundary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
